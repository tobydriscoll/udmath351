[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for Math 351 @ UD",
    "section": "",
    "text": "Contents"
  },
  {
    "objectID": "index.html#schedule-for-fall-2023",
    "href": "index.html#schedule-for-fall-2023",
    "title": "Notes for Math 351 @ UD",
    "section": "Schedule for Fall 2023",
    "text": "Schedule for Fall 2023\n\nFirst-order ODEs\n\n\n\nDate\nSection\nZill\nSuggested practice\n\n\n\n\nAug 30\nIntro\n–\n\n\n\nAug 30\nIVPs\n–\n\n\n\nSep 1\nQualitative methods\n2.1\n21, 24\n\n\nSep 6\nSeparable equations\n2.2\n5, 6, 7, 10, 2\n\n\nSep 6\nLinear equations\n2.3\n3, 6, 7, 11, 25, 29\n\n\nSep 8\nHomogeneous problems\n(2.3)\n\n\n\nSep 8\nVariation of parameters\n(2.3)\n\n\n\nSep 11\nModeling\n2.7\n1, 3, 5, 15, 23, 27\n\n\n\n\n\nSecond-order ODEs\n\n\n\nDate\nSection\nZill\nSuggested practice\n\n\n\n\nSep 13\n2nd order linear problems\n3.1\n16, 17, 21\n\n\nSep 15\nHomogeneous solutions\n3.3\n1-14, 29, 33, 49, 51\n\n\nSep 15\nComplex solutions\n3.3\n\n\n\nSep 18\nVariation of parameters\n3.5\n3, 5, 10, 16\n\n\nSep 20\nUndetermined coefficients\n3.4\n1, 3, 5, 8, 31\n\n\nSep 22\nOscillators\n3.8\n1, 6, 27, 31, 33, 43, 44\n\n\n\n\n\nLinear systems\n\n\n\nDate\nSection\nPoole\nSuggested practice\n\n\n\n\nSep 25\nIntro to linear systems\n2.1\n1-5, 19, 20, 21, 27-30, 31\n\n\nSep 27\nRow elimination\n2.1, 2.2\n1-8, 9-14, 25-28, 35-38, 40-41\n\n\nSep 29\nEXAM 1\nGoals A1–A13, B1–B10\n\n\n\nOct 2\nRRE form\n2.2\n\n\n\nOct 4\nHomogeneous systems\n2.2\n\n\n\nOct 6\nLinear combinations\n2.3\n1-4, 7, 9, 11, 22-27\n\n\n\n\n\nMatrix algebra\n\n\n\nDate\nSection\nPoole\nSuggested practice\n\n\n\n\nOct 9\nElementwise operations\n3.1\n1, 3, 5, 7, 11, 21-22, 40\n\n\nOct 9\nMatrix-vector multiplication\n3.1\n\n\n\nOct 11\nMatrix multiplication\n3.1, 3.2\n23, 28\n\n\nOct 13\nIdentity and inverse\n3.3\n1-4 (any method), 13a, 42a, 43a\n\n\nOct 13\nFundamental theorem\n3.3\n\n\n\nOct 13\nComputing inverses\n3.3\n\n\n\nOct 16\nSubspaces\n3.5\n(no row spaces) 11-12, 17-20, 27-28, 35-38, 43, 45-46\n\n\nOct 18\nDeterminants\n4.2\n7-12, 45, 47, 49, 50, 57-58\n\n\nOct 20\nEigenvalues (properties)\n4.1\n1-4, 11-12, 27-28, 36\n\n\nOct 23\nEigenvalues (computing)\n4.3\n1-3, 10, 23\n\n\nOct 25\nReview\n\n\n\n\nOct 27\nEXAM 2\nGoals C1–C7, D1–D12\n\n\n\nOct 30\nDiagonalization\n4.4\n5-6, 8-10, 17, 19\n\n\nNov 1\nVector spaces\n6.1\n1, 2, 9, 28, 31, 35, 36, 51-52, 53-54\n\n\nNov 3\nSpan, independence, basis\n6.1, 6.2\n1-3, 5-7, 18-19, 22-24, 28-29, 51-52\n\n\nNov 6\nChange of basis\n6.3\n1-3, 5-7\n\n\n\n\n\nLinear ODE systems"
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Notes for Math 351 @ UD",
    "section": "Learning goals",
    "text": "Learning goals\n\nA. First-order scalar ODEs\n\nVerify an ODE or IVP solution.\nInterpret a direction field for a scalar nonautonomous problem.\nSketch or interpret a phase diagram for an autonomous equation.\nFind the steady states/equilibria of an autonomous equation.\nClassify the stability of an equilibrium solution.\nSolve an initial-value problem if given, or after finding, a general solution.\nSolve separable equations that have tractable integrals, in explicit or implicit form.\nDistinguish between linear and nonlinear ODEs.\nConvert between standard and operator expressions of a linear equation.\nAssemble a general solution from homogeneous and particular solutions.\nFind the general homogeneous solution of a linear equation.\nUse variation of parameters (or an integrating factor) to find a particular solution of a linear equation.\nInterpret and apply models of Newtonian cooling/heating, population dynamics, and free fall.\n\n\n\nB. Second-order linear ODEs\n\nAssemble a general solution from homogeneous and particular solutions.\nUse initial values to solve for the integration constants in a general solution.\n\nCompute the Wronskian of two or three functions.\nUse a Wronskian to determine the linear independence of given solutions.\nSolve a homogeneous linear constant-coefficient equation by way of its characteristic polynomial.\nConvert between complex exponentials and real exponentials/oscillations using Euler’s identity.\nUse variation of parameters to find a particular solution.\nUse undetermined coefficients to find a particular solution with polynomial, exponential, or harmonic forcing.\nRelate a linear constant-coefficient equation to a mechanical oscillator, and classify as undamped, underdamped, critically damped, or overdamped.\nIdentify resonance or pseudoresonance for a harmonically forced oscillator.\n\n\n\nC. Linear algebraic systems\n\nConvert between linear systems and matrix–vector representations of them.\nDistinguish matrices in RRE form from other matrices.\nPerform row elimination to put a matrix into triangular or RRE form.\nUse the RRE form of a linear system to find its solution(s).\nConnect null space to a homogeneous linear system.\nFind the null space and rank of a matrix.\nFind a basis for a null space.\n\n\n\nD. Matrix algebra\n\nConvert between a linear combination and an equivalent matrix–vector multiplication.\nCompute products of matrices.\nApply properties of matrix algebra.\nRecognize or produce an identity matrix.\nApply properties of inverse matrices.\nApply the equivalencies in the Fundamental Theorem of Linear Algebra.\nUse a matrix inverse to solve a linear system.\nCompute the inverse of a 2x2 matrix.\nFind a basis for the row space or the column space of a matrix.\nFind the dimension of a subspace.\nCompute the determinant of a small matrix.\nUse a determinant to determine whether a matrix is singular.\nApply the definition and properties of eigenvalues and eigenvectors.\nCompute eigenvalues and eigenspaces of 2x2 matrices and certain 3x3 matrices.\nFind the algebraic and geometric multiplicities of an eigenvalue.\nDetermine whether an eigenvalue or matrix is defective.\nExpress a nondefective matrix in terms of its diagonalization.\n\n\n\nE. Vector spaces\n\nRelate the abstract vector spaces \\(\\mathbb{R}^{m\\times n}\\) and \\(\\mathcal{P}_n\\) to \\(\\mathbb{R}^n\\).\nDetermine whether given vectors in an abstract space are linearly independent.\nDetermine whether a given vector lies in the span of other vectors in an abstract space.\nCompute the coordinates of a given vector in a given basis.\nCompute a change-of-basis matrix between two given bases.\n\n\n\nF. Linear ODE systems\n\nWrite a linear ODE system in matrix-vector form.\nDescribe the properties of a fundamental matrix.\nCompute a Wronskian and use it to determine the independence of solutions.\nSolve a constant-coefficient homogeneous system by eigenvalues and eigenvectors (nondefective case).\nReplace a complex homogeneous solution form by a real form.\nDeduce the stability and character of the origin in a homogeneous linear system from its eigenvalues or a phase plane diagram.\nCompute a matrix exponential using a fundamental matrix.\nCompute a matrix exponential using a diagonalization.\nCompute a 2x2 or 3x3 matrix exponential using the polynomial method (including defective case).\nUse a matrix exponential to solve an IVP."
  },
  {
    "objectID": "ivp_first_order.html#sec-first-order-preview",
    "href": "ivp_first_order.html#sec-first-order-preview",
    "title": "1  First-order IVPs",
    "section": "1.1 Case studies",
    "text": "1.1 Case studies\nBefore getting bogged down with generalities, let’s dive in to some illustrative archetypes of first-order equations. Each is simple enough to understand in detail, and yet they illustrate aspects of things we will encounter all the time.\n\n1.1.1 Constant change\nYou won’t find an ODE easier than\n\\[\n\\dd{x}{t} = a,\n\\tag{1.3}\\]\nwhere \\(a\\) is a constant. Every function of the form \\(x(t) = at + C\\), where \\(C\\) is an arbitrary constant, is a solution. That is, the ODE says the solution has a constant rate of change, and therefore the solution has a constant slope.\nThis problem is a special case of\n\\[\n\\dd{x}{t} = f(t).\n\\]\nUnlike in the general ODE Equation 1.1, the function \\(f\\) here does not depend on the unknown solution \\(x\\). Therefore, its solution is an indefinite integral: \\(x(t) = \\int f(t) \\, dt\\). As usual, the indefinite integration leaves an arbitrary constant.\n\n\n\n\n\n\nNote\n\n\n\nBecause solving ODEs encompasses standard integration as a special case, the process of solving an ODE is often referred to as integrating it.\n\n\nAs banal as Equation 1.3 is, it reveals an important fact:\n\n\n\n\n\n\nNote\n\n\n\nSolutions of ODEs are not necessarily unique.\n\n\nNow let’s move one rung up the ladder of complexity.\n\n\n1.1.2 Constant growth\nSuppose \\(x(t)\\) represents a quantity of something that reproduces itself at a constant rate. In addition to biological agents such as bacteria and infected individuals, this could be upvoted memes on a social network, for example. Mathematically we have a fixed growth rate per capita, i.e.,\n\\[\n\\frac{1}{x} \\dd{x}{t} = a\n\\]\nfor a positive constant \\(a\\). This gives the ODE\n\\[\n\\dd{x}{t} = ax \\qquad (a &gt; 0).\n\\tag{1.4}\\]\n\n\n\n\n\n\n\nTip\n\n\n\nA named quantity such as \\(a\\) that is neither the independent nor the dependent variable is often called a parameter. A parameter is assumed to be constant unless notation indicates otherwise. A common convention is to choose parameters to be positive where reasonable. If we needed a negative value, we would write \\(-a\\) with the understanding that \\(a\\) is positive.\n\n\nSuppose we define\n\\[\nx(t) = C e^{at}\n\\tag{1.5}\\]\nfor an unspecified constant \\(C\\). Then\n\\[\n\\dd{x}{t} = C\\cdot ae^{at} = a\\left( Ce^{at} \\right) = ax.\n\\]\nHence we have proved that Equation 1.5 is a solution of Equation 1.4. Note that \\(x\\) never changes sign, and \\(|x(t)|\\) grows without bound as \\(t\\to\\infty\\).\n\nThe solution Equation 1.5 again has an arbitrary constant. Unlike for Equation 1.3, however, this constant appears as a multiplier, not as an addend. It turns out that we will always get such a constant popping up in some form, which leads to the following definition:\n\nDefinition 1.3 The general solution of a first-order ODE is a family of functions, parameterized by \\(C\\), such that every member of the family for a particular value of \\(C\\) is a solution.\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s common to refer to the parameter \\(C\\) as an integration constant. In some problems, there may be restrictions placed on \\(C\\).\n\n\n\n\n1.1.3 Constant decay\nNow consider\n\\[\n\\dd{x}{t} = -ax  \\qquad (a &gt; 0).\n\\tag{1.6}\\]\nThis represents constant per capita decay, which is characteristic of radioactive isotopes and populations of organisms dying faster than they can reproduce. The general solution is\n\\[\nx(t) = C e^{-at},\n\\]\nwhich indicates exponential decay to zero as \\(t\\to\\infty\\).\n\nIn this and the growth archetype, the function \\(f\\) in Equation 1.1 depends only on the solution \\(x\\) and not explicitly on the independent variable \\(t\\). Such an ODE is called autonomous, time-invariant, or constant-coefficient:\n\\[\n\\dd{x}{t} = f(x).\n\\tag{1.7}\\]\n\n\n1.1.4 Variable growth/decay\nOur next stop is the ODE\n\\[\n\\dd{x}{t} = 2tx.\n\\tag{1.8}\\]\nBecause the formula for \\(\\dd{x}{t}\\) has a dependence on time, we say it is a non-autonomous equation, also termed time-varying or variable-coefficient. Comparing Equation 1.8 to constant growth, \\(x'=ax\\), we could say that the function \\(2t\\) now plays the role of the growth rate \\(a\\). In other words, this is a situation of accelerating growth. The general solution is\n\\[\nx(t) = C e^{t^2},\n\\tag{1.9}\\]\nwhich grows superexponentially as \\(t\\to\\infty\\).\n\nExample 1.3 Substituting Equation 1.9 into Equation 1.8 yields\n\\[\n\\begin{split}\n\\frac{d}{dt} \\left( C e^{t^2} \\right) &= 2t \\left(C e^{t^2} \\right), \\\\\nC \\left(2t e^{t^2} \\right) &= 2t \\left( C e^{t^2} \\right),\n\\end{split}\n\\]\nwhich is evidently true. Hence we have established Equation 1.9 as a solution for any constant \\(C\\).\n\nMore general variable growth looks like \\(x'=a(t)x\\). If \\(a\\) changes sign, the character of the equation flips between momentary growth and decay.\n\n\n\n1.1.5 Nonlinear growth\nAll of the examples so far in this section have an ODE in the form\n\\[\n\\dd{x}{t} = a(t) x + b(t),\n\\tag{1.10}\\]\nin which the dependent variable appears (if at all) just as itself to the first power. This is called a linear first-order ODE, because the dependence of \\(\\dd{x}{t}\\) on \\(x\\) is linear. It does not imply that the solution \\(x(t)\\) is a linear function of \\(t\\), but linear problems have a lot of structure that will make them relatively easy to understand.\nOur final archetype, by contrast, is nonlinear. Comparing\n\\[\n\\dd{x}{t} = x^2\n\\tag{1.11}\\]\nto constant growth \\(x'=ax\\), it is irresistible to interpret this equation as a growth process whose rate at any instant is the momentary value of the solution itself. This suggests a kind of runaway feedback loop.\nThe general solution is\n\\[\nx(t) = \\frac{1}{C-t}.\n\\tag{1.12}\\]\n\nExample 1.4 The derivative of Equation 1.12 leads to\n\\[\n\\dd{x}{t} = -(C-t)^{-2}\\cdot \\dd{}{t}(C-t) = (C-t)^{-2} = x^2.\n\\]\nThis proves we have a solution of Equation 1.11.\n\n\nNote that for \\(C&gt;0\\) in Equation 1.11, \\(x(t)\\to \\infty\\) as \\(t\\to C^{-1}\\). This is a finite-time blowup, which we did not observe in any of the linear growth processes.\n\n\n\n\n\n\nNote\n\n\n\nIs there any point to an ODE whose solution blows up in a finite time? Maybe. This particular ODE describes, for instance, the evolution of the slope of the line of sight to an airplane flying straight over you. When the airplane is directly overhead, the slope is infinite. So while the model becomes mathematically invalid at that moment, it does describe a concrete physical situation.\n\n\nThat concludes our bus tour of first-order equations. It’s time to get serious about solving some of them."
  },
  {
    "objectID": "ivp_first_order.html#initial-value-problems",
    "href": "ivp_first_order.html#initial-value-problems",
    "title": "1  First-order IVPs",
    "section": "1.2 Initial-value problems",
    "text": "1.2 Initial-value problems\nOne of the important findings in Section 1.1 is that solutions of first-order ODEs are not unique. The manifestation of the nonuniqueness is the presence of an integration constant, and we get a general solution that describes a family of functions that provide all possible solutions.\n\nIn scientific and engineering problems, we typically have an additional constraint that picks out a single member of the general solution family—a particular solution. Usually that constraint takes the form of a specified value,\n\\[\nx(t_0) = x_0,\n\\]\nwhere \\(t_0\\) and \\(x_0\\) are considered to be given. In this case the constraint \\(x(t_0)=x_0\\) is called an initial condition, and it’s usually understood that the problem is to be solved for \\(t&gt;t_0\\). Such a constraint combined with a first-order ODE leads to an initial-value problem:\n\nDefinition 1.4 (Initial-value problem, IVP) \\[\n\\dd{x}{t} = f(t,x), \\quad x(t_0) = x_0.\n\\tag{1.13}\\]\n\n\nA solution of an IVP has to satisfy both the ODE and the initial condition. This is (for our purposes) enough to specify the solution uniquely.\n\nExample 1.5 Solve the IVP\n\\[\n\\dd{x}{t} = ax, \\qquad x(2)=5.\n\\]\n\nSolution. In Section 1.1 we found that the general solution of \\(x'=ax\\) is \\(x(t)=Ce^{at}\\). If we are supplied with the initial value \\(x(2)=5\\), then we require\n\\[\n5 = x(2) = Ce^{2a},\n\\]\nin which case \\(C=5e^{-2a}\\). Thus the solution to this IVP is\n\\[\nx(t) = 5e^{-2a}\\cdot e^{at} = 5e^{a(t-2)}.\n\\]\n\n\nA graphical interpretation of the role of an initial condition is simple: the general solution is a family of curves in the \\((t,x)\\) plane, and the initial condition is a point in the plane that the one particular solution of interest must pass through.\n\nExample 1.6 Here is an illustration of how an initial condition \\(x(3)=20\\) selects one solution of \\(x'=1.25x\\).\n\n\nCode\na = 1.25\nplt = plot(xaxis=(\"t\"),yaxis=(\"x\",(-60,60)))\nfor C in (-3:3)/3\n    plot!(t -&gt; C*exp(a*t),1,4)\nend\n\nscatter!([3],[20],m=(8,:black))\nplot!(t-&gt;20exp(a*(t-3)),1,4,l=(2.5,:black))\ntitle!(\"Picking the solution with x(3)=20\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn standard problems, at most one particular solution can pass through a given point \\((t_0,x_0)\\). Otherwise, the state of the system for \\(t &gt; t_0\\) would seem to be ambiguous.\n\n1.2.1 Numerical solutions\nAn initial-value problem has a unique solution, which makes it a suitable target for a numerical approximation. This is how virtually all IVPs are solved in practice.\n\nExample 1.7 (Constant growth) Here’s a numerical approximation in Julia for the ODE of constant growth \\(x'=2x\\), subject to \\(x(1)=3\\):\n\n\nCode\nf = (x,p,t) -&gt; 2x\nivp = ODEProblem(f,3.,(1.,5.))\nx = solve(ivp)\nplot(x,label=\"\",xlabel=\"t\",ylabel=\"x\",title=\"Constant growth rate\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExponential growth or decay is best plotted on a log-linear scale, where the solution becomes a straight line:\n\n\nCode\nplot(x,label=\"\",xaxis=\"t\",yaxis=(\"x\",:log10),title=\"Constant growth rate (log scale)\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1.8 (Variable growth) Here’s our example of variable growth. Note that we are not using the known exact solution, but just letting Julia create a numerical approximation by other means.\n\n\nCode\nf = (x,p,t) -&gt; 2t*x\nivp = ODEProblem(f,1.,(0.,5.))\nx = solve(ivp)\nplot(x,label=\"\",xlabel=\"t\",yaxis=(\"x\",:log10),title=\"Growing growth rate\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEven on the log scale, the solution bends upward, showing superexponential growth.\n\n\nExample 1.9 (Nonlinear) Finally, here is a numerical solution of the nonlinear archetype problem \\(x'=x^2\\), \\(x(0)=0.5\\), for \\(0 \\le t \\le 4\\):\n\n\nCode\nf = (x,p,t) -&gt; x^2\nivp = ODEProblem(f,0.5,(0.,4.))\nx = solve(ivp)\nplot(x,label=\"\",xlabel=\"t\",yaxis=(\"x\",:log10),title=\"Nonlinear growth\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe warning issued by Julia above can mean that there is a bug in the code, but in this case, it’s just Julia noticing the finite-time blowup. In fact, it gets the true blowup time rather accurately."
  },
  {
    "objectID": "ivp_first_order.html#qualitative-methods",
    "href": "ivp_first_order.html#qualitative-methods",
    "title": "1  First-order IVPs",
    "section": "1.3 Qualitative methods",
    "text": "1.3 Qualitative methods\nExact solutions of ODEs are few and far-between. In addition to numerical solutions, there are a few tools that can be used to gain insight about an ODE without knowing or producing a solution. These are grouped together under the name of qualitative methods.\n\n1.3.1 Direction field\nIn the scalar equation \\(x'=f(t,x)\\), the function \\(f\\) gives the slope of any solution at any point in the \\((t,x)\\) plane. Hence, while it is usually not trivial to draw curves for the solutions, it is straightforward to draw the instantaneous slopes of them. Here is a basic Julia function for it.\n\n\nCode\nfunction dirfield(f,tlims,xlims)\n    t = range(tlims...,length=16)\n    x = range(xlims...,length=16)\n    T = vec([t for t in t, x in x])\n    X = vec([x for t in t, x in x])\n    F = f.(T,X)\n\n    avg = sum( @. sqrt(F^2+1) ) / length(F)\n    scale = 0.05*max(tlims[2]-tlims[1],xlims[2]-xlims[1])/avg\n    quiver( T,X,quiver=(ones(size(F))*scale,F*scale) )\nend;\n\n\n\nExample 1.10 The logistic equation is\n\\[\n\\dd{x}{t} = ax-bx^2,\n\\tag{1.14}\\]\nwhere \\(a\\) and \\(b\\) are positive parameters. For the case \\(a=3\\), \\(b=2\\), its direction field is\n\n\nCode\nf = (t, x) -&gt; 3x - 2x^2\ndirfield(f, [0, 2], [0, 1.6])\ntitle!(\"Direction field of 3x-2x²\")\nxlabel!(\"t\");  ylabel!(\"x\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that at \\(x=0\\), the arrows are purely horizontal. That’s because the function \\(x(t)=0\\) is a solution.\nEvery plot of a solution will have the arrows as tangents to it. For instance, we will add a few solution curves to the direction field above:\n\n\nCode\nfor x0 in [0.2, 0.6, 1.0]\n  ivp = ODEProblem((x, _, t) -&gt; 3x - 2x^2, x0, (0., 2.))\n  x = solve(ivp)\n  plot!(x.t, x.u, l=2)\nend\nplot!()\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1.11 Here is a direction field for \\(x'=t-x^2\\).\n\n\nCode\nf = (t, x) -&gt; t - x^2\ndirfield(f, [-1,2], [-2,2])\ntitle!(\"Direction field of t-x²\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the arrows are horizontal along the sideways parabola \\(t=x^2\\), because that is where the slope is zero.\n\n\n\n1.3.2 Equilibria\nMany problems describe a self-contained system. For example, a small pendulum operates the same regardless of what time of day you first push on it.\n\nDefinition 1.5 (Autonomous ODE) An autonomous first-order ODE is an equation of the form\n\\[\n\\dd{x}{t}=f(x).\n\\tag{1.15}\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nAn ODE is autonomous if, and only if, its direction field looks the same along every vertical line in the \\((t,x)\\) plane. This can be seen, for example, in Example 1.10.\n\n\nSuppose that \\(\\hat{x}\\) is a root of the function \\(f\\) in Equation 1.15, that is, it satisfies \\(f(\\hat{x})=0\\). Then the constant function \\(x(t)\\equiv \\hat{x}\\) a solution of Equation 1.15.\n\nDefinition 1.6 (Steady state / Equilibrium) For the autonomous ODE \\(x'=f(x)\\), any root \\(\\hat{x}\\) of \\(f\\) is called a steady state, equilibrium value, fixed point, or critical point.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you see one concept with a lot of different names, pay attention. It was important enough to have been discovered multiple times.\n\n\nOne of the most important characteristics of each steady state is its stability. Imagine slicing a globe in half and laying a marble at the bottom inside the southern hemisphere. That marble should lay still in an equilibrium state. If you nudge the marble a little, it will oscillate but eventually come back to rest at the bottom of the hemisphere. We call this an asymptotically stable equilibrium—if the system is perturbed gently enough, solutions return to the equilibrium.\nNow suppose you put down the northern hemisphere with the equator on the surface of a table. If you can place the marble perfectly at the north pole, it should stay there, in principle; it should experience a purely downward force. In practice, though, this is probably impossible to achieve, because any deviation from perfect symmetry in the globe, marble, or placement will cause the marble to roll away from the pole. We call this pole an unstable equilibrium.\n(There are other classes of equilibria, but they are much less frequently encountered.)\nThe major difference between asymptotically stable and unstable equilibria in the real world is that usually, only the asymptotically stable ones are observable. It’s easy to dangle a broom downward practically forever using just two fingers, but try that with the broom pointing upward!\nIt’s usually quite easy to determine the stability of an equilibrium point without finding solutions.\n\nExample 1.12 Consider the ODE \\(x'=x-x^3\\). We will start by graphing \\(f(x)=x-x^3\\).\n\nThe equilibrium solutions occur at the crossings of the \\(x\\)-axis in the plot, where \\(dx/dt=0\\).\n\nNow we highlight those parts of the plot where the graph is above the \\(x\\)-axis, i.e., where \\(dx/dt &gt; 0\\).\n\nImagine that the solution is represented by a point \\(x(t)\\) sliding along the \\(x\\)-axis. Under the highlighted regions, this point has to be sliding rightward, because \\(x'&gt;0\\) there.\n\nSimilarly, on the remaining segments of the \\(x\\)-axis, the solution must be sliding leftward.\n\nGraphically it is now clear that the steady states at \\(x=\\pm 1\\) are asymptotically stable, while the one at \\(x=0\\) is unstable.\n\nIf the function \\(f\\) is differentiable, there is also a simple analytical test:\n\nIf \\(f'(\\hat{x})&lt;0\\), then \\(\\hat{x}\\) is asymptotically stable.\nIf \\(f'(\\hat{x})&gt;0\\), then \\(\\hat{x}\\) is unstable.\n\nAs you might surmise, the case \\(f'(\\hat{x})=0\\) is ambiguous, and additional analysis is needed to reveal the stability."
  },
  {
    "objectID": "ivp_first_order.html#separable-equations",
    "href": "ivp_first_order.html#separable-equations",
    "title": "1  First-order IVPs",
    "section": "1.4 Separable equations",
    "text": "1.4 Separable equations\n\nAll first-order problems in the autonomous form \\(x'=f(x)\\), and more generally problems in the form\n\\[\nx' = f(x,t) = g(x) h(t),\n\\]\nare called separable equations and can be solved systematically, up to performing integrations. The process is more convenient to describe if we instead suppose that\n\\[\n\\dd{x}{t} = \\frac{h(t)}{g(x)},\n\\]\nwhich is equivalent (only changing how \\(g\\) is defined). Ignoring formalities, we treat \\(dx/dt\\) as though it’s a fraction and rewrite this as\n\\[\ng(x)\\, dx = h(t)\\, dt.\n\\]\n(That step will give a pure mathematician the vapors, but we soldier on.) In principle, all we need to do is integrate both sides:\n\\[\n\\int g(x)\\, dx = \\int h(t)\\, dt \\quad \\Rightarrow \\quad G(x) = H(t) + C,\n\\]\nwhere \\(C\\) is the integration constant. (Having an integration constant on both sides would not change anything.)\nThis description is all rather generic. The process is very easy to grasp from following a few examples.\n\nExample 1.13 Solve the variable growth archetype ODE, \\(x'=2t x\\).\n\nSolution. We express \\(x'\\) as \\(dx/dt\\) and then isolate the variables:\n\\[\n\\frac{d x}{x} = 2t\\,d t.\n\\]\nIntegrating both sides leads to \\(\\ln |x| = t^2 + C\\), or\n\\[\n|x(t)|=A e^{t^2},\n\\]\nwhere \\(A\\) must be positive, since it is the exponential of a real \\(C\\). Taking the absolute value off of \\(x\\), however, is the same as allowing \\(A\\) to be negative as well. Note also that \\(A=0\\) clearly leads to the constant solution \\(x(t)\\equiv 0\\). Therefore, we finally conclude that \\(x=B e^{t^2}\\) for an arbitrary \\(B\\).\n\n\nIn Example 1.13, the first step involved dividing the equation through by \\(x\\). This implies that we made \\(x=0\\) impossible to achieve. That’s why we had to check it manually at the very end. In the general separable case of \\(x'=g(x)h(t)\\), we first divide through by \\(g(x)\\), so the last step is to reintroduce each root \\(\\hat{x}\\) of \\(g\\) as an equilibrium solution.\n\nExample 1.14 Solve \\(x'=t^2/(x^3-2)\\), subject to \\(x(0)=4\\).\n\nSolution. Separation and integration lead to\n\\[\n\\int x^3\\, dx = \\int t^2\\, dt,\n\\]\nor\n\\[\n\\frac{1}{4}x^4 - 2x  = C + \\frac{1}{3}t^3.\n\\]\nSince we did not divide through by anything, there are no additional equilibrium solutions to include.\nWe could work hard to try to solve the final expression explicitly for \\(x\\), but it’s best to leave it in implicit form. This is a common limitation of separable solutions. Even in implicit form, though, we can apply an initial condition to obtain a particular solution. If we add the condition \\(x(0)=4\\), for instance, then we set \\(x=4\\) and \\(t=0\\) in the implicit equation to get\n\\[\n\\frac{4^4}{4} - 2 \\cdot 4 = C + 0.\n\\]\nHence \\(C=56\\).\n\n\nSometimes the separable structure isn’t immediately apparent, and you have to manipulate the expressions before you can see it.\n\nExample 1.15 Find the general solution of \\(t x' = x - t x\\).\n\nSolution. Nothing happens until you see that you can factor out \\(x\\) on the right side to get\n\\[\n\\dd{x}{t} = \\frac{x(1-t)}{t},\n\\]\nwhich is in a separated form. Then\n\\[\n\\frac{dx}{x} = (t^{-1}-1)\\,dt,\n\\]\nand thus \\(\\ln|x| = \\ln|t|-t+C\\), or\n\\[\n|x| = e^C |t| e^{-t}.\n\\]\nOnce we take off the absolute values, and defining \\(A=e^C\\), we have\n\\[\nx= \\pm A t e^{-t} \\quad (A&gt;0).\n\\]\nWe divided the ODE through by \\(x\\) and therefore have to include \\(x(t) \\equiv 0\\) as an equilibrium solution. This is easily expressed by noting that \\(\\pm A\\) can actually be any real number, so we can write the general solution as\n\\[\nx= C t e^{-t}.\n\\]\n\n\nNote that an autonomous equation is, by definition, separable.\n\nExample 1.16 Solve the autonomous ODE \\(x'=\\csc x\\).\n\nSolution. Multiplying through by \\(\\sin x\\), we obtain \\(\\sin x\\, dx = dt\\), so integration produces\n\\[\n-\\cos(x) = t + C.\n\\]\nSince \\(|\\cos(x)| \\le 1\\), this implies a significant restriction relating \\(t\\) and \\(C\\)."
  },
  {
    "objectID": "ivp_first_order.html#linear-equations",
    "href": "ivp_first_order.html#linear-equations",
    "title": "1  First-order IVPs",
    "section": "1.5 Linear equations",
    "text": "1.5 Linear equations\nWe now begin a close look at the linear case.\n\n\n\n\n\n\nNote\n\n\n\nLinear problems are pretty much all we consider from this point on.\n\n\n\n\n\nDefinition 1.7 (First-order linear ODE) A first-order linear ODE is an equation of the form\n\\[\na_1(t)\\dd{x}{t} + a_0(t) x = g(t).\n\\tag{1.16}\\] The standard form of such a problem is\n\\[\n\\dd{x}{t} + P(t) x = f(t).\n\\tag{1.17}\\]\nWe call \\(f(t)\\) the forcing function and \\(P(t)\\) the coefficient of the ODE.\n\n\nA linear ODE has a linear dependence on the dependent variable \\(x\\). It may have arbitrary dependence on the independent variable \\(t\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nNever overlook the \\(a_1(t)\\) in Equation 1.16. If you forget to divide through by it to get the standard form, everything that follows will be wrong.\n\n\n\n1.5.1 Operator notation\nThere is an alternate notation for Equation 1.17 that will help us highlight important properties of linearity as we go.\n\nDefinition 1.8 (Linear operator) A linear operator \\(\\opA\\) is a rule for transforming functions to other functions, such that\n\\[\\begin{split}\n\\opA[cx(t)] & =c\\opA[x(t)], \\\\\n\\opA[x(t) + y(t)] &= \\opA[x(t)] + \\opA[y(t)],\n\\end{split}\\]\nfor all functions \\(x\\) and \\(y\\) and numbers \\(c\\).\n\nIn the present context we are interested in the linear operator\n\\[\n\\opA[x] = \\dd{x}{t} + P(t)x.\n\\tag{1.18}\\]\nWe can now express the ODE Equation 1.16 abstractly as\n\\[\n\\opA[x]=f.\n\\]\n\nExample 1.17 The equation\n\\[\nt x' = \\sin(t) - x\n\\]\nis linear. To write it in operator form, we rearrange to\n\\[\nx' + \\frac{1}{t}x = \\frac{\\sin(t)}{t}.\n\\]\nThus the linear operator for this ODE is \\(\\opA[x]=x' + \\tfrac{1}{t}x\\), and the ODE is\n\\[\n\\opA[x] =  \\frac{\\sin(t)}{t}.\n\\]\n\n\n\n1.5.2 Linear homogeneous equations\n\nThe equation \\(\\opA[x]=0\\), or Equation 1.17 with forcing function \\(f\\) set to zero, plays a key role.\n\nDefinition 1.9 (Homogeneous ODE) A linear homogeneous ODE takes the form\n\\[\n\\dd{x}{t} + P(t)x = 0,\n\\]\nor \\(\\opA[x]=0\\).\n\n\nLinear homogeneous problems have a key property that allows turning individual solutions into sets of them.\n\nTheorem 1.1 (Superposition) If \\(x_1(t), x_2(t),\\ldots x_k(t)\\) are solutions of \\(\\opA[x]=0\\), then so is any linear combination \\(c_1x_1 + \\cdots + c_kx_k\\) for constants \\(c_j\\).\n\n\nProof. Because of linearity we can write\n\\[\n\\opA[c_1x_1 + \\cdots + c_kx_k] = c_1\\opA[x_1] + \\cdots + c_k\\opA[x_k].\n\\]\nBy assumption, each \\(\\opA[x_j]=0\\). So the sum on the right is zero as well.\n\n\nCorollary 1.1 If \\(x(t)\\) is a solution of \\(\\opA[x]=0\\), then so is \\(c x(t)\\) for an arbitrary constant \\(c\\).\n\n\n\n1.5.3 General solutions\nThe following fact shows that the problem \\(\\opA[x]=f\\) with nonzero forcing \\(f\\) still brings the homogeneous case along with it.\n\n\nTheorem 1.2 All solutions of \\(\\opA[x]=f\\) may be written as\n\\[\nx(t) = x_h(t) + x_p(t),\n\\] where \\(x_h\\) is the general solution of \\(\\opA[x]=0\\) and \\(x_p\\) is any one particular solution of \\(\\opA[x]=f\\). We call this the general solution of the linear ODE.\n\nWe can now outline a plan of attack for solving \\(\\opA[x]=f\\).\n\nTheorem 1.3 (Solution of a first-order linear ODE) Follow these steps to find the general solution of the linear first-order ODE \\(\\opA[x]=f\\): 1. Find the general solution \\(x_h\\) of the associated homogeneous problem \\(\\opA[x]=0\\). 2. Find any particular solution \\(x_p\\) of the original \\(\\opA[x]=f\\). 3. Add them to get the general solution, \\(x=x_h + x_p\\). 4. If an initial condition is given, use it to solve for the integration constant.\n\nNote that because of Theorem 1.1, the homogeneous part \\(x_h\\) will contain the integration constant.\nThe next two sections provide the details for steps 1 and 2 of Theorem 1.3."
  },
  {
    "objectID": "ivp_first_order.html#sec-first-order-homogeneous1",
    "href": "ivp_first_order.html#sec-first-order-homogeneous1",
    "title": "1  First-order IVPs",
    "section": "1.6 Homogeneous solutions",
    "text": "1.6 Homogeneous solutions\nStep 1 of our solution strategy in Theorem 1.3 is to solve the homogeneous problem \\(\\opA[x]=0\\), or\n\\[\n\\dd{x}{t} + P(t) x = 0.\n\\tag{1.19}\\]\nWe know that the solution of \\(x=ax\\) for constant \\(a\\) is \\(x(t)=\\exp(at)\\). The solution is exponential in this more general case as well:\n\\[\nx(t) = \\exp\\bigl[q(t)\\bigr],\n\\]\nfor some \\(q(t)\\) yet to be determined. Note that\n\\[\nx'(t) = q'(t) e^{\\,q(t)} = q'(t) x.\n\\]\nIf this is to be a solution of Equation 1.19, all we need is that \\(q'(t)=-P(t)\\). This is solved by simple integration. To summarize:\n\nTheorem 1.4 (Solution of \\(x'+P(t)x=0\\)) To solve the linear first-order homogeneous ODE \\(\\opA[x]=0\\), also written as Equation 1.19, perform the following steps, ignoring constants of integration: \\[\n\\begin{split}\nq(t) &= - \\int P(t) \\, dt, \\\\\nx_1(t) &= \\exp\\bigl[q(t)\\bigr] \\\\\nx(t) &= c_1 x_1(t).\n\\end{split}\n\\tag{1.20}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIn our terminology, \\(x_1\\) in Equation 1.20 is a particular solution of \\(\\opA[x]=0\\), while \\(x\\) is the general solution of \\(\\opA[x]=0\\) as well as the homogenous part of the solution associated with \\(\\opA[x]=f\\) in Theorem 1.3. Got that?\n\n\nIf you’re anxious about having \\(c_1\\) and \\(x_1\\) in Equation 1.20 without also having \\(c_2\\) and \\(x_2\\)—if you know, you know—just sit tight until the next chapter.\n\nExample 1.18 Solve \\(x'=\\sin(t) x\\).\n\nSolution. By comparison to the standard form Equation 1.19, we find that \\(P(t)=-\\sin(t)\\). Hence\n\\[\nq(t) = - \\int -\\sin(t)\\, dt = - \\cos(t).\n\\]\nThe general solution is thus\n\\[\nx(t) = c_1 \\exp[ -\\cos(t) ].\n\\]\n\n\n\nExample 1.19 Solve the homogeneous IVP\n\\[\nt x' = 2x, \\quad x(2) = 12.\n\\]\n\nSolution. First we rewrite the ODE in standard form as\n\\[\nx' - \\frac{2x}{t} = 0,\n\\]\nfrom which we see that \\(P(t)=-2/t\\). We first find the general solution by integration:\n\\[\nx(t) = c_1 \\exp\\left[ \\int 2t^{-1}\\, dt \\right] = c_1 \\exp\\bigl[2\\ln(t)\\bigr] = c_1 t^2.\n\\]\nTo eliminate the integration constant, we apply the initial condition:\n\\[\n12 = x_h(2) = c_1\\cdot 2^2.\n\\]\nHence \\(x(t) = 3t^2\\).\n\n\n\n1.6.1 Singularities\nIn Example 1.19, the standard form of the ODE had \\(P(t)=-2/t\\) as the coefficient of \\(x\\). Because \\(P(0)\\) is undefined, we call \\(t=0\\) a singular point of the ODE. It is possible for the general solution to be undefined at a singular point. That did not happen in Example 1.19, but here is a similar problem where it does.\n\nExample 1.20 The homogeneous equation\n\\[\nx' + \\frac{2x}{t} = 0\n\\]\nhas \\(P(t)=2/t\\), so\n\\[\nq(t) = - \\int \\frac{2}{t} \\, dt = -2 \\ln(t),\n\\]\nwhich gives the homogeneous solution\n\\[\nx_1(t) = e^{q(t)} = t^{-2}.\n\\]\nThis solution fails to exist at \\(t=0\\) because of the singular point implied by \\(P\\)."
  },
  {
    "objectID": "ivp_first_order.html#sec-first-order-vop",
    "href": "ivp_first_order.html#sec-first-order-vop",
    "title": "1  First-order IVPs",
    "section": "1.7 Variation of parameters",
    "text": "1.7 Variation of parameters\nSection 1.6 explained how to find the general solution \\(x_h(t)\\) of a homogeneous linear system \\(\\opA[x] = 0\\). The next step of our solution strategy in Theorem 1.3 is to find some particular solution of \\(\\opA[x]=f\\) for the given \\(f(t)\\). After that, the general solution is in our hands.\nThe form of the homogeneous solution is\n\\[\nx_h(t) = c_1 e^{q(t)},\n\\]\nwhere \\(c_1\\) is an arbitrary constant and \\(q = -\\int P\\, dt\\). Something weird and useful happens if we upgrade \\(c_1\\) to an unknown function \\(k(t)\\). Setting \\(x_p(t)=k(t) e^{q(t)}\\), then\n\\[\n\\begin{split}\n\\opA[x_p] &= x_p' + P(t) x_p \\\\\n& = k'e^q + k (e^q q') + P k e^q  \\\\\n& = e^q ( k'+ q'k + Pk ) \\\\\n&= e^q (k' + (-P)k + Pk) = e^q k'.\n\\end{split}\n\\]\nSo, to solve \\(\\opA[x_p] = f\\), we can set \\(e^q k' = f\\). Since \\(f\\) is considered given and we already know how to compute \\(q\\), this can be considered a way to define \\(k'\\), and then \\(k\\) is just one more integration away.\nThis technique for finding \\(x_p\\) is known as the variation of parameters formula, or VoP formula for short. We present it here along with the rest of the steps in Theorem 1.3 as a complete method to find the general solution.\n\nTheorem 1.5 (Variation of parameters (VoP)) To find the general solution of \\(\\opA[x]=f\\), compute the following, ignoring constants of integration:\n\\[\n\\begin{split}\nq(t) &= -\\int P(t)\\, dt, \\\\\nx_1(t) &= e^{q(t)}, \\\\\nk(t) &= \\int \\frac{f(t)}{x_1(t)} \\, dt, \\\\\nx_p(t) &= k(t) x_1(t), \\\\\nx(t) &= c_1 x_1(t) + x_p(t).\n\\end{split}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nKeeping track of all the characters appearing in Theorem 1.5 is as difficult as following Game of Thrones. To remind you: \\(x_1(t)\\) is a solution of the associated homogeneous equation \\(\\opA[x]=0\\), \\(c_1 x_1(t)\\) is the general solution of that problem, \\(x_p(t)\\) is one particular solution of \\(\\opA[x]=f\\), and the final \\(x(t)\\) is the general solution of \\(\\opA[x]=f\\).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe unstated setup step before Theorem 1.5 can even be applied is to put a given linear ODE into the standard form, \\(x' + P(t) x = f\\). On this, everything depends.\n\n\n\nExample 1.21 Solve \\(3x'=12x+24t\\).\n\nSolution. Rewriting the ODE in standard form as \\(x'-4x=8t\\), we identify \\(P(t)=-4\\) and \\(f(t)=8t\\). Then\n\\[\nq(t) = - \\int -4\\,dt = 4t,\n\\]\nand\n\\[\nx_1(t) = e^{4t},\n\\]\nand\n\\[\nk(t) = \\int \\frac{8t}{e^{4t}}\\, dt = -\\frac{1}{2} (4t+1) e^{-4t},\n\\]\nwhere you (or your computer) use integration by parts to perform the integral. We continue to\n\\[\nx_p(t) = x_1(t) k(t) = -\\frac{1}{2} (4t+1),\n\\]\nso that finally, we have the general solution\n\\[\nx(t) =  c_1 e^{4t} - \\frac{1}{2} (4t+1).\n\\]\n\n\n\nExample 1.22 Find a particular solution of \\(x'= 2t x + 6t\\).\n\nSolution. This problem is deliberately worded sneakily to see if you have been paying attention. The function \\(x_p(t)\\) in Theorem 1.5 is all we need. First, note that \\(\\opA[x]=x'-2tx\\) and \\(f(t)=6t\\). Then \\(P(t)=-2t\\), so\n\\[\nq(t) = - \\int -2t\\, dt = t^2,\n\\]\nand\n\\[\nx_1(t) = e^{t^2}.\n\\]\nThus,\n\\[\nk(t) = \\int \\frac{6t}{e^{t^2}}\\, dt = \\int 6t e^{-t^2}\\, dt = -3 e^{-t^2},\n\\]\nleading to\n\\[\nx_p(t) = k(t) x_1(t) = -3.\n\\]\n(Technically, this problem has infinitely many solutions. Any solution of the ODE is a particular solution of it. So you could freeze \\(c_1=1\\) in Theorem 1.5 and present \\(e^{t^2} - 3\\) as a particular solution. You’d be right, but you’d also be a little annoying.)\n\n\n\n\n\n\n\n\nTip\n\n\n\nFirst-order linear ODEs can also be separable! In Example 1.22 we could have started with writing\n\\[\n\\dd{x}{t} = 2t(x + 3),\n\\]\non which separation gives \\(\\ln|x+3| = t^2 + C\\). For a particular solution, we can use any \\(C\\) we want, and \\(C=0\\) is the easiest. This leads to \\(x(t) = e^{t^2} - 3\\), which is a much less annoying way to derive the alternative solution that was given.\n\n\n\nExample 1.23 Solve the IVP\n\\[\n(2+t) x'= x - 1, \\quad x(0) = -5.\n\\]\n\nSolution. First we put the ODE into standard form,\n\\[\nx' - \\frac{1}{2+t} x = -\\frac{1}{2+t}.\n\\]\nNote that \\(P(t) = -1/(2+t)\\) and \\(f(t)=-1/(2+t)\\). Next,\n\\[\nq(t) = - \\int - \\frac{1}{2+t}\\, dt = \\ln|2+t|.\n\\]\nTaking the point of view that \\(t\\ge 0\\) is implied by the initial condition, we observe that \\(|2+t|=2+t\\). Hence\n\\[\nx_1(t) = \\exp[ \\ln(2+t) ] = 2+t.\n\\]\nNext,\n\\[\n\\begin{split}\nk(t) & = \\int - \\frac{1}{2+t} \\cdot \\frac{1}{2+t} \\, dt \\\\\n&= -\\int (2+t)^{-2} \\, dt \\\\\n&= (2+t)^{-1}.\n\\end{split}\n\\]\nThus, \\(x_p(t)=k(t)x_1(t)=1\\) and the general solution is\n\\[\nx(t) = c_1(2+t) + 1.\n\\]\nFinally, we apply the initial condition to solve for \\(c_1\\):\n\\[\n-5 = x(0) = 2c_1+1 \\quad \\implies \\quad c_1=-3.\n\\]\nHence \\(x(t) = 1-3(2+t) = -5-3t.\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt takes a rather special relationship between \\(P(t)\\) and \\(f(t)\\) to make all the integrals in the VoP formula reasonable to do by hand. So, if you are solving an exercise and find yourself faced with an impossible or really ugly integral, you likely have made some earlier mistake."
  },
  {
    "objectID": "ivp_first_order.html#integrating-factor",
    "href": "ivp_first_order.html#integrating-factor",
    "title": "1  First-order IVPs",
    "section": "1.8 Integrating factor",
    "text": "1.8 Integrating factor\n\n\n\n\n\n\nWarning\n\n\n\nYou should probably skip this section. You don’t need it.\n\n\nTextbooks be like:\n\nA different process for arriving at the solution to a linear first-order ODE goes by the name of the integrating factor. Mathematically and computationally, it’s identical to Theorem 1.3, but organized slightly differently.\nThe ODE is again\n\\[\nx'(t) + P(t) x(t) = f(t).\n\\]\nDefine the integrating factor\n\\[\n\\mu(t) = \\exp \\left[ \\int P(t)\\, dt \\right].\n\\]\n(This is just \\(1/q(t)\\) as defined in Equation 1.20.) When you multiply the entire ODE through by \\(\\mu(t)\\), the left-hand side of the equation can be simplified by the product rule and chain rule for derivatives:\n\\[\n\\begin{split}\n\\dd{}{t} \\left[ \\mu(t) x(t) \\right] &= \\mu x' + \\mu' x \\\\\n&= \\mu x' + \\left(P \\mu \\right) x \\\\\n&= \\mu(t) f(t).\n\\end{split}\n\\]\nNow integrate both sides:\n\\[\n\\mu(t) x(t) = \\int \\mu(t) f(t)\\, dt + c_1.\n\\]\nSolve for \\(x\\), and you are finished—assuming you can do the integrals.\nThere is nothing new going on here, really. You do the same two integrals that make up Theorem 1.3, it works out for exactly the same reason, and it gives an equivalent result. The integrating factor involves a bit less notation, but the notation we developed supports machinery you will learn in the next chapter, when the integrating factor is no longer an option."
  },
  {
    "objectID": "ivp_first_order.html#modeling",
    "href": "ivp_first_order.html#modeling",
    "title": "1  First-order IVPs",
    "section": "1.9 Modeling",
    "text": "1.9 Modeling\nFirst-order ODEs are often used to model situations of growth and decay. The linear problem \\(x'=ax+f(t)\\) is a prototype for many important applications:\n\nPopulation\n\nNumber \\(x(t)\\) of organisms has a constant net per-capita growth/decay rate.\n\nInterest\n\nMoney amount \\(x(t)\\) grows at fixed positive interest rate \\(a\\) (\\(a&gt;0\\)).\n\nRadioactivity\n\nMass \\(x(t)\\) of a radioactive isotope decays (\\(a &lt; 0\\)).\n\nPharmacokinetics\n\nAmount or concentration \\(x(t)\\) of a drug being metabolized in the body (\\(a&lt;0\\)).\n\nNewtonian cooling or heating\n\nTemperature difference \\(x(t)\\) between object and environment decays to zero (\\(a&lt;0\\)).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn practice, interest rates may be quoted yearly, or quarterly, or according to any other finite time period. The rate in an ODE model is the continuously compounded rate.\n\n\nMathematically, positive \\(a\\) represents growth and negative \\(a\\) leads to decay. In the simplest models, \\(a\\) is constant.\nThe units of \\(dx/dt\\) are those of \\(x\\) divided by those of \\(t\\). Let’s write these as \\(X/T\\). Added terms all need to have the same units, so both \\(f(t)\\) and \\(a(t)x\\) have those units as well. Consequently the rate constant \\(a\\) has units \\(1/T\\). Since we find it easy to think in terms of time, many applications refer to \\(1/a\\) in some form:\n\nWhen \\(a &lt; 0\\), the time \\(\\tau=-1/a\\) may be the relaxation time or characteristic time. If there is no forcing, then \\(x(\\tau)= e^{-1} x(0) \\approx 0.37 x(0)\\).\nIn radioactivity it’s common to use \\[\nt_h=-\\frac{\\ln(2)}{a},\n\\] which is the half-life. That’s because \\(\\exp(at_h)=1/2\\), so half of the radioactive isotope is depleted in that much time.\nSimilarly, in population or another growth situation with \\(a &gt; 0\\), the time \\[\nt_D= \\frac{\\ln(2)}{a}\n\\] is the doubling time.\n\n\n\n\n\n\n\nWarning\n\n\n\nBanish linear extrapolation from your thinking! Here are examples with incorrect deductions highlighted:\n\nIf the half-life of an element is 10 years, then half of a sample will be gone in 10 years, and the other half will be gone after another 10 years.\nSince there were 24 ducks at the lake last year, and there are 36 of them now, by this time next year there will be 48 ducks.\nMy coffee cooled by 5 degrees in the last 15 minutes, so it will cool by another 5 degrees in the next 15 minutes.\n\n\n\nIn a modeling word problem, the game is to use given data to figure out all of the parameters that determine the dynamics. For instance, a population \\(x(t)\\) satisfying \\(x'=ax\\), \\(x(0)=p\\) is known to obey \\(x(t)=p e^{at}\\), which is determined completely by the values of \\(a\\) and \\(p\\). Hence we need to find two independent pieces of information in order to fully deduce the dynamics at any time. Keep in mind that knowing half-life or doubling time is equivalent to knowing the rate constant \\(a\\).\n\n1.9.1 Population dynamics\n\nExample 1.24 The population of a bacterial culture grows at a rate proportional to the number of bacteria present. You observe that there are 300 bacteria after 1 hour and 1000 after another 3 hours. How many bacteria (to the nearest whole number) were initially present?\n\nSolution. The first statement affirms that we are dealing with an ODE in the form\n\\[\n\\dd{x}{t} = a x,\n\\]\nwhich means that \\(x(t)=x(0)e^{at}\\). We are also told that \\(x(1)=300\\) and \\(x(4)=1000\\), so\n\\[\n\\begin{split}\n300 &= x(0) e^{a} \\\\\n1000 &= x(0) e^{4a}.\n\\end{split}\n\\]\nEverything from here on is pure algebra. We aren’t asked to find the value of \\(a\\), so we can be a little clever by substituting from the first equation into the second:\n\\[\n1000 = x(0) (e^a)^4 = x(0) \\left[ \\frac{300}{x(0)} \\right]^4,\n\\]\nwhich rearranges to\n\\[\n[x(0)]^3 = \\frac{(300)^4}{1000},\n\\]\nor \\(x(0)=(8.1\\times 10^6)^{1/3} \\approx 201\\).\n\n\n\nExample 1.25 Suppose that after 2 hours, a radioactive sample has lost 2% of its mass. What is the half-life of the sample?\n\nSolution. We are given that \\(x(2) = 0.98 x(0)\\). Thus,\n\\[\n0.98 x(0) = x(2) = x(0) e^{2a} \\implies 2a = \\ln(0.98).\n\\]\nThe half-life is the time \\(t_h\\) such that \\(x(t_h) = \\frac{1}{2} x(0)\\), so\n\\[\n\\frac{1}{2} x(0) = x(t_h) = x(0) e^{a t_h},\n\\]\nfrom which we get \\(a t_h = \\ln(0.5)\\). Overall,\n\\[\n\\frac{a t_h}{2a} = \\frac{\\ln(0.5)}{\\ln(0.98)} \\approx 34.31,\n\\]\nwhich yields \\(t_h = 68.6\\) hours.\n\n\nAn obvious criticism of the population growth ODE \\(x'=ax\\) is that it assumes resources (food, space) are unlimited, which is why \\(x(t)\\) can grow forever. It’s more reasonable to assume that the growth rate will not stay constant but decrease as the population size increases. The simplest such model is the logistic equation, in which the per capita growth rate is \\(a-bx\\):\n\\[\n\\dd{x}{t} = x (a - bx).\n\\]\nThis equation is autonomous and therefore separable. The solution requires integration via partial fractions, which I won’t show here. The result with \\(x(0)=x_0\\) is\n\\[\nx(t) = \\frac{a x_0}{b x_0 + (a-bx_0) e^{-at}}.\n\\]\nAs \\(t\\to \\infty\\), \\(x\\to a/b\\), which is called the carrying capacity of the model. The direction field, and several solutions, are shown in Example 1.10.\n\n\n\n\n1.9.2 Cooling and heating\nThere are formulas to plug and play for this type of problem, but these are not hard to solve directly if you understand how to interpret them.\nSuppose an object is put into an environment that is held at constant temperature \\(E\\). Let \\(T(t)\\) be the temperature of the object. The Newton Cooling Law (which applies equally to heating) states that the difference between \\(T\\) and \\(E\\) decreases at a constant decay rate. Mathematically, if we define \\(x(t) = T(t) - E\\), then\n\\[\n\\dd{x}{t} = -k x, \\qquad (k&gt;0).\n\\]\nThe usual assumption is that \\(k\\) is constant, so the general solution is \\(x(t)=c_1 e^{-kt}\\). Remember that this is the temperature difference, so to get the object’s temperature, you need to use \\(T(t) = E + x(t)\\).\nNumerically you need to know three things to completely know the solution: the initial temperature, the decay rate \\(k\\), and the environmental temperature \\(E\\). If a word problem gives you any three pieces of relevant information, you need to convert them into these three values.\n\nExample 1.26 A mug of coffee at 90 C is put in a room kept at 20 C. After 10 minutes the coffee has cooled by \\(7\\) C. When will the coffee reach 60 C?\n\nSolution. In terms of the notation above, we have \\(E=20\\), \\(T(0)=90\\), and \\(T(10)=90-7=83\\). The cooling rate is not yet known. So we have \\(x(t)=T(t)-20\\) and the IVP\n\\[\nx' = -kx,\\quad x(0)=90-20.\n\\]\nThis has solution \\(x(t)=70e^{-kt}\\). Now we use the given value of \\(x(10)=83-20\\) to deduce\n\\[\n63 = 70 e^{-10k} \\quad \\Rightarrow \\quad k = \\frac{1}{10} \\ln\\left( \\frac{70}{63} \\right).\n\\]\n(There’s no need to calculate a numerical value for \\(k\\), though if you do, keep at least 4 significant digits for safety. It’s a positive value because we put a negative sign into the ODE.)\nThe problem asks us to find the value of the time \\(s\\) such that \\(T(s)=60\\), which implies \\(x(s)=40\\). Thus\n\\[\n40 = x(s) = 70 \\exp(-sk),\n\\]\nwhich leads to\n\\[\nsk = \\ln \\left( \\frac{70}{40} \\right).\n\\]\nFinally,\n\\[\ns = 10 \\frac{\\ln(7/4)}{\\ln(10/9)} \\approx 53.1 \\text{ min}.\n\\]\n\n\nSometimes you don’t need to find the rate constant explicitly because you can manipulate expressions to substitute for it.\n\nExample 1.27 Suppose Fred and Wilma each pour out a mug of coffee that is at 85 degrees C. Assume that the cooling rate in a mug is proportional to the cross-section of the mug’s top.Wilma’s mug has a diameter that is \\(\\sqrt{2}\\) times as large as Fred’s. Both mugs are left sitting out for 30 minutes in a room that is at 25\\({}^\\circ\\) C, at which point Fred’s coffee is at 60 degrees. What is the temperature of Wilma’s coffee at the same moment?\n\nSolution. Suppose Fred’s mug has cooling rate \\(k\\). Since area is proportional to diameter squared, Wilma’s mug has a cooling rate \\(2k\\). Define \\(F(t)\\) and \\(W(t)\\) to be the temperatures of the two coffees, and let \\(x(t)=F(t)-25\\), \\(y(t)=W(t)-25\\). We know that \\(F(0) = W(0) = 85\\), or\n\\[\nx(0) = y(0) = 60.\n\\]\nNow \\(x'=-kx\\), so \\(x(t)=60e^{-kt}\\). We could use \\(F(30)=60\\) to find \\(k\\) and proceed from there, but there is a clever shortcut.\nNote that \\(y'=-2ky\\), so \\(y(t)=60e^{-2kt}\\). Hence\n\\[\ny(t) = 60 \\bigl( e^{-kt} \\bigr)^2 = 60 \\bigl( x(t)/60 \\bigr)^2 = \\frac{x(t)^2}{60}.\n\\]\nSince we know that \\(x(30)=60-25\\), we conclude that \\(y(30)=35^2/60 \\approx 20.4\\) C, so \\(W(30) = 25 + y(30) \\approx 45.4\\) degrees.\n\n\n\n\n1.9.3 Skydiving\nIf a skydiver of mass \\(m\\) jumps from a plane, their downward speed \\(v\\) is initially governed approximately by\n\\[\nm \\dd{v}{t} = mg - kv,\n\\]\nwhere \\(g\\) is gravitational acceleration and \\(k &gt; 0\\) is a drag coefficient. We can assume \\(v(0)=0\\). We write the ODE as\n\\[\n\\dd{v}{t} + \\gamma v = g,\n\\]\nwhere \\(\\gamma =k/m\\). This is a linear equation with general solution\n\\[\nv(t) = c_1 e^{-\\gamma t} + \\frac{g}{\\gamma},\n\\]\nand the initial condition leaves us with\n\\[\nv(t) = \\frac{g}{\\gamma} \\left( 1 - e^{-\\gamma t} \\right).\n\\]\nAs \\(t\\to \\infty\\), \\(v\\to g/\\gamma = mg/k\\), which is the terminal speed. By opening the parachute, the skydiver dramatically increases \\(k\\) so that the terminal speed is reduced.\nOnce \\(v(t)\\) is known, it can be integrated to find the net displacement:\n\\[\ns(t) - s(0) = \\int_0^t v(τ) \\, dτ.\n\\]\nIf \\(v\\) never changes sign, then \\(|s(t)-s(0)|\\) is the total distance traveled."
  },
  {
    "objectID": "ivp_second_order.html#sec-second-linear",
    "href": "ivp_second_order.html#sec-second-linear",
    "title": "2  Second-order linear IVPs",
    "section": "2.1 Linear problems",
    "text": "2.1 Linear problems\nWe will focus on linear problems,\n\\[\nx'' + p(t)x' + q(t) x = f(t).\n\\tag{2.1}\\]\nEquation Equation 2.1 is in the form of \\(\\opA[x]=f\\) for the linear operator\n\\[\n\\opA[x] = \\ddd{x}{t} + p \\dd{x}{t} + q x.\n\\]\nWe now reap rewards for using the abstract language of operators with first-order problems, because much of what we learned there applies here as well. In particular, we still have a superposition theorem like Theorem 1.1 and a solving strategy like Theorem 1.3:\n\nFind the general solution \\(x_h\\) of the associated homogeneous problem \\(\\opA[x]=0\\).\nFind any particular solution \\(x_p\\) of the original \\(\\opA[x]=f\\).\nAdd them to get the general solution.\nIf given, use initial conditions to solve for the integration constants.\n\nThe second-order problem does bring in a major new wrinkle regarding the homogeneous solution, however.\n\n2.1.1 Independence\nConsider again the trivial problem \\(x''=0\\). It’s certainly true that \\(x=t\\) and \\(x=5t\\) are different solutions of this equation. So it would be tempting to write the general solution of this homogeneous problem as\n\\[\nc_1\\cdot (t) + c_2\\cdot (5t)\n\\]\nfor arbitrary constants \\(c_1\\) and \\(c_2\\). While this expression does always give a solution, it doesn’t give all of them, such as \\(x=2\\).\nThe issue is that \\(t\\) and \\(5t\\) are not different enough, in a way we now make precise.\n\nDefinition 2.3 (Linear independence) Functions \\(u_1(t),u_2(t),\\ldots,u_n(t)\\) are linearly dependent on interval \\(I\\) if there is a way to choose constants \\(c_1,\\ldots,c_n\\), not all zero, such that\n\\[\nc_1u_1(t) + c_2u_2(t) + \\cdots c_n u_n(t) = 0\n\\]\nfor all \\(t \\in I\\). If the functions are not linearly dependent, then they are linearly independent on \\(I\\).\n\n\n\n\n\n\n\nNote\n\n\n\nLinear independence is a notion that extends well beyond differential equations, as we will see.\n\n\n\nExample 2.1 The functions \\(u_1=t\\) and \\(u_2=5t\\) are linearly dependent on any interval, because \\(-5u_1 + u_2 = -5t+5t=0\\) for all \\(t\\).\n\n\nExample 2.2 The functions \\(u_1=\\cos(t)\\) and \\(u_2=\\sin(t)\\) are independent on the real line \\((-\\infty,\\infty)\\), because the equation\n\\[\nc_1\\cos(t) + c_2\\sin(t) = 0\n\\]\nis equivalent to \\(\\tan(t)=-c_1/c2\\) if \\(c_2\\neq 0\\), which is impossible for all \\(t\\). Hence \\(c_2=0\\), which implies that \\(c_1 \\cos(t)=0\\) for all \\(t\\), and this requires \\(c_1=0\\) as well.\n\n\n\n2.1.2 Wronskian\nThere is a mechanical way to determine the independence of given functions.\n\nDefinition 2.4 Suppose \\(u_1,\\ldots,u_n\\) all have at least \\(n-1\\) continuous derivatives. Their Wronskian is\n\\[\nW(t) = \\begin{vmatrix}\nu_1 & u_2 & \\cdots & u_n \\\\\nu_1' & u_2' & \\cdots & u_n' \\\\\n\\vdots & \\vdots &  & \\vdots  \\\\\nu_1^{(n-1)} & u_2^{(n-1)} & \\cdots & u_n^{(n-1)}\n\\end{vmatrix}.\n\\tag{2.2}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nEquation 2.2 is a determinant, which we will not learn in general for a while yet. In the \\(2\\times 2\\) case, it’s\n\\[\n\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} = ad-bc.\n\\]\nIn the \\(3\\times 3\\) case, it’s the same process as finding cross products in vector calculus. You can look ahead at Example 4.11 for a refresher.\n\n\nHere’s the punch line.\n\nTheorem 2.1 If \\(x_1,\\ldots,x_n\\) are solutions of a linear homogeneous ODE \\(\\opA[x]=0\\), all defined for all \\(t\\in I\\), then they are linearly independent on \\(I\\) if and only if their Wronskian is nonzero at all \\(t\\in I\\).\n\nThe upshot is that we can check the Wronskian at any one value of \\(t\\) we want; zero means linear dependence, and nonzero means independence.\n\nExample 2.3 You can check that \\(\\cos(t)\\) and \\(\\sin(t)\\) are solutions of \\(x''+x=0\\). Since\n\\[\nW(t) = \\twodet{\\cos(t)}{\\sin(t)}{-\\sin(t)}{\\cos(t)} = \\cos^2(t) + \\sin^2(t) = 1,\n\\]\nthey are linearly independent.\n\n\n\n2.1.3 General solution\nNow we can state precisely what goes into a general solution in the linear, second-order case.\n\nTheorem 2.2 Suppose \\(x_1\\) and \\(x_2\\) are linearly independent solutions of the second-order linear problem \\(\\opA[x]=0\\). Then the general solution of \\(\\opA[x]=0\\) is\n\\[\nx(t) = c_1 x_1(t) + c_2 x_2(t),\n\\]\nwhere \\(c_1\\) and \\(c_2\\) are arbitrary constants. Furthermore, the general solution of the nonhomogeneous problem \\(\\opA[x]=f\\) is\n\\[\nx(t) = x_h(t) + x_p(t),\n\\]\nwhere \\(x_h\\) is the general homogeneous solution and \\(x_p\\) is any one particular solution of \\(\\opA[x]=f\\).\n\n\n\n2.1.4 Constant coefficients\nThe general linear second-order problem \\(\\opA[x]=f\\) has the structure described above, but only a few such problems are straightforward to solve. When it comes to actually finding solutions, we will limit ourselves to a friendly subset of problems:\n\nDefinition 2.5 The second-order linear constant-coefficient problem is \\[\nx'' + bx' + k x = f(t),\n\\tag{2.3}\\]\nwhere \\(b\\) and \\(k\\) are constants. The function \\(f(t)\\) is the forcing function.\n\n\n\n\n\n\n\nNote\n\n\n\nIn Equation 2.3 the coefficient \\(b\\) has units \\(1/T\\), the coefficient \\(k\\) has units \\(1/T^2\\), and \\(f(t)\\) has units \\(X/T^2\\), where \\(X\\) and \\(T\\) are the units of \\(x\\) and \\(t\\), respectively."
  },
  {
    "objectID": "ivp_second_order.html#sec-second-const_coeff",
    "href": "ivp_second_order.html#sec-second-const_coeff",
    "title": "2  Second-order linear IVPs",
    "section": "2.2 Homogeneous solutions",
    "text": "2.2 Homogeneous solutions\nWe start with solutions of the homogeneous equation\n\\[\nx'' + b x' +  k x = 0.\n\\]\nWe will use \\(x_h\\) to name the general solution, since it might be part of the solution to a nonhomogeneous equation.\n\n2.2.1 Characteristic polynomial\nWe employ the super-sophisticated method of guessing the answer and then proving we’re right. This starts with\n\\[\nx_h(t) = e^{\\lambda t}\n\\]\nfor a to-be-determined value of \\(\\lambda\\). We substitute \\(x_h\\) into the ODE and get\n\\[\n\\begin{split}\n0 &= \\lambda^2 e^{\\lambda t} + b \\bigl(\\lambda e^{\\lambda t}\\bigr) + k \\bigl(e^{\\lambda t}\\bigr)\\\\\n&= e^{\\lambda t} \\bigl(\\lambda^2 + b \\lambda + k \\bigr).\n\\end{split}\n\\]\nWe therefore know that \\(x_h\\) is a homogeneous solution provided that \\(\\lambda^2 + b \\lambda + k =0\\).\n\nDefinition 2.6 (Characteristic polynomial of a linear ODE) The characteristic polynomial of the linear operator \\(\\opA[x]=x''+bx'+kx\\) is\n\\[\np(s) = s^2 + b s + k.\n\\]\nIts roots are called the characteristic values of \\(\\opA\\). For brevity, we will often refer to these as simply the roots of \\(\\opA\\), although this usage is not standard.\n\n\nTheorem 2.3 (Homogeneous solution, distinct roots) Let \\(\\lambda_1,\\lambda_2\\) be the characteristic values of \\(\\opA\\), where \\(\\opA[x]=x''+bx'+kx.\\) If \\(\\lambda_1\\neq \\lambda_2\\), then the general solution of \\(\\opA[x]= 0\\) is\n\\[\nx_h(t) = c_1 e^{\\lambda_1 t} + c_2 e^{\\lambda_2 t}.\n\\tag{2.4}\\]\n\nPart of what makes Equation 2.4 work is that \\(e^{\\lambda_1 t}\\) and \\(e^{\\lambda_2 t}\\) are independent solutions, because\n\\[\n\\twodet{e^{\\lambda_1 t}}{e^{\\lambda_2 t}}{\\lambda_1 e^{\\lambda_1 t}}{\\lambda_2 e^{\\lambda_2 t}}\n= (\\lambda_2-\\lambda_1)e^{(\\lambda_1+\\lambda_2)t},\n\\]\nwhich is never zero.\nWe have to handle the case \\(\\lambda_1=\\lambda_2\\) separately.\n\nTheorem 2.4 (Homogeneous solution, repeated root) Let \\(\\lambda\\) be a double root of \\(\\opA\\), where \\(\\opA[x]=x''+bx'+kx.\\) The general solution of \\(\\opA[x]= 0\\) is\n\\[\nx_h(t) = e^{\\lambda t} \\bigl( c_1 t + c_2 \\bigr).\n\\tag{2.5}\\]\n\n\nExample 2.4 The trivial problem \\(x''=0\\) is linear, with \\(b=k=0\\). The characteristic polynomial is \\(s^2\\), which makes \\(\\lambda=0\\) a double root. So the general solution is\n\\[\nx_h(t) = e^{0t} (c_1 t + c_2) = c_1 t + c_2.\n\\]\nThis was not exactly a mystery to us, but it’s a good idea to check new formulas on cases you already understand.\n\n\nExample 2.5 Find the general solution of \\(x''-x'-2x=0\\).\n\nSolution. The characteristic polynomial is \\(s^2-s-2\\), which has roots \\(\\lambda_1=-1\\), \\(\\lambda_2=2\\). This gives the general solution \\(x_h(t)=c_1 e^{-t} + c_2 e^{2t}\\).\n\n\n\nExample 2.6 Solve the IVP \\(x'' - 5 x = 0\\), \\(x(0)=6\\), \\(x'(0)=0\\).\n\nSolution. The roots of \\(s^2-5\\) are \\(\\lambda_1=\\sqrt{5}\\), \\(\\lambda_2=-\\sqrt{5}\\). The general solution is\n\\[\nx(t) = c_1 e^{\\sqrt{5}\\, t} + c_2 e^{-\\sqrt{5}\\, t}.\n\\]\nThe initial conditions lead to\n\\[\\begin{split}\n6 &= x(0) = c_1 e^0 + c_2 e^0 = c_1 + c_2, \\\\\n0 &= x'(0) = \\sqrt{5} c_1 e^0 - \\sqrt{5} c_2 e^0 = \\sqrt{5}(c_1-c_2).\n\\end{split}\\]\nIt’s easy to conclude from here that \\(c_1=c_2=3\\). In general we might have to solve a \\(2\\times 2\\) linear algebraic system for the constants.\n\n\nAll this seems simple enough. But remember that the roots of a real quadratic polynomial may come as a pair of complex conjugate numbers. How then do we interpret Equation 2.4 or Equation 2.5? The answer is what makes second-order problems really different from first-order ones, and we tackle it by taking a brief deep dive into complex numbers."
  },
  {
    "objectID": "ivp_second_order.html#sec-second-complex",
    "href": "ivp_second_order.html#sec-second-complex",
    "title": "2  Second-order linear IVPs",
    "section": "2.3 Complex characteristic values",
    "text": "2.3 Complex characteristic values\n\n\nCode\nusing Plots,LaTeXStrings,Printf\ndefault(linewidth=2,label=\"\",size=(400,280))\nusing Logging\ndisable_logging(Logging.Info);\n\n\nFor a really smart and entertaining introduction to complex numbers, I recommend this video series.\nThere’s often a lot of wariness about complex numbers. Terminology is part of the reason. Using “real” and “imaginary” to label numbers is the residue of a value judgment that ruled mathematics for centuries. But complex numbers are actually just as “real” as so-called real numbers. If anything, they are arguably more fundamental.\nAs a practical matter, you can pretty much always replace a complex value with two real ones, and vice versa. But the complex point of view often provides a unifying perspective. Sometimes the formula manipulations are easier in the complex form, too; in particular, you may be able to replace trigonometry with algebra.\n\n\n2.3.1 Basic arithmetic\nWe can write a complex number \\(z\\in \\complex\\) as \\(z=x+iy\\), where \\(i^2=-1\\) and \\(x\\) and \\(y\\) are real numbers known as the real part and imaginary part of \\(z\\),\n\\[\nz = x + i y \\quad \\Leftrightarrow \\quad x = \\text{Re}(z), \\quad y = \\text{Im}(z).\n\\]\nWriting a complex number this way is equivalent to using rectangular or Cartesian coordinates in the plane to specify a point \\((x,y)\\). Thus complex numbers are on a number plane rather than a number line.\nThe generalization of absolute value to complex numbers is known as the modulus or magnitude, a real nonnegative quantity defined as\n\\[\n|z|^2 = [\\text{Re}(z)]^2 + [\\text{Im}(z)]^2.\n\\]\nLike the absolute value, \\(|z|\\) is the distance from \\(z\\) to the origin, and \\(|w-z|\\) is the distance between two complex numbers. Here, though, the distances are in the plane rather than on a line.\nAn important operation on complex numbers that has no real-value counterpart is the conjugate,\n\\[\n\\bar{z} =\\text{Re}(z) - i \\,\\text{Im}(z).\n\\]\nGeometrically the conjugate is the reflection across the real axis of the plane. No matter how complicated an expression is, you just replace \\(i\\) by \\(-i\\) everywhere to get the conjugate.\nYou add, subtract, and multiply complex numbers by applying the usual algebraic rules, applying \\(i^2=-1\\) as needed. They should give little trouble. Division can be a little trickier, even though the rules are always the same. One trick is to give a complex ratio a purely real denominator:\n\\[\n\\frac{w}{z} = \\frac{w \\bar{z}}{z \\bar{z}} = \\frac{w \\bar{z}}{|z|^2}.\n\\]\nThis is a lot like rationalizing a denominator with square roots.\n\nExample 2.7 \\[\n\\frac{2-i}{3+2i} = \\frac{2-i}{3+2i}\\cdot \\frac{3-2i}{3-2i}\n= \\frac{6-4i-3i+2i^2}{3^2+2^2} = \\frac{4-7i}{13}.\n\\]\n\n\nExample 2.8 Suppose that \\(|z|=1\\). Then\n\\[\n\\frac{1}{z} = \\frac{1}{z}\\cdot \\frac{\\bar{z}}{\\bar{z}} = \\frac{\\bar{z}}{|z|^2} = \\bar{z}.\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nMemorize the special case \\(\\dfrac{1}{i} = -i\\).\n\n\nHere are some more simple rules to know:\n\nTheorem 2.5 (Complex arithmetic) For complex numbers \\(w\\) and \\(z\\),\n\n\\(|\\bar{z}| = |z|\\)\n\\(|wz| = |w|\\cdot |z|\\)\n\\(|w+z|\\le |w| + |z|\\) (triangle inequality)\n\\(\\left| \\dfrac{1}{z} \\right| = \\dfrac{1}{|z|}\\)\n\\(\\overline{wz}=\\bar{w}\\cdot \\bar{z}\\)\n\\(\\overline{w+z}=\\bar{w}+\\bar{z}\\)\n\\(\\overline{\\left(\\dfrac{1}{z} \\right)} = \\dfrac{1}{\\bar{z}}\\)\n\\(|z|^2 = z\\cdot \\bar{z}\\)\n\n\n\n\n2.3.2 Euler’s identity\nFor second-order equations we need to make sense of \\(e^{\\lambda t}\\) when \\(\\lambda\\) is complex. The key is one of the most famous equations in mathematics.\n\nTheorem 2.6 (Euler’s identity) \\[\n:label: secondlin-euler\ne^{it} = \\cos(t) + i \\sin(t)\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nEuler is a German name, and as such, it rhymes with “boiler,” not “Bueller.”\n\n\n\nEuler’s identity shows that imaginary exponents produce oscillation, rather than the growth/decay of real exponents.\n\n\nCode\nc = t -&gt; real(exp(1im*t))\ns = t -&gt; imag(exp(1im*t))\n@gif for t in range(0,4π,length=60)\n    plot([c s],0,t,label=[\"Re part\" \"Im part\"],xaxis=(L\"t\",(0,4π)),yaxis=(L\"e^{it}\",(-1.05,1.05)))\n    title!(\"Complex exponential\")\nend\n\n\n\n\n\nAlternatively, if we take \\(x\\) and \\(y\\) to be the real and imaginary parts of \\(z=e^{it}\\), then they parametrically describe a unit circle in the complex plane.\n\n\nCode\n@gif for t in range(0,4π,length=60)\n    plot(c,s,0,t,aspect_ratio=1,xaxis=(\"Re z\",(-1.1,1.1)),yaxis=(\"Im z\",(-1.1,1.1)))\n    title!(\"Complex exponential = unit circle\")\nend\n\n\n\n\n\n\n\n2.3.3 Polar form\nSuppose \\(z\\) is any nonzero complex value. We can write it in the form\n\\[\nz = |z|\\cdot \\frac{z}{|z|},\n\\]\nwhere \\(|z|\\) is the modulus (distance from origin in the plane). Since \\(\\frac{z}{|z|}\\) has modulus equal to 1, it must lie on the unit circle. Hence there is a real \\(\\theta\\) such that\n\\[\nz = |z|\\, e^{i\\theta}.\n\\tag{2.6}\\]\nWe call Equation 2.6 the polar form of a complex number, because it expresses \\(z\\) as a distance from zero and an angle from the positive Re axis. Just as with any point in the plane, we can express a complex number either in Cartesian form using Re and Im parts, or in polar form using modulus and Euler’s identity.\n\nExample 2.9 Suppose \\(r\\) is a positive real number. Then \\(-r\\) lies at a distance \\(r\\) from the origin along the negative real axis. Hence\n\\[\n-r = |r|\\, e^{i\\pi}.\n\\]\nSupposing that we may take the log of both sides, we get\n\\[\n\\ln(-r) = \\ln |r| + i\\pi.\n\\]\nUsing complex numbers, then, we can take the log of a negative number. You will find that this is the case in MATLAB.\n\n\nExample 2.10 What’s the square root of \\(i\\)?\n\nSolution. Since \\(i\\) lies at a distance 1 from the origin along the positive imaginary axis, its polar form is \\(e^{i\\pi/2}\\). Then the square root is\n\\[\n\\bigl( e^{i\\pi/2} \\bigr)^{1/2} = e^{i\\pi/4}.\n\\]\nAppealing to Euler’s formula, this is equivalent to \\(\\frac{1}{\\sqrt{2}}(1+i)\\). We should note that of course, the negative of this value is also a square root.\n\n\n\n\n2.3.4 Complex exponentials\nExponential functions still obey the properties you already know, even when the exponents are imaginary or complex numbers. This allows us to handle the exponential of any complex number. Writing \\(a+i \\omega\\) for real \\(a\\) and \\(\\omega\\), we have the function\n\nTheorem 2.7 (Complex exponential function) \\[\ne^{(a+i \\omega)t} = e^{at} \\cdot e^{i\\omega t} = e^{at} \\bigl[ \\cos(\\omega t) + i \\sin(\\omega t)\\bigr].\n\\tag{2.7}\\]\n\nIn the function \\(e^{\\lambda t}\\), the real part of \\(\\lambda\\) controls growth or decay in time, as we are familiar with, and the imaginary part of \\(\\lambda\\) controls a frequency of oscillation around a circle in the complex plane.\n\n2.3.4.1 Growing\nIf \\(\\text{Re} \\lambda &gt; 0\\), the magnitude of the function grows exponentially. The result is an outward spiral. The imaginary part of \\(\\lambda\\) controls the frequency or pitch of the spiral. The real and imaginary parts of \\(e^{\\lambda t}\\) oscillate between two growing real exponentials.\n\n\nCode\na = 0.025;  ω = 0.75;\nt = range(0, 25, length=120)\nz = @. exp((a+1im*ω)*t)\n\n@gif for k in [eachindex(t);fill(length(t),5)]\n    plot(t[1:k],real(z[1:k]),imag(z[1:k]),xaxis=(\"t\",(0,30),:flip),yaxis=(\"Re z\",(-2,2)),zaxis=(\"Im z\",(-2,2)))\n    plot!(t[1:k],real(z[1:k]),fill(-1.5,k))\n    plot!(t[1:k],fill(2,k),imag(z[1:k]))\n    plot!(fill(30,k),real(z[1:k]),imag(z[1:k]),l=:black)\n    title!(@sprintf(\"Growth and oscillation: a = %.2f, ω = %.2f\",a,ω))\nend\n\n\n\n\n\n\n\n2.3.4.2 Neutral\nWhen \\(\\lambda\\) is purely imaginary, the function values stay on the unit circle in the complex plane. Taking the real and imaginary parts of \\(e^{\\lambda t}\\) gives cosine and sine, respectively.\n\n\nCode\na = 0.;  ω = 0.75;\nt = range(0, 25, length=120)\nz = @. exp((a+1im*ω)*t)\n\n@gif for k in [eachindex(t);fill(length(t),5)]\n    plot(t[1:k],real(z[1:k]),imag(z[1:k]),xaxis=(\"t\",(0,30),:flip),yaxis=(\"Re z\",(-2,2)),zaxis=(\"Im z\",(-2,2)))\n    plot!(t[1:k],real(z[1:k]),fill(-1.5,k))\n    plot!(t[1:k],fill(2,k),imag(z[1:k]))\n    plot!(fill(30,k),real(z[1:k]),imag(z[1:k]),l=:black)\n    title!(@sprintf(\"Pure oscillation: a = %.2f, ω = %.2f\",a,ω))\nend\n\n\n\n\n\n\n\n2.3.4.3 Decaying\nFinally, if \\(\\text{Re} \\lambda &lt; 0\\), the spiral is a decaying one. The real and imaginary parts are attenuated oscillations.\n\n\nCode\na = -0.07;  om = 2.4;\nt = range(0, 25, length=120)\nz = @. exp((a+1im*ω)*t)\n\n@gif for k in [eachindex(t);fill(length(t),5)]\n    plot(t[1:k],real(z[1:k]),imag(z[1:k]),xaxis=(\"t\",(0,30),:flip),yaxis=(\"Re z\",(-2,2)),zaxis=(\"Im z\",(-2,2)))\n    plot!(t[1:k],real(z[1:k]),fill(-1.5,k))\n    plot!(t[1:k],fill(2,k),imag(z[1:k]))\n    plot!(fill(30,k),real(z[1:k]),imag(z[1:k]),l=:black)\n    title!(@sprintf(\"Decay and oscillation: a = %.2f, ω = %.2f\",a,ω))\nend\n\n\n\n\n\n\n\n\n2.3.5 Homogeneous solutions\nNow we can finish the story of computing solutions to homogeneous constant-coefficient ODEs. We can still use Theorem 2.3 and Theorem 2.4, now with the complex-valued interpretations of the exponentials.\n\nExample 2.11 Solve the IVP \\(x''+9x=0\\), \\(x(0)=2\\), \\(x'(0)=-12\\).\n\nSolution. The characteristic polynomial is \\(s^2+9\\), giving the roots \\(\\pm 3i\\). Hence the general solution is \\(x_h(t) = c_1e^{3it} + c_2e^{-3it}\\).\nThe initial conditions require\n\\[\\begin{split}\n2 &= x(0) = c_1e^0 + c_2e^0 = c_1 +c_2,\\\\\n-12 &= x'(0) = 3i c_1e^0 - 3i c_2e^0 = 3i c_1 - 3i c_2.\n\\end{split}\\]\nThis system is easy to solve for \\(c_1\\) and \\(c_2\\) to get \\(c_1=1+2i\\), \\(c_2=1-2i\\).\n\n\nThere are some helpful nuances to point out about the preceding example.\n\nTheorem 2.8 If a real second-order IVP has complex conjugate characteristic values, then the integration constants satisfy \\(c_2=\\overline{c_1}\\).\n\nThis can simplify the algebra a bit. If we set \\(c_1=\\alpha + i\\beta\\), then\n\\[\nc_1 + c_2 = 2\\alpha, \\quad c_1-c_2 = 2i\\beta.\n\\tag{2.8}\\]\nIn Example 2.11 this would have led us right to \\(2\\alpha=2\\) and \\(3i(2i\\beta)=-12\\).\nIt might seem odd to use complex numbers to represent what we know must be a real-valued solution. There’s nothing wrong with doing so, but we can also convert it to an explicitly real form. The following is easily proved using Euler’s identity.\n\nTheorem 2.9 The general homogeneous solution\n\\[\nx_h(t) = c_1 e^{\\lambda_1 t} + c_2 e^{\\lambda_2 t},\n\\]\nwhere \\(\\lambda_{1,2} = a \\pm i\\omega\\) are complex, is equivalent to the real expression\n\\[\nx_h(t) = e^{at} \\bigl[ b_1 \\cos(\\omega t) + b_2 \\sin(\\omega t) \\bigr],\n\\]\nwhere \\(b_1,b_2\\) are real constants.\n\n\nExample 2.12 In Example 2.11, the roots were \\(0 \\pm 3i\\). Thus another expression for the general solution is\n\\[\nx_h(t) = c_1\\cos(3t) + c_2\\sin(3t).\n\\]\nThe initial conditions now yield\n\\[\n\\begin{split}\n2 &= x(0) = c_1\\cos(0) + c_2\\sin(0) = c_1,\\\\\n-12 &= x'(0) = -3 c_1 \\sin(0) + 3 c_2 \\cos(0) = 3c_2.\n\\end{split}\n\\]\nHence the IVP solution is \\(2\\cos(3t) - 4\\sin(3t).\\)\n\n\nExample 2.13 Solve the IVP \\(x'' + 4x' + 5x=0\\), \\(x(0)=6\\), \\(x'(0)=0\\).\n\nSolution. The roots of \\(s^2+4s+5\\) are \\(-2\\pm i\\). Thus the general solution is\n\\[\nx(t) = c_1 e^{(-2+i)t} + \\overline{c_1}e^{(-2-i)t}.\n\\]\nNote that\n\\[\nx'(t) = c_1 (-2+i) e^{(-2+i)t} + \\overline{c_1}(-2-i) e^{(-2-i)t}.\n\\]\nWriting \\(c_1=\\alpha + i\\beta\\), the initial condition on \\(x\\) requires that\n\\[\n6 = x(0) =  \\alpha + i\\beta + \\alpha - i\\beta = 2\\alpha,\n\\]\nso \\(\\alpha=3\\). The other initial condition leads to\n\\[\n\\begin{split}\n0 = x'(0) &= (3+i\\beta) (-2+i)\\cdot 1 + (3-i\\beta) (-2-i)\\cdot 1  \\\\\n&= (-6-2i+3i-\\beta) + (-6+2i-3i-\\beta) = -12 - 2\\beta,\n\\end{split}\n\\]\nwhich yields \\(\\beta=-6\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you have a complex expression “stuff \\(+\\) conjugate of that stuff”, the result has to be real, so you needn’t bother computing the imaginary part of that stuff.\n\n\n\nExample 2.14 Solve the IVP of Example 2.13 without using complex numbers.\n\nSolution. The roots of \\(s^2+4s+5\\) are \\(-2\\pm i\\). Thus the general solution is\n\\[\nx(t) = b_1 e^{-2t}\\cos(t) + b_2 e^{-2t}\\sin(t).\n\\]\nNote that\n\\[\nx'(t) = b_1[ -2e^{-2t}\\cos(t) - e^{-2t}\\sin(t)] + b_2[  -2e^{-2t}\\sin(t) + e^{-2t}\\cos(t)].\n\\]\nThe initial condition on \\(x\\) requires that\n\\[\n6 = x(0) = b_1,\n\\]\nso \\(b_1=6\\). The other initial condition leads to\n\\[\n0 = x'(0) = 6(-2+0) + b_2(0+1) = -12 + b2,\n\\]\nwhich yields \\(b_2=12\\).\n\n\nSince complex exponentials are much less familiar to you than sin and cos, the real form might seem more appealing to you. If you acquaint yourself with complex exponentials, though, there is nothing wrong with leaving things in that form, and you have less to memorize."
  },
  {
    "objectID": "ivp_second_order.html#sec-second-vop",
    "href": "ivp_second_order.html#sec-second-vop",
    "title": "2  Second-order linear IVPs",
    "section": "2.4 Variation of parameters",
    "text": "2.4 Variation of parameters\nSection 2.2 and Section 2.3 explained how to find the general solution \\(x_h(t)\\) of a homogeneous linear system \\(\\opA[x]=0\\). We need to perform that same step for a nonhomogeneous problem \\(\\opA[x]=f\\), but then also find a particular solution of the original equation. One possibility is to adapt the variation of parameters technique used for first-order problems in Section 1.7.\nThe form of the homogeneous solution is\n\\[\nx_h(t) = c_1 x_1(t) + c_2 x_2(t),\n\\]\nwhere \\(x_1\\) and \\(x_2\\) are independent homogeneous solutions. Now we replace the constants \\(c_1\\) and \\(c_2\\) with functions of \\(t\\):\n\\[\nx_p(t) = u_1(t) x_1(t) + u_2(t) x_2(t).\n\\tag{2.9}\\]\nIt follows that\n\\[\nx_p' = u_1'x_1 + u_1 x_1' + u_2'x_2 + u_2 x_2'.\n\\]\nWe are trying to find a way to choose \\(u_1\\) and \\(u_2\\) that makes \\(x_p\\) a particular solution. Our lives will be a lot easier if we start with the constraint\n\\[\nu_1'x_1 + u_2'x_2 = 0,\n\\tag{2.10}\\]\nwhich implies \\(x_p' = u_1 x_1' + u_2 x_2'\\). Let’s see if we can get away with that. We differentiate again to get\n\\[\nx_p'' = u_1'x_1' + u_1 x_1'' + u_2'x_2' + u_2 x_2''.\n\\]\nSince \\(\\opA[x] = x'' + bx' +k\\), we get\n\\[\n\\opA[x_p] = u_1 (x_1'' + bx_1' + kx_1) + u_2 (x_2'' + bx_2' + kx_2) + ( u_1'x_1' + u_2'x_2').\n\\]\nWe are in great shape now. Since \\(x_1\\) and \\(x_2\\) are homogeneous solutions, we just have \\(\\opA[x_p] = u_1'x_1' + u_2'x_2'\\). So we need only to introduce the additional requirement\n\\[\nu_1'x_1' + u_2'x_2' = f,\n\\tag{2.11}\\]\nand then \\(x_p\\) will be the desired particular solution.\nEquation 2.10 and Equation 2.11 impose\n\\[\n\\begin{split}\nx_1 u_1' + x_2 u_2' &= 0, \\\\\nx_1'u_1' + x_2'u_2' &= f.\n\\end{split}\n\\]\nWriting \\(u_1'=v_1\\) and \\(u_2'=v_2\\), we see that this is a pair of linear equations for \\(v_1\\) and \\(v_2\\), which is routine to solve, leading to the following.\n\nTheorem 2.10 (Variation of parameters, 2nd order) Let\n\\[\nu_1 = \\int \\frac{-x_2 f}{W}\\, dt, \\quad u_2 = \\int \\frac{x_1 f}{W}\\, dt,\n\\]\nwhere \\(W\\) is the Wronskian of independent homogeneous solutions \\(x_1\\) and \\(x_2\\). Then a particular solution of \\(\\opA[x]=f\\) is\n\\[\nx_p(t) = u_1(t) x_1(t) + u_2(t) x_2(t).\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIsn’t it handy now that the Wronskian is never zero?\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe integration constants for \\(u_1\\) and \\(u_2\\) can be taken to be zero. Any value is valid, and we need just one particular solution.\n\n\n\nExample 2.15 Find a particular solution of \\(x'' - x = 4\\).\n\nSolution. There’s no getting around the need for the homogeneous solution here. The characteristic roots are \\(\\pm 1\\), so we take \\(x_1=e^t\\), \\(x_2=e^{-t}\\). Then \\(W=(e^t)(-e^{-t}) - (e^{-t})(e^t) = 2\\), and\n\\[\nu_1' = \\frac{-4 e^{-t}}{-2}, \\quad u_2' = \\frac{4 e^{t}}{-2}.\n\\]\nFrom here we find \\(u_1 = -2e^{-t}\\) and \\(u_2 = -2e^t\\). Hence\n\\[\nx_p = (-2e^{-t})(e^t) + (-2e^t)(e^{-t}) = -4.\n\\]\nYou can instantly check that this result is correct.\n\n\n\nExample 2.16 Find a particular solution of \\(x'' - \\lambda^2 x = 2 e^{\\lambda t}\\), where \\(\\lambda\\neq 0\\).\n\nSolution. The homogeneous equation has roots \\(\\pm \\lambda\\). We choose \\(x_1=e^{\\lambda t}\\), \\(x_2=e^{-\\lambda t}\\). Now\n\\[\nW = \\twodet{e^{\\lambda t}}{e^{-\\lambda t}}{\\lambda e^{\\lambda t}}{-\\lambda e^{-\\lambda t}} = -2\\lambda.\n\\]\nThis gives\n\\[\nu_1' = \\frac{ - 2 e^{-\\lambda t} e^{\\lambda t}}{-2\\lambda}, \\quad u_2' = \\frac{ 2 e^{\\lambda t} e^{\\lambda t} }{-2\\lambda}.\n\\]\nThus,\n\\[\nu_1 = \\frac{1}{\\lambda} \\int dt = \\frac{t}{\\lambda}, \\quad\nu_2 = -\\frac{1}{\\lambda} \\int e^{2 \\lambda t}\\, dt  = -\\frac{1}{2 \\lambda^2} e^{2 \\lambda t}.\n\\]\nFinally,\n\\[\nx_p(t) = \\frac{1}{\\lambda} t e^{\\lambda t}  - \\frac{1}{2 \\lambda^2} e^{\\lambda t}\n\\]\nis a particular solution. However, the second term is a constant multiple of \\(x_1\\), so we could drop it and still have a particular solution.\n\n\nAs you can see, the VoP formula is pretty great if you happen to be a robot. [Note to editor: insert CAPTCHA here.] Us humans, though, could use something a little easier, and that is coming next."
  },
  {
    "objectID": "ivp_second_order.html#undetermined-coefficients",
    "href": "ivp_second_order.html#undetermined-coefficients",
    "title": "2  Second-order linear IVPs",
    "section": "2.5 Undetermined coefficients",
    "text": "2.5 Undetermined coefficients\nWhen it comes to finding particular solutions of nonhomogeneous problems, there is a simpler alternative to variation of parameters in the important special case when\n\nThe coefficients in the ODE are constants, and\nThe forcing function \\(f(t)\\) is a polynomial, exponential, sin, or cos, or a combination of these.\n\nThus we now consider\n\\[\nx'' + bx' + kx = f(t),\n\\tag{2.12}\\]\nwhere \\(b\\) and \\(k\\) are constants, and \\(f\\) appears in Table 2.1.\n\n\nTable 2.1: Particular solutions for the method of undetermined coefficients\n\n\n\\(f(t)\\)\n\\(x_p(t)\\)\n\n\n\n\n\\(b_n t^n + \\cdots b_0\\)\n\\(B_n t^n + \\cdots + B_0\\)\n\n\n\\(e^{rt}(b_n t^n + \\cdots b_0)\\)\n\\(e^{rt}(B_n t^n + \\cdots B_0)\\)\n\n\n\\(\\cos(\\omega t)\\)\n\\(A \\cos(\\omega t) + B \\sin(\\omega t)\\)\n\n\n\\(\\sin(\\omega t)\\)\n\\(A \\cos(\\omega t) + B \\sin(\\omega t)\\)\n\n\n\n\nCorresponding to each \\(f\\) in Table 2.1 is a specific form for \\(x_p\\) containing one or more constants (given by capital letters) that must be deduced by substituting into the particular ODE under consideration. This procedure is what gives the method its name, the method of undetermined coefficients.\n\nExample 2.17 Rework Example 2.15, finding a particular solution of \\(x'' - x = 4\\).\n\nSolution. The forcing function is a polynomial of degree zero, so \\(x_p\\) should be as well. Let \\(x_p=A.\\) Then it is a solution if\n\\[\n4 = A'' - A = -A.\n\\]\nHence \\(A=-4\\).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIn the case where \\(f\\) is a polynomial that may have some zero coefficients, you cannot assume that \\(x_p\\) has zero coefficients in the same positions.\n\n\n\nExample 2.18 Find a particular solution of \\(x'' +4x'+4x=8t^2\\).\n\nSolution. The correct form of \\(x_p\\) is a quadratic polynomial, i.e.,\n\\[\nx_p(t) = A + Bt + Ct^2.\n\\]\nPlugging that into the ODE yields\n\\[\\begin{split}\n2C + 4(B+2Ct) + 4(A+Bt+Ct^2) & = 8t^2,\\\\\n4C t^2 + (8C+4B) + (2C+4B+4A) & = 8t^2.\n\\end{split}\\]\nThis has to be an identity for all \\(t\\). Matching like powers, we get a \\(3\\times 3\\) linear system for the coefficients. Fortunately, it can be solved easily if we go in order: first we read off \\(C=2\\), then \\(B=-2C=-4\\), and finally \\(A=-B-C/2=3\\). This provides us with\n\\[\nx_p(t) =2t^2+4t+3.\n\\]\n\n\n\nExample 2.19 Find a particular solution of \\(x'' - 2x'-3x=10e^{4t}\\).\n\nSolution. The proper choice is\n\\[\nx_p(t) = Ae^{4t}.\n\\]\nEverything else is algebra.\n\\[\\begin{split}\n16A e^{4t} - 2 (4A e^{4t}) - 3 (Ae^{4t}) & =10 e^{4t},\\\\\n16A -8A -3A &=  10.\n\\end{split}\\]\n.\nFrom this it’s clear that \\(A=2\\).\n\n\n\nExample 2.20 Find the general solution of \\(x''+x'=\\sin(2t)\\).\n\nSolution. The homogeneous problem \\(x''+x'=0\\) has roots \\(0\\) and \\(-1\\), hence\n\\[\nx_h(t) = c_1 + c_2 e^{-t}.\n\\]\nFor the particular solution we must choose\n\\[\nx_p(t) = A\\cos(2t) + B\\sin(2t).\n\\]\nInserting this into the original ODE leads to\n\\[\n[-4A\\cos(2t) - 4B\\sin(2t)] + [-2A\\sin(2t) + 2B\\cos(2t)] = \\sin(2t).\n\\]\nThis is identically true for all time if and only if we match the coefficients of like trig functions:\n\\[\n-4A + 2B = 0, \\qquad -4B-2A = 1.\n\\]\nThe solution of these equations is \\(A=-1/10\\), \\(B=-1/5\\). Thus\n\\[\nx(t) = c_1 + c_2 e^{-t} - \\frac{1}{10}\\cos(2t) - \\frac{1}{5}\\sin(2t).\n\\]\n\n\nThe examples above are the fundamental ones. If you have a forcing function that is a sum of terms from Table 2.1, then you can treat each term separately as above and add together the particular solutions found.\n\n2.5.1 Limitations\nThere are rules for more complicated combinations of the same functions, but we won’t go into them. At some point the calculus/algebra becomes pretty intense. Furthermore, we are not going to cover a refinement of the rules necessary in certain circumstances where our simplified version fails.\n\nExample 2.21 The equation \\(x''+x=\\cos(\\omega t)\\) suggests the particular solution \\(x_p(t)=A\\cos(\\omega t)+B\\sin(\\omega t)\\). Upon substitution,\n\\[\n[-\\omega^2 A\\cos(\\omega t) - \\omega^2 B\\sin(\\omega t) ] + [ A\\cos(\\omega t) + B\\sin(\\omega t)] =\\cos(\\omega t),\n\\]\nwhich leads to the conclusion that \\(B=0\\) and, if \\(\\omega^2 \\neq 1\\), \\(A=1/(1-\\omega^2)\\). However, if \\(\\omega = 1\\), the substitution would leave us with \\(0=\\cos(t)\\), which is impossible to satisfy for all \\(t\\).\n\nThe failure of Example 2.21 at \\(\\omega = 1\\) was due to the fact that the \\(x_p\\) we tried is actually a homogeneous solution. This is not too hard to fix, but a general description is not worth the effort; we can always fall back to variation of parameters.\n\nExample 2.22 We can make the tricky case \\(x''+x=\\cos(t)\\) from Example 2.21 quite easy by relating it to complex exponentials. Since \\(\\cos(t)\\) is the real part of \\(e^{it}\\), it follows that the solution we seek is the real part of any particular solution of\n\\[\nx'' + x = e^{it}.\n\\]\nReferring to Example 2.16 with \\(\\lambda=i\\), we use variation of parameters to find the particular solution\n\\[\n\\frac{1}{2i} t e^{i t} = -\\frac{i}{2}t [\\cos(t) + i\\sin (t)].\n\\]\nUpon taking the real part, we get\n\\[\nx_p(t) = \\frac{1}{2} t \\sin(t).\n\\]"
  },
  {
    "objectID": "ivp_second_order.html#sec-second-oscillators",
    "href": "ivp_second_order.html#sec-second-oscillators",
    "title": "2  Second-order linear IVPs",
    "section": "2.6 Oscillators",
    "text": "2.6 Oscillators\n\n\nCode\nusing Plots,LaTeXStrings,Printf\ndefault(linewidth=2,label=\"\")\n\n\nThe distinguishing feature of second-order ODEs compared to the first-order case is that they permit oscillations as well as exponential growth and decay. These equations appear in models throughout engineering and science as a result.\nFor instance, an object attached to an ideal spring satisfies the ODE\n\\[\nmx'' + b x' + kx = 0,\n\\tag{2.13}\\]\nwhere \\(x(t)\\) is the displacement from the natural resting position, \\(m\\) is the mass of the object, \\(b\\) is an attenuation coefficient due to friction or mechanical damping, and \\(k\\) is a property of the spring called the spring constant. This constant is the ratio of the restoring force of the spring to the amount by which it is stretched or compressed.\n\nAll named parameters such as \\(m\\), \\(k\\), and \\(b\\) are assumed to be nonnegative unless stated otherwise.\n\n\n\n\n\n\n\nNote\n\n\n\nEquation 2.13 applies equally well to horizontal and vertical oscillations. Gravity does not appear in the vertical case because it is accounted for by measuring \\(x\\) from the static equilibrium position.\n\n\nIf given, initial values for Equation 2.13 supply the initial position and initial velocity, which determine the solution uniquely.\nA pendulum is another type of mechanical oscillator. The proper ODE is\n\\[\n\\theta'' + \\gamma \\theta + \\frac{g}{L}\\sin (\\theta) = 0,\n\\]\nwhere \\(\\theta(t)\\) is the angle made with vertically-down position, \\(L\\) is the arm length, and \\(g\\) is gravitational acceleration. This equation is nonlinear and difficult to analyze without developing new tools. But if the angle remains small, then a reasonable approximation is\n\\[\n\\theta'' + \\gamma \\theta + \\frac{g}{L}\\theta = 0,\n\\]\nwhich is a linear oscillator with constant coefficients.\nAn AC circuit typically has elements of resistance, capacitance, and inductance. These analogize perfectly to friction/damping, spring constant, and mass. If these elements are wired in series, the governing ODE is\n\\[\nLI'' + RI' + \\frac{1}{C}I = 0,\n\\]\nwhere \\(I(t)\\) is the current flowing through the circuit, \\(L\\) is inductance, \\(R\\) is resistance, and \\(C\\) is capacitance.\nThese homogeneous models are examples of free oscillations. Of course, any of the models above might come with an external forcing function. For a pendulum, this could be someone pushing you on a swing, or for a circuit, it could be an AC generator. We call these forced oscillations.\n\n2.6.1 Unifying notation\nWhen you have many versions of the same fundamental problem, each using different symbols and units, you have three options.\n\nSolve each new problem from scratch.\nDerive custom formulas for each application.\nFind a minimal set of parameters and express the problem and solution in terms of them.\n\nOption 1 is highly inefficient. Option 2 would be appropriate for an engineering course. Here, we take option 3 and develop generalized knowledge that can be reinterpreted for each new application.\nThe ODE of a free mass-spring oscillator, for example, has standard form\n\\[\nx'' + \\frac{b}{m} x' + \\frac{k}{m} x = 0.\n\\]\nThis suggests that we only need two parameters, not three, to express the full range of behavior. (An added convenience is that both \\(b/m\\) and \\(\\sqrt{k/m}\\) have units of inverse time.) Accordingly, we introduce\n\\[\n\\omega_0 = \\sqrt{\\frac{k}{m}}, \\qquad \\zeta = \\frac{b/m}{2\\omega_0} = \\frac{b}{\\sqrt{4km}}.\n\\]\nThe parameter \\(\\omega_0\\) is known as the natural frequency, with units of inverse time, and \\(\\zeta\\) is a dimensionless damping coefficient describing the relative intensity of the damping.\n\nIn math we usually use frequency to mean the multiplier of \\(t\\) in a sin or cos function. That is our usage. In some fields this is called angular frequency, and frequency is used to mean the number of cycles per time unit, as in Hz.\n\nIf we include a forcing term, we arrive at the ODE\n\\[\nx'' + 2 \\zeta \\omega_0\\, x' + \\omega_0^2\\, x = f(t).\n\\tag{2.14}\\]\nA similar derivation can be done starting from the pendulum or AC circuit equations. The only type of forcing we shall consider is harmonic forcing of the form\n\\[\nf(t) = \\cos(\\omega t).\n\\]\n\n\n2.6.2 Classifications\nThe characteristic roots of this ODE are\n\\[\n\\lambda_{1,2} = -\\zeta \\omega_0 \\pm \\omega_0 \\sqrt{\\zeta^2-1}.\n\\]\nThe discussion now splits into four cases, marked by increasing values of \\(\\zeta\\).\n\n2.6.2.1 Undamped oscillator, \\(\\zeta=0\\)\nThe oscillator \\(x'' + \\omega_0^2 x\\) has homogeneous solutions that can be expressed either as combinations of\n\\[\ne^{i\\omega_0 t},\\, e^{-i\\omega_0 t}\n\\]\nor of\n\\[\n\\cos(\\omega_0 t),\\, \\sin(\\omega_0 t).\n\\]\nEither way, these describe pure oscillation at frequency \\(\\omega_0\\). This is known as simple harmonic motion. A particularly useful third form is the amplitude–phase form\n\\[\nx_h(t) = R\\cos(\\omega_0 t+\\phi),\n\\tag{2.15}\\]\nwhere constants \\(R\\) and \\(\\phi\\) can be used to satisfy initial conditions.\n\nExample 2.23 When a 2 kg mass is hung on a spring, the spring stretches by 0.25 m. What is the natural frequency of the mass-spring system? Suppose the mass is pulled down 0.2 m past equilibrium and then thrown upward at 1 m/s. What is the amplitude of the motion?\n\nSolution. Hooke’s Law for a spring states that \\(F=k x\\), so we find the spring constant from \\(k=F/x=2g/0.25=8g\\), where \\(g=9.8\\) m per second squared. The ODE for free motion of the system is thus \\(2 x'' + 8g x = 0\\), or\n\\[\nx'' + 4g x   = 0.\n\\]\nFrom this we identify the natural frequency\n\\[\n\\omega_0 = \\sqrt{4g} = 2\\sqrt{g} \\approx 6.26 \\text{s}^{-1}.\n\\]\nWe can apply the initial conditions directly to the phase–amplitude form:\n\\[\n\\begin{split}\n-0.2 & = x(0) = R\\cos(\\phi) \\\\\n1 & = x'(0) = \\omega_0 R\\sin(\\phi).\n\\end{split}\n\\]\nTherefore,\n\\[\nR^2 = [R\\cos(\\phi)]^2 + [R\\sin(\\phi)]^2 =  0.04 + \\omega_0^{-2},\n\\]\nwhich works out to \\(R \\approx 0.256 \\text{m}\\).\n\n\nThe particular solution for harmonic forcing \\(f(t)=\\cos(\\omega t)\\) is\n\\[\nx_p(t) = \\frac{1}{\\omega_0^2-\\omega^2} \\cos(\\omega t),\n\\]\nprovided \\(\\omega\\neq \\omega_0\\). Note that the harmonic amplitude grows without bound as \\(\\omega\\to \\omega_0\\).\nIf the forcing is exactly at the natural frequency, then the situation is like that of Example 2.16:\n\\[\nx_p(t) = \\frac{1}{2 \\omega_0} t \\sin(\\omega_0 t),\n\\]\nwhich is unbounded as \\(t\\to \\infty\\). This is known as resonance, one of the most important phenomena in physics.\n\n\n2.6.2.2 Underdamped oscillator, \\(0 &lt; \\zeta &lt; 1\\)\nFor \\(0&lt; \\zeta &lt; 1\\) the roots of Equation 2.14 are complex, with negative real part:\n\\[\n\\lambda_{1,2} = -\\zeta \\omega_0 \\pm i \\omega_0 \\sqrt{1-\\zeta^2}.\n\\]\nDefine the damped frequency\n\\[\n\\omega_d=\\omega_0 \\sqrt{1-\\zeta^2}.\n\\tag{2.16}\\]\nThe unforced part of the solution is\n\\[\nx_h(t) = e^{- \\omega_0 \\zeta t} [ c_1 \\cos( \\omega_d t) + c_2 \\sin(\\omega_d t) ].\n\\tag{2.17}\\]\nThis solution is pseudoperiodic, combining oscillation at frequency \\(\\omega_d &lt; \\omega_0\\) inside an exponential decay envelope. We call this an underdamped oscillator. The homogeneous solution Equation 2.17 is also called a transient solution, because it vanishes as \\(t \\to \\infty\\).\nIf harmonic forcing \\(f(t)=\\cos(\\omega t)\\) is added, then it determines the steady-state behavior \\(x_s=A\\cos(\\omega t + \\theta)\\), where\n\\[\nA = \\frac{1}{\\rule{0pt}{1em} \\sqrt{\\rule{0pt}{0.8em}(\\omega_0^2-\\omega^2)^2 + 4\\omega_0^2\\omega^2 \\zeta^2 } }\n\\tag{2.18}\\]\nThe value \\(A\\) is known as the gain of the oscillator; it is the ratio of amplitudes of the steady response and the forcing. A little calculus shows that as a function of \\(\\omega\\), the gain is maximized at\n\\[\n\\omega_\\text{max} = \\begin{cases}\n\\omega_0 \\sqrt{1-2\\zeta^2}, & \\text{ if } 0 &lt; \\zeta^2 &lt; \\frac{1}{2},\\\\ 0, & \\text{ if } \\frac{1}{2} \\le \\zeta^2.\n\\end{cases}\n\\tag{2.19}\\]\nTherefore, if the damping coefficient \\(\\zeta\\) is small but finite, we have a pseudoresonance of finite amplitude at a frequency just a bit less than the natural frequency. The following figure shows the gain as a function of driving frequency and damping when \\(\\omega_0=1\\). The black curve shows the maximal driving frequency at any given \\(\\zeta\\).\n\n\nCode\nω = range(1e-3,2,length=90)\nζ = range(1e-3,1.2,length=90)\nA = [ 1/(1-ω^2 + 2im*z*ω ) for ω in ω, z in ζ ]\nlog_g = @. log10(abs(A))\ncontour(ω,ζ,log_g',levels=20,l=(:bluesreds,2),clims=(-1.5,1.5))\nζ = range(1e-3,1/sqrt(2),length=100)\nωmax = @. real( sqrt(1+0im-2ζ^2) ) \nplot!(ωmax,ζ,l=(:black),\n  xlabel=\"driving frequency ω\", ylabel=\"damping coefficient ζ\", title=\"Log gain for ω₀=1\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s a short video showing how the gain of an oscillator varies with the forcing frequency \\(\\omega\\). (I use the term “eigenvalues” to mean characteristic values here.)\n\n\n\n\n\n\n2.6.2.3 Critically damped oscillator, \\(\\zeta = 1\\)\nAt \\(\\zeta=1\\) the complex roots coalesce into a double real root,\n\\[\n\\lambda_1 = \\lambda_2 = -\\omega_0,\n\\]\nwith general homogeneous solution\n\\[\nx_h(t) = e^{-\\omega_0 t} (c_1 + c_2 t).\n\\]\nThere is no longer any oscillation present, and we have a critically damped system. The linear growth of \\(c_2 t\\) doesn’t make much of a difference against the exponential decay, and \\(x_h\\to 0\\) as \\(t\\to\\infty\\). Any steady response is due to the forcing term, but no resonance is possible.\n\n\n2.6.2.4 Overdamped, \\(\\zeta &gt;1\\)\nFor \\(\\zeta &gt; 1\\) the roots are\n\\[\n\\lambda_{1,2} =  -\\omega_0 \\zeta \\pm \\omega_0 \\sqrt{\\zeta^2-1},\n\\]\nwhich are negative and real. This gives an exponentially decaying homogeneous solution. In this case we have an overdamped oscillator. Again, no resonance is possible.\n\n\n\n2.6.3 Examples\n\n\nTable 2.2: Damping coefficient and characteristic values\n\n\nDamping coefficient\nRoot property\n\n\n\n\n\\(\\zeta=0\\)\nimaginary\n\n\n\\(0 &lt; \\zeta &lt; 1\\)\ncomplex\n\n\n\\(\\zeta=1\\)\nreal, negative, repeated\n\n\n\\(\\zeta &gt; 1\\)\nreal, negative, distinct\n\n\n\n\n\nExample 2.24 A 5 kg mass is hung on a spring with constant \\(11\\) N per m and connected to a dashpot that provides 8 N-sec per meter of resistance. Is this system underdamped, overdamped, or critically damped?\n\nSolution. The ODE for the mass-spring system is\n\\[\n\\begin{split}\n    5 x'' + 8x' + 11 x  & = 0 ,\\\\\n    x'' + 1.6 x' + 2.2 x  & = 0,\n\\end{split}\n\\]\nfrom which we identify the natural frequency\n\\[\n  \\omega_0 = \\sqrt{2.2} \\approx 1.483 \\text{s}^{-1}.\n\\]\nThe damping coefficient is therefore\n\\[\n  \\zeta = \\frac{1.6}{2\\omega_0} \\approx 0.539.\n\\]\nSince this value is less than one, the system is underdamped.\n\n\n\nExample 2.25 Suppose the system from the previous example is initially at equilibrium when the mass is suddenly pushed downward at 0.5 m/sec. Find the motion of the mass.\n\nSolution. We derived the governing ODE \\(x'' + 1.6 x' + 2.2 x = 0\\). The roots are the roots of \\(\\lambda^2 + 1.6\\lambda + 2.2\\), which are found numerically to be\n\\[\n  \\lambda \\approx -0.8000 \\pm 1.2490i.\n\\]\n(The imaginary part is smaller than the natural frequency found in the last example, as it must be.) Choosing the sin-cos form of the general solution, we have\n\\[\nx_h(t) = c_1 e^{-0.8 t} \\cos(1.249 t) + c_2 e^{-0.8 t} \\sin(1.249 t).\n\\]\nWe apply the initial conditions \\(x(0)=0\\), \\(x'(0)=-0.5\\) to find\n\\[\n\\begin{split}\n0 & = x_h(0) = c_1, \\\\\n-0.5 & = x_h'(0) = c_1( -0.8 ) + c_2 (1.249 ),\n\\end{split}\n\\]\nthus \\(c_2 = -0.4003\\). The motion is therefore given by \\(x(t)=-0.4003\\, e^{-0.8 t} \\sin(1.249 t)\\)."
  },
  {
    "objectID": "linear_algebra_systems.html#overview",
    "href": "linear_algebra_systems.html#overview",
    "title": "3  Linear algebraic systems",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nIt might feel silly, but let’s review what it means to solve the linear equation\n\\[ax = b\\]\nfor \\(x\\).\n\nIf \\(a\\neq 0\\), there is a unique solution, \\(x=b/a\\).\nOtherwise,\n\nIf also \\(b=0\\), then every value of \\(x\\) is a valid solution.\nOtherwise, there are no solutions.\n\n\nIt turns out that no matter how many equations and variables we have, the only three possibilities are the ones above: zero, one, or infinitely many solutions. The main difference is that the condition “\\(a=0\\)” has to be generalized.\n\nDefinition 3.1 (Consistent linear system) A linear system that has at least one solution is called consistent. Otherwise, it is inconsistent.\n\n\n3.1.1 Two variables\nGoing one more baby step to two equations in two variables, we want to solve\n\\[\n\\begin{split}\nax + by &= f, \\\\\ncx + dy &= g.\n\\end{split}\n\\]\nThere is some easy geometric intuition here. Each equation represents a straight line in the plane, and solving both equations simultaneously means finding an intersection of these lines. Our cases above translate directly into:\n\nIf the lines are not parallel, there is a unique solution.\nOtherwise,\n\nIf the lines are identical, there are infinitely many solutions.\nOtherwise, there are no solutions.\n\n\nIt’s not hard to turn those statements into algebraic conditions. The slopes of the two lines are (ignoring infinities for a moment) \\(-a/b\\) and \\(-c/d\\), and we can say the slopes are equal if and only if \\(ad=bc\\).\n\nIf \\(ad \\neq bc\\), there is a single solution.\nOtherwise,\n\nIf one equation is a multiple of the other, there are infinitely many solutions.\nOtherwise, there are no solutions.\n\n\n\nExample 3.1 Find all solutions to the equations\n\\[\n\\begin{split}\nx - 3y & = 1, \\\\\n-2x + 6y & = 2.\n\\end{split}\n\\]\n\nSolution. We identify \\((a,b,c,d)=(1,-3,-2,6)\\), and \\(ad-bc=6-6=0\\). So we know there is not a unique solution (i.e., the lines are parallel). Dividing the second equation by \\(-2\\) leads to the equivalent system\n\\[\n\\begin{split}\nx - 3y & = 1, \\\\\nx - 3y & = -1.\n\\end{split}\n\\]\nIt’s now clear that there is no way to satisfy both equations simultaneously. The system is inconsistent.\n\n\n\n\n3.1.2 General case\nTime to put on our big-kid pants. For \\(m\\) equations in \\(n\\) variables, we need to use subscripts rather than different letters for everything.\n\nDefinition 3.2 A linear-algebraic system, usually just written linear system, is the set of simultaneous equations \\[\n\\begin{split}\nA_{11} x_1 + A_{12} x_2 + \\cdots + A_{1n} x_n & = b_1 \\\\\nA_{21} x_1 + A_{22} x_2 + \\cdots + A_{2n} x_n & = b_2 \\\\\n& \\vdots \\\\\nA_{m1} x_1 + A_{m2} x_2 + \\cdots + A_{mn} x_n & = b_m,\n\\end{split}\n\\tag{3.1}\\] where all the \\(A_{ij}\\) and \\(b_i\\) are assumed to be known. A solution of the system is a list of values \\(x_1,\\ldots,x_n\\) that makes all the equations true.\n\nWe want to gather similar elements in Equation 3.1 into collective mathematical objects.\n\nDefinition 3.3 (Vector) A vector is a finite collection of numbers known as its elements. The set of all vectors with \\(n\\) real-valued elements is denoted \\(\\real^n\\). If the elements are complex numbers, we use the symbol \\(\\complex^n\\).\n\nI use boldfaced lowercase letters to represent vectors, and a subscript to refer to an individual element within one. For instance, \\(x_3\\) is the third element of a vector \\(\\bfx\\).\nNext, we come to the doubly-indexed values \\(A_{ij}\\). The first index is the equation number, and the second corresponds to the variable number. This implies a 2D table or array of values.\n\nDefinition 3.4 (Matrix) A matrix is an \\(m\\times n\\) array of numbers known as its elements. The set of all \\(m\\times n\\) matrices with real elements is denoted \\(\\rmn{m}{n}\\), and with complex elements it’s \\(\\cmn{m}{n}\\).\n\nI use boldfaced uppercase letters to represent matrices, and a pair of subscripts to refer to an individual element within one. For instance, \\(A_{23}\\) is the element in row 2, column 3 of the matrix \\(\\bfA\\).\n\n\n\n\n\n\nImportant\n\n\n\nMatrix subscripts are always in the order row first, column second. (This is the opposite of Excel—which is just one of myriad reasons that Excel is evil.)\n\n\nIn the context of Equation 3.1, we call the \\(m\\times n\\) matrix \\(\\bfA\\) the coefficient matrix of the linear system.\n\n\n\n\n\n\nImportant\n\n\n\nThe rows of the coefficient matrix correspond to the equations in the linear system, and the columns correspond to the variables of the system.\n\n\nThe vector \\(\\bfb\\in\\real^m\\) implied in Equation 3.1 doesn’t have a standard name, but I will call it the forcing vector by analogy with linear ODEs. Altogether, \\(\\bfA\\) and \\(\\bfb\\) are the data of the linear system, and the vector \\(\\bfx \\in \\real^{n}\\) is the solution.\n\nExample 3.2 Sometimes zeros are needed as placeholders in the coefficient matrix. In the linear system\n\\[\n\\begin{split}\n  x_1 - x_2 + 3 x_3 &= 4,\\\\\n  2x_2 + 5x_4 &= -1,\n\\end{split}\n\\]\nthe coefficient matrix is \\(2\\times 4\\),\n\\[\n\\bfA =\n\\begin{bmatrix}\n1 & -1 & 3 & 0 \\\\ 0 & 2 & 0 & 5\n\\end{bmatrix},\n\\]\nand the right-side vector is \\(\\twovec{4}{-1}\\)."
  },
  {
    "objectID": "linear_algebra_systems.html#row-elimination",
    "href": "linear_algebra_systems.html#row-elimination",
    "title": "3  Linear algebraic systems",
    "section": "3.2 Row elimination",
    "text": "3.2 Row elimination\nYou’ve probably solved small systems of equations by substitution. In order to solve systems with more equations and variables, we systematize this idea as an elimination algorithm. The goal of elimination is to transform the system to an equivalent one whose solution(s) we can deduce easily. There are three legal moves in this game:\n\nDefinition 3.5 (Row operations) The following operations do not change the solutions of a linear system:\n\nSwap the positions of two equations.\nMultiply an equation by a nonzero constant.\nAdd a multiple of one equation to another equation.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that the equations of a linear system correspond to rows of the coefficient matrix \\(\\bfA\\) and forcing vector \\(\\bfb\\), hence the name “row operations.”\n\n\n\nExample 3.3 Let’s use elimination to solve the \\(3\\times 3\\) system\n\\[\n\\begin{split}\nx_1 - x_2  - x_3 & = 2, \\\\\n3x_1 - 2x_2 & = 9, \\\\\nx_1 - 2x_2 - x_3 & = 5.\n\\end{split}\n\\]\nThe first step is to use the first equation to eliminate \\(x_1\\) from the second and third equations. We therefore add \\(-3\\) times equation 1 to equation 2, and \\(-1\\) times equation 1 to equation 3:\n\\[\n\\begin{split}\nx_1 - x_2  - x_3  & = 2, \\\\\n(3x_1 - 2x_2) - 3(x_1 - x_2  - x_3) & = 9 - 3(2),\\\\\n(x_1 - 2x_2 - x_3) - 1(x_1 - x_2  - x_3) & = 5 - 1(2) .\n\\end{split}\n\\]\nThis takes us to \\[\n\\begin{split}\nx_1 - x_2  - x_3  & = 2,  \\\\\nx_2 + 3x_3 &= 3, \\\\\n-x_2 &= 3.\n\\end{split}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIt’s tempting to grab that last equation above and use it to remove \\(x_2\\) from everything else. That move certainly works out here. But in the context of a fully automatic algorithm that works every time, we will continue as though the last equation still might contain a dependence on \\(x_3\\).\n\n\nNext, we leave the first equation alone and use the second to eliminate \\(x_2\\) from all the others below it.\n\\[\n\\begin{split}\nx_1 - x_2  - x_3  & = 2, \\\\\nx_2 + 3x_3 & = 3, \\\\\n(-x_2) + (x_2+3x_3)  & = 3 + 3.\n\\end{split}\n\\]\nWe now have the system, equivalent to the original one, comprising\n\\[\n\\begin{split}\nx_1 - x_2  - x_3  & = 2, \\\\\nx_2 + 3x_3 & = 3, \\\\\n3x_3  & = 6.\n\\end{split}\n\\]\nWe can now deduce that \\(x_3=2\\) from the last equation. Moving up to the second equation, we then find that \\(x_2=3-3(2)=-3\\). Finally, the first equation yields \\(x_1=2+(-3)+(2)=1\\).\n\nThe process in Example 3.3 leading to triangular system is commonly known as Gaussian elimination. (It’s a poor name, as the process was known at least in China thousands of years before Gauss.) Finding the solution from the triangular form is called backward substitution.\n\n3.2.1 Augmented matrix\nBefore looking further into the elimination process, we will lighten the notational load by using matrices.\n\nDefinition 3.6 The augmented matrix of the linear system Equation 3.1 is the \\(m\\times (n+1)\\) matrix \\(\\mathbf{G} = \\augmat{\\bfA}{\\bfb}\\).\n\n\nExample 3.4 The starting system in Example 3.3 has augmented matrix\n\\[\n\\augmat{\\begin{matrix}\n1 & -1 & -1  \\\\\n3 &-2 & 0 \\\\\n1 & -2 & -1\n\\end{matrix}}{\n  \\begin{matrix}2\\\\9\\\\5\\end{matrix}\n  }.\n\\]\nThe final system in that example has augmented matrix \\[\n\\augmat{\\begin{matrix}\n1 & -1 & -1  \\\\\n0 & 1 & 3 \\\\\n0 & 0 & 3\n\\end{matrix}}{\n  \\begin{matrix}2\\\\3\\\\6\\end{matrix}\n  }.\n\\]\n\nThe nonzeros in the final augmented matrix have a triangular shape that is the key to making back substitution work. Next, we will get specific about what this form requires.\n\n\n3.2.2 Row echelon form\nHere is a formal description of the goal of row elimination.\n\nDefinition 3.7 (Row echelon form (RE form)) A matrix is in RE form if:\n\nAny rows that are completely zero are at the bottom.\nThe leftmost nonzero element in a row, called the leading element, is to the left of any other leading elements below it.\n\n\nTo achieve RE form from a given starting matrix, we start at the top left and use row operations to introduce zeros under the leading element of the first row. We then move down one row and repeat. It’s easier to understand from more examples than to write out a formal algorithm.\n\nExample 3.5 We start from \\[\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 2 & 4 & 0 & 2 \\\\ 2 & 3 & 2 & 5 \\\\ -1 & 1 & 3 & 5\n\\end{bmatrix}\n\\]\nIn the first row, the 1 will be the leading element. We then multiples of the first row to create zeros in the rows underneath it.\n\\[\n\\begin{split}\n& \\stackrel{R_2=R_2 - 2R_1}{\\Longrightarrow}\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 0 & 0 & 8 & -8 \\\\ 2 & 3 & 2 & 5 \\\\ -1 & 1 & -8 & 5\n\\end{bmatrix} \\\\[1ex]\n& \\stackrel{R_3=R_3 - 2R_1}{\\Longrightarrow}\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 0 & 0 & 8 & -8 \\\\ 0 & -1 & 10 & -5 \\\\ -1 & 1 & -8 & 5\n\\end{bmatrix} \\\\[1ex]\n& \\stackrel{R_4=R_4 + 1R_1}{\\Longrightarrow}\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 0 & 0 & 8 & -8 \\\\ 0 & -1 & 10 & -5 \\\\ 0 & 3 & -12 & 10\n\\end{bmatrix}\n\\end{split}\n\\]\nNow that leading 1 in the first row is all set. From now on, we can safely ignore the first column, as those zeros will remain throughout.\nMoving to the second row, we notice that if we designated the 8 to be the leading element, then at least one of the rows underneath it would have a leading element to its left. That’s a violation of RE form. So we can swap either of those rows with the current row 2:\n\\[\n\\stackrel{R_2 \\leftrightarrow R_3}{\\Longrightarrow}\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 0 & -1 & 10 & -5 \\\\ 0 & 0 & 8 & -8 \\\\ 0 & 3 & -12 & 10\n\\end{bmatrix}\n\\]\nWe now can select \\(-1\\) as the leading element in row 2, and create zeros underneath it. Of course, we don’t have any work to do in the current row 3.\n\\[\n\\stackrel{R_4 = R_4 + 3R_2}{\\Longrightarrow}\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 0 & -1 & 10 & -5 \\\\ 0 & 0 & 8 & -8 \\\\ 0 & 0 & 18 & -5\n\\end{bmatrix}\n\\]\nOur first two rows are set. We’re happy choosing \\(8\\) as the leading element in row 3, and one more operation is needed to clear the element below it:\n\\[\n\\stackrel{R_4 = R_4 - \\frac{18}{8}R_3}{\\Longrightarrow}\n\\begin{bmatrix}\n1 & 2 & -4 & 5 \\\\ 0 & -1 & 10 & -5 \\\\ 0 & 0 & 8 & -8 \\\\ 0 & 0 & 0 & 13\n\\end{bmatrix}\n\\]\nThe matrix is now in RE form.\nIf our starting point is the augmented matrix of a linear system, then our new linear system would be\n\\[\n\\begin{split}\n1x_1 + 2x_2 -4x_3 &= 5  \\\\\n0x_1 - 1x_2 +10x_3 &= -5 \\\\\n0x_1 + 0x_2 +8x_3 &= -8 \\\\\n0x_1 + 0x_2 + 0x_3 &= 13\n\\end{split}\n\\]\nThat last equation is a dealbreaker! The system is inconsistent.\n\n\nDefinition 3.8 (Pivots) The leading elements encountered during row elimination are called pivots. The columns they occur in are called leading variables, while the other variables (if any) are called free variables.\n\n\nExample 3.6 Suppose we begin with the augmented matrix \\[\n\\augmat{\\begin{matrix}\n1 & 1 & -1  \\\\\n1 & 2 & 2  \\\\\n2 & 1 & -5\n\\end{matrix}}{\n  \\threevec{4}{3}{9}\n}.\n\\]\nThe first 1 in the first row is a fine pivot, leading to\n\\[\n\\stackrel{R_2 = R_2 - R_1;\\; R_3 = R_3 - 2R_1}{\\Longrightarrow}\n\\augmat{\\begin{matrix}\n1 & 1 & -1  \\\\\n0 & 1 & 3  \\\\\n0 & -1 & -3\n\\end{matrix}}{\n  \\begin{matrix}4\\\\-1\\\\1\\end{matrix}\n}\n\\]\nThe leading 1 in the second row is the next pivot:\n\\[\n\\stackrel{R_3 = R_3 + R_2}{\\Longrightarrow}\n\\augmat{\\begin{matrix}\n1 & 1 & -1  \\\\\n0 & 1 & 3  \\\\\n0 & 0 & 0\n\\end{matrix}}{\n  \\begin{matrix}4\\\\-1\\\\0\\end{matrix}\n}.\n\\]\nOnly the first two variables are considered leading variables, and \\(x_3\\) remains free. So we say that \\(x_3=t\\) for any arbitrary value of \\(t\\). The last row of the final augmented matrix represents the equation \\(0=0\\), which is true and implies no constraint. The second row implies\n\\[\nx_2 + 3x_3 = -1,\n\\]\nwhich is equivalent to \\(x_2=-1-3t\\). Finally, the first row says that\n\\[\nx_1 + x_2 - x_3 = 4,\n\\]\nso that \\(x_1 = 4 - (-1-3t) + (t) = 5+4t\\). The system has infinitely many solutions."
  },
  {
    "objectID": "linear_algebra_systems.html#rre-form",
    "href": "linear_algebra_systems.html#rre-form",
    "title": "3  Linear algebraic systems",
    "section": "3.3 RRE form",
    "text": "3.3 RRE form\nAn alternative to backward substitution is to transform the RE form further into something even simpler.\n\nDefinition 3.9 (RRE form) A matrix is in RRE form (reduced row-echelon form) if it meets all of these requirements:\n\nAny rows of all zeros appear below all nonzero rows.\nThe leading element of any row is a one.\nEvery leading 1 that is lower than another leading 1 is also to the right of it.\nEvery leading 1 is the only nonzero in its column.\n\nThe columns that have leading ones are called pivot columns. The other columns are called free columns.\n\nThere are two things that make RRE form useful. One is that it makes a unique target:\n\nTheorem 3.1 Every matrix is equivalent to a unique matrix in RRE form.\n\nThe other useful fact is that RRE form completely exposes everything we want to know about a linear system. In fact, everything else we will do theoretically with matrices comes back to the RRE form, whether or not it’s an augmented matrix.\n\nExample 3.7 At the end of Example 3.3, we had reached the augmented matrix\n\\[\n\\augmat{\\begin{matrix}\n1 & -1 & -1  \\\\\n0 & 1 & 3 \\\\\n0 & 0 & 3\n\\end{matrix}}{\n  \\begin{matrix}2\\\\3\\\\6\\end{matrix}\n  }.\n\\]\nNote that the leading element of row 3 is not a 1. We fix that by multiplying it through:\n\\[\n\\stackrel{R_3 = \\frac{1}{3}R_3}{\\Longrightarrow}\n\\augmat{\\begin{matrix}\n1 & -1 & -1  \\\\\n0 & 1 & 3 \\\\\n0 & 0 & 1\n\\end{matrix}}{\n  \\begin{matrix}2\\\\3\\\\2\\end{matrix}\n  }.\n\\]\nThe leading one in row 1 is alone in its column. But the other two leading ones are not. We start at the bottom right and eliminate upwards now:\n\\[\n\\begin{split}\n& \\stackrel{R_2 = R_2 - 3R_3}{\\Longrightarrow}\n\\augmat{\\begin{matrix}\n1 & -1 & -1  \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{matrix}}{\n  \\begin{matrix}2\\\\-3\\\\2\\end{matrix}\n  } \\\\[1ex]\n  & \\stackrel{R_1 = R_1 + 1R_3}{\\Longrightarrow}\n\\augmat{\\begin{matrix}\n1 & -1 & 0  \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{matrix}}{\n  \\begin{matrix}4\\\\-3\\\\2\\end{matrix}\n  }.\n\\end{split}\n\\]\nAt this point, the leading one of row 3 is ready to go. We have just one more upwards step to perform from row 2:\n\\[\n\\stackrel{R_1 = R_1 + 1R_2}{\\Longrightarrow}\n\\augmat{\\begin{matrix}\n1 & 0 & 0  \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{matrix}}{\n  \\begin{matrix}1\\\\-3\\\\2\\end{matrix}\n  }.\n\\]\nThe system is now trivial: \\(x_1=1\\), \\(x_2=-3\\), and \\(x_3=2\\). Done!\n\nThe process above is called Gauss–Jordan elimination, to distinguish it from Gaussian elimination. \n\nExample 3.8 Here is an example of reduction to RRE form for a linear system using Julia to do the arithmetic.\n\\[\n\\bfA = \\begin{bmatrix}\n2 & 0 & 4 & 3 \\\\ -2 & 0 & 2 & -13 \\\\ -4 & 5 & -7 & -10 \\\\ 1 & 15 & 2 & -4.5\n\\end{bmatrix}, \\qquad\n\\bfb = \\begin{bmatrix}\n4\\\\40\\\\9\\\\29\n\\end{bmatrix}.\n\\]\n\n\nCode\nA = [\n     2    0    4    3 \n    -2    0    2  -13\n    -4    5   -7  -10 \n     1   15    2   -4.5\n    ];\nb = [ 4, 40, 9, 29 ];\n\nS = Rational.([A b])\n\n\n4×5 Matrix{Rational{Int64}}:\n  2//1   0//1   4//1    3//1   4//1\n -2//1   0//1   2//1  -13//1  40//1\n -4//1   5//1  -7//1  -10//1   9//1\n  1//1  15//1   2//1   -9//2  29//1\n\n\nWe start at the top, working downward and rightward. In the first row, the leading nonzero occurs in column 1, which is the pivot column for this row. We normalize this row so that the leading nonzero is a 1.\n\n\nCode\nS[1,:] = S[1,:] / S[1,1]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n  1//1   0//1   2//1    3//2   2//1\n -2//1   0//1   2//1  -13//1  40//1\n -4//1   5//1  -7//1  -10//1   9//1\n  1//1  15//1   2//1   -9//2  29//1\n\n\nNow multiples of row 1 are added to the rows below it in order to put zeros in the first column.\n\n\nCode\nS[2,:] = S[2,:] - S[2,1]*S[1,:]\nS[3,:] = S[3,:] - S[3,1]*S[1,:]\nS[4,:] = S[4,:] - S[4,1]*S[1,:]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1   0//1  2//1    3//2   2//1\n 0//1   0//1  6//1  -10//1  44//1\n 0//1   5//1  1//1   -4//1  17//1\n 0//1  15//1  0//1   -6//1  27//1\n\n\nLooking at rows 2 to 4, we see that the leftmost nonzero occurs in column 2. Since row 2 has a zero there, we swap rows 2 and 3 to bring a nonzero up.\n\n\nCode\nS[[2, 3],:] = S[[3, 2],:]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1   0//1  2//1    3//2   2//1\n 0//1   5//1  1//1   -4//1  17//1\n 0//1   0//1  6//1  -10//1  44//1\n 0//1  15//1  0//1   -6//1  27//1\n\n\nNow we normalize row 2 so that the leading nonzero is a 1.\n\n\nCode\nS[2,:] = S[2,:] / S[2,2]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1   0//1  2//1    3//2   2//1\n 0//1   1//1  1//5   -4//5  17//5\n 0//1   0//1  6//1  -10//1  44//1\n 0//1  15//1  0//1   -6//1  27//1\n\n\nMultiples of row 2 are now used to put zeros below it in the pivot column.\n\n\nCode\nS[3,:] = S[3,:] - S[3,2]*S[2,:]\nS[4,:] = S[4,:] - S[4,2]*S[2,:]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1  0//1   2//1    3//2    2//1\n 0//1  1//1   1//5   -4//5   17//5\n 0//1  0//1   6//1  -10//1   44//1\n 0//1  0//1  -3//1    6//1  -24//1\n\n\nRows 3 and 4 have a pivot in column 3, and we only need to normalize row 3 to make it 1. Then we subtract a multiple of row 3 from row 4 to put a zero beneath it.\n\n\nCode\nS[3,:] = S[3,:] / S[3,3];\nS[4,:] = S[4,:] - S[4,3]*S[3,:]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1  0//1  2//1   3//2   2//1\n 0//1  1//1  1//5  -4//5  17//5\n 0//1  0//1  1//1  -5//3  22//3\n 0//1  0//1  0//1   1//1  -2//1\n\n\nWe complete the downward phase by normalizing row 4 to get a leading 1.\n\n\nCode\nS[4,:] = S[4,:] / S[4,4]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1  0//1  2//1   3//2   2//1\n 0//1  1//1  1//5  -4//5  17//5\n 0//1  0//1  1//1  -5//3  22//3\n 0//1  0//1  0//1   1//1  -2//1\n\n\nNow we turn around for the upward phase. The leading 1 in row 4 needs to have zeros above it. We accomplish that by subtracting multiples of row 4 from the others.\n\n\nCode\nS[3,:] = S[3,:] - S[3,4]*S[4,:]\nS[2,:] = S[2,:] - S[2,4]*S[4,:]\nS[1,:] = S[1,:] - S[1,4]*S[4,:]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1  0//1  2//1  0//1   5//1\n 0//1  1//1  1//5  0//1   9//5\n 0//1  0//1  1//1  0//1   4//1\n 0//1  0//1  0//1  1//1  -2//1\n\n\nWe move up to row 3 and use multiples of it to put zeros above its leading 1.\n\n\nCode\nS[2,:] = S[2,:] - S[2,3]*S[3,:]\nS[1,:] = S[1,:] - S[1,3]*S[3,:]\nS\n\n\n4×5 Matrix{Rational{Int64}}:\n 1//1  0//1  0//1  0//1  -3//1\n 0//1  1//1  0//1  0//1   1//1\n 0//1  0//1  1//1  0//1   4//1\n 0//1  0//1  0//1  1//1  -2//1\n\n\nThe last move is to use a multiple of row 2 to put a zero above its leading 1. As it has played out in this example, this line of code changes nothing because the position was already zero.\n\n\nCode\nS[1,:] = S[1,:] - S[1,2]*S[2,:]\nfloat(S)\n\n\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  -3.0\n 0.0  1.0  0.0  0.0   1.0\n 0.0  0.0  1.0  0.0   4.0\n 0.0  0.0  0.0  1.0  -2.0\n\n\nThis matrix is in RREF. We interpret it as the trivial linear system\n\\[\n\\begin{split}\nx_1 &= -3,\\\\ x_2 &= 1,\\\\ x_3 &= 4, \\\\ x_4 &= -2.\n\\end{split}\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Wolfram Alpha to find the RRE form of a matrix.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhile MATLAB has an rref command, it should not be relied on. It isn’t really MATLAB’s fault; the RRE form is a terrible target for numerical computations, because it is infinitely sensitive to rounding errors, and MATLAB calculates using numerical approximations for speed.\n\n\n\n3.3.1 Solution from the RRE form\nThe RRE form of an augmented matrix represents a linear system that we can solve by inspection.\n\nTheorem 3.2 To solve a system with augmented matrix in RRE form:\n\nIgnore all zero rows.\nIf a leading one occurs in the last column, the system is inconsistent.\nOtherwise, each variable associated with a free column is assigned to a free parameter (e.g., \\(s\\), \\(t\\), etc.).\nUse the leading columns to solve for their corresponding variables in terms of the free parameters.\n\n\n\nExample 3.9 A linear system has an augmented matrix equivalent to the RRE matrix\n\\[\n\\augmat{\n\\begin{matrix}\n0 & 1 & -3 & 0 \\\\\n0 & 0 & 0  & 1  \\\\\n0 & 0 & 0  & 0   \\\\\n0 & 0 & 0  & 0  \n\\end{matrix} }{\n  \\begin{matrix}2 \\\\ -4 \\\\ 0 \\\\ 0 \\end{matrix}\n}\n\\]\nFind all solutions to the linear system.\n\nSolution. The last two rows have no information and can be ignored. Columns 2 and 4 are pivot columns. We give variables from the other columns arbitrary values, \\(x_1=s\\) and \\(x_3=t\\). The pivot variables are solved in terms of these to get \\(x_2=2+3t\\) and \\(x_4=-4\\). All solutions are expressed as the vector\n\\[\n\\bfx = \\begin{bmatrix}\ns \\\\ 2+3t \\\\ t \\\\ -4\n\\end{bmatrix}, \\qquad s,t\\in \\real.\n\\]\n\n\n\nExample 3.10 A linear system has an augmented matrix equivalent to the RRE form\n\\[\n\\augmat{\n\\begin{matrix}\n1 & 0 & -4 & 0  \\\\\n0 & 1 & 1  & 0 \\\\\n0 & 0 & 0  & 1  \\\\\n0 & 0 & 0  & 0\n\\end{matrix}}{\n  \\begin{matrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{matrix}\n}\n\\]\nFind all solutions to the linear system.\n\nSolution. Look at the last row of the system. It expresses the equation \\(0=1\\), which is impossible to satisfy. Thus the system is inconsistent.\n\n\n\n\n3.3.2 Rank\nLeaving aside the actual calculation of solutions, a glance at the RRE form of the augmented matrix tells us instantly whether the system has no solution, a unique solution, or infinitely many solutions. Because of the requirements on leading ones in the RREF definition, the outcomes are constrained simply by how many of those ones there are.\n\nDefinition 3.10 (Rank) The rank of a matrix is the number of pivot columns in its RRE form, i. e., the number of leading ones.\n\n\nTheorem 3.3 If \\(\\bfA\\) is an \\(m\\times n\\) matrix, then \\(\\rank(\\bfA) \\le m\\) and \\(\\rank(\\bfA) \\le n\\).\n\n\nProof. Each leading one in the RRE form requires a row and column of its own.\n\n\nTheorem 3.4 Suppose \\(\\bfA\\) is \\(m\\times n\\). If \\(\\rank(\\bfA)&lt;n\\), then the linear system with augmented matrix \\(\\augmat{\\bfA}{\\bfb}\\) cannot have a unique solution. It is inconsistent for some choices of \\(\\bfb\\) and has infinitely many solutions for other choices of \\(\\bfb\\).\n\n\nProof. Let \\(r=\\rank(\\bfA)\\). Then the RRE form of \\(\\bfA\\) has all zero elements in rows \\(r+1\\) to \\(m\\). Now augment \\(\\bfA\\) with \\(\\bfb\\) and perform the row operations that reduce \\(\\bfA\\) to RRE form. If there are any nonzeros in rows \\(r+1\\) to \\(m\\) of the last column, the system is inconsistent. Otherwise, the system has at least one free variable. We can start with either type of outcome and run the row elimination process backward to find a \\(\\bfb\\) that led to it.\n\n\nCorollary 3.1 A linear system with more variables than equations cannot have a unique solution.\n\n\nProof. \\(\\rank(\\bfA) \\le m &lt; n\\)."
  },
  {
    "objectID": "linear_algebra_systems.html#homogeneous-systems",
    "href": "linear_algebra_systems.html#homogeneous-systems",
    "title": "3  Linear algebraic systems",
    "section": "3.4 Homogeneous systems",
    "text": "3.4 Homogeneous systems\nThere are only three possible outcomes for a linear system, all deducible from the RRE form of the augmented matrix:\n\nThere is a leading 1 in the last column, in which case there are no solutions.\nThere are fewer pivot columns than variables, in which case there are infinitely many solutions.\nThere is a unique solution.\n\nThere is an important situation where we can rule out case 1 above.\n\nDefinition 3.11 (Homogeneous linear system) A homogeneous linear system is one that has forcing vector \\(\\bfb\\) equal to zero.\n\nAs the following theorem points out, we can work with just the RRE form of the coefficient matrix \\(\\bfA\\) instead of the augmented matrix of a homogeneous system.\n\nTheorem 3.5 For a homogeneous linear system, the last column of the RRE form of the augmented matrix is a zero vector.\n\n\nProof. Row operations can’t change the zeros in that column.\n\n\n3.4.1 Null space\n\nDefinition 3.12 The nullspace of a matrix \\(\\bfA\\), written \\(\\nullsp(\\bfA)\\), is the set of all solutions to the homogeneous linear system with coefficient matrix \\(\\bfA\\).\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes the null space is called the kernel of the matrix.\n\n\n\nFor a homogeneous linear system, the zero vector \\(\\bfx=\\bfzero\\) is always a solution. The only interesting question is whether zero is all by itself in the nullspace. That is, we only need to look at case 2 from the beginning of this section.\nThe following conclusions ensue immediately from our definitions and previous results. Often, linear algebra is like standup comedy: all the work is in the setup, not the punchline.\n\nTheorem 3.6 A homogeneous system in \\(n\\) variables with coefficient matrix \\(\\bfA\\) has a unique solution if and only if \\(\\rank(\\bfA)=n\\).\n\n\nCorollary 3.2 A homogeneous linear system with more variables than equations has infinitely many solutions.\n\n\n\n3.4.2 Relating to nonhomogeneous systems\nThere is a strong correspondence between a nonhomogeneous linear ODE \\(\\opA[x]=f\\) and a nonhomogeneous linear system \\(\\bfA\\bfx=\\bfb\\).\n\nTheorem 3.7 If \\(\\bfu\\) is in the nullspace of \\(\\bfA\\) and \\(\\bfv\\) is any solution of \\(\\bfA\\bfx = \\bfb\\), then \\(\\bfu+\\bfv\\) is also a solution of \\(\\bfA\\bfx = \\bfb\\).\n\n\nProof. \\[\n\\bfA(\\bfu + \\bfv) = \\bfA\\bfu + \\bfA\\bfv = \\bfzero + \\bfb = \\bfb.\n\\]\n\n\nExample 3.11 Suppose\n\\[\n\\begin{bmatrix}\n2 & -3\n\\end{bmatrix}\n\\bfx = \\begin{bmatrix}\n5\n\\end{bmatrix}.\n\\]\nGeometrically, all solutions \\([x_1,\\:x_2] \\in \\real^2\\) lie on the line defined by \\(2x_1 - 3x_2 = 5\\). Note that the vector \\([1,\\:-1]\\) is one such solution.\nThe null space of the matrix is the set of vectors such that \\(2x_1-3x_2=0\\). If we let \\(x_2=t\\) be a free parameter, then \\(x_1=\\frac{3}{2}t\\). So we can write solutions of the original problem in the form\n\\[\n\\bfx = \\begin{bmatrix}\n\\tfrac{3}{2}t \\\\ t\n\\end{bmatrix} +\n\\begin{bmatrix}\n1 \\\\ -1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 + \\tfrac{3}{2}t \\\\ -1 + t\n\\end{bmatrix},\n\\]\nwhich is simply one way to parameterize the line \\(2x_1 - 3x_2 = 5\\)."
  },
  {
    "objectID": "linear_algebra_systems.html#linear-combinations",
    "href": "linear_algebra_systems.html#linear-combinations",
    "title": "3  Linear algebraic systems",
    "section": "3.5 Linear combinations",
    "text": "3.5 Linear combinations\n\n3.5.1 Spanning sets\nWhen we have infinitely many solutions in a linear system, it’s nice to have a concise description of all of them. This takes us on a considerable side quest.\n\n\nDefinition 3.13 (Linear combination) A linear combination of vectors \\(\\bfv_1, \\ldots, \\bfv_k\\) with coefficients \\(c_1,\\ldots,c_k\\) is the vector\n\\[\nc_1 \\bfv_1 + c_2 \\bfv_2 + \\cdots + c_k \\bfv_k.\n\\]\n\n\nAs we are about to see, statements about linear combinations can be transformed into statements about linear systems of equations, and vice versa.\n\n\n3.5.2 Span\n\nDefinition 3.14 The set of all linear combinations of \\(\\bfv_1,\\ldots,\\bfv_k\\) is known as the span of those vectors. The span of an empty set is the singleton set \\(\\{\\bfzero\\}\\).\n\n\nExample 3.12 Show that \\(\\bfv=[1,4,3]\\) is in the span of \\(\\bfu_1=[1,-2,-1]\\) and \\(\\bfu_2=[3,0,1]\\). (That is, \\(\\bfv\\) can be written as a linear combination of these two vectors.)\n\nSolution. Suppose that \\(\\bfv = c_1 \\bfu_1 + c_2 \\bfu_2\\). Looking at each component of this vector equation, we conclude that\n\\[\n\\begin{split}\n  c_1 + 3c_2 &= 1 \\\\\n  -2c_1 + 0c_2 &= 4 \\\\\n  -c_1 + c_2 &= 3,\n\\end{split}\n\\]\nwhich is a linear system with augmented matrix\n\\[\n\\augmat{\n\\begin{matrix}\n  1 & 3  \\\\ -2 & 0  \\\\ -1 & 1\n\\end{matrix}}{\n    \\begin{matrix}\n   1 \\\\  4 \\\\  3\n\\end{matrix}}\n\\]\nThe RRE form of this matrix is\n\\[\n\\stackrel{RRE}{\\Longrightarrow}\n\\augmat{\n\\begin{matrix}\n  1 & 0  \\\\ 0 & 1  \\\\ 0 & 0\n\\end{matrix}}{\n    \\begin{matrix}\n   -2 \\\\  1 \\\\  0\n\\end{matrix}\n}\n\\]\nwhich is consistent. In fact, the unique solution is \\(c_2=1\\), \\(c_1=2\\).\n\n\nHere is a formal statement of the idea used in Example 3.12.\n\nTheorem 3.8 Let \\(\\bfa_1,\\ldots,\\bfa_n\\) be vectors in \\(\\real^n\\), and let\n\\[\n\\bfA = \\begin{bmatrix}\n\\bfa_1 & \\bfa_2 & \\cdots & \\bfa_n\n\\end{bmatrix} \\in \\rmn{m}{n}.\n\\]\nThen a vector \\(\\bfb \\in \\real^m\\) lies in the span of \\(\\bfa_1,\\ldots,\\bfa_n\\) if and only if the system with augmented matrix \\(\\augmat{\\bfA}{\\bfb}\\) is consistent.\n\nThe idea behind span is that it allows us to represent an infinite set, such as a null space, using a finite number of vectors.\n\nExample 3.13 Suppose the RRE form of a matrix \\(\\bfA\\) is\n\\[\n\\mathbf{R} =\n\\begin{bmatrix}\n0 & 1 & 4 & 0 & 1 \\\\\n0 & 0 & 0  & 1 & -3 \\\\\n0 & 0 & 0  & 0 & 0  \\\\\n0 & 0 & 0  & 0 & 0\n\\end{bmatrix}.\n\\]\nTherefore, the RRE form of the augmented matrix \\(\\augmat{\\mathbf{A}}{\\boldsymbol{0}}\\) is \\(\\augmat{\\mathbf{R}}{\\boldsymbol{0}}\\). The pivot columns are 2 and 4, which makes \\(x_1=r\\), \\(x_3=s\\), and \\(x_5=t\\) free variables. The nonzero rows lead to\n\\[\n\\begin{split}\nx_2 + 4x_3 + x_5 &= 0 &\\quad ⇒ \\quad x_2 &= -4s-t,\\\\\nx_4 -3x_5 &= 0 & \\quad ⇒ \\quad x_4 &= 3t.\n\\end{split}\n\\]\nOne way to express a generic solution vector in the null space is by a linear combination of constant vectors:\n\\[\n\\begin{bmatrix}\n  r \\\\ -4s-t \\\\ s \\\\ 3t\\\\ t\n\\end{bmatrix}\n= r \\begin{bmatrix}\n  1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n+ s \\begin{bmatrix}\n  0 \\\\ -4 \\\\ 0 \\\\ 1 \\\\ 0\n\\end{bmatrix}\n+ t \\begin{bmatrix}\n  0 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ 1\n\\end{bmatrix} = r\\bfv_1 + s\\bfv_2 + t\\bfv_3 .\n\\]\nHence \\(\\nullsp(\\bfA)=\\span(\\bfv_1,\\bfv_2,\\bfv_3)\\), where the \\(\\bfv_j\\) are the constant vectors above.\n\n\n\n3.5.3 Independence\nNow that we have a finite description of the null space, we want to make sure that the description is as simple as possible. For this, we need to put some fresh paint on an old barn.\n\nDefinition 3.15 (Linear independence) Vectors \\(\\bfv_1,\\bfv_2,\\ldots,\\bfv_n\\) are linearly dependent if there is a way to choose constants \\(c_1,\\ldots,c_n\\), not all zero, such that\n\\[\nc_1\\bfv_1 + c_2\\bfv_2 + \\cdots c_n \\bfv_n = 0.\n\\]\nIf the vectors are not linearly dependent, then they are linearly independent.\n\nBecause linear combinations are equivalent to linear systems, statements about linear independence are equivalent to statements about homogeneous linear systems, and therefore about null spaces.\n\nTheorem 3.9 Let \\(\\bfa_1,\\ldots,\\bfa_n\\) be vectors in \\(\\real^n\\), and let\n\\[\n\\bfA = \\begin{bmatrix}\n\\bfa_1 & \\bfa_2 & \\cdots & \\bfa_n\n\\end{bmatrix} \\in \\rmn{m}{n}.\n\\]\nThe following statements are equivalent.\n\nThe vectors \\(\\bfa_1,\\ldots,\\bfa_n\\) are linearly independent.\nThe system with augmented matrix \\(\\augmat{\\bfA}{\\bfzero}\\) has only zero as a solution.\nThe null space of \\(\\bfA\\) contains only the zero vector.\n\\(\\rank(\\bfA) = n\\).\n\n\n\nExample 3.14 Any set of vectors that contains the zero vector must be dependent. That’s because the zero vector will put a column of zeros in the matrix \\(\\bfA\\) in Theorem 3.9, and since that column can’t possibly have a leading nonzero in it, the rank of \\(\\bfA\\) will be less than the number of columns.\n\n\nExample 3.15 Determine whether the vectors \\(\\bfv_1=\\threevec{1}{0}{-2}\\), \\(\\bfv_2=\\threevec{1}{0}{1}\\), and \\(\\bfv_3 = \\threevec{0}{0}{-1}\\) are linearly independent.\n\nSolution. These vectors are dependent if, and only if, we can find \\(c_1,c_2,c_3\\), not all zero, such that\n\\[\nc_1 \\bfv_1 + c_2 \\bfv_2 + c_3 \\bfv_3 = \\bfzero.\n\\]\nEquating components on the two sides of this equation leads to the linear system with augmented matrix\n\\[\n\\augmat{\n    \\begin{matrix}\n    1 & 1 & 0  \\\\ 0 & 0 & 0  \\\\ -2 & 1 & -1\n    \\end{matrix}}{\n\\begin{matrix}\n    0 \\\\  0 \\\\  0\n    \\end{matrix}\n}\n\\]\nThe RRE form of this system is\n\\[\n\\augmat{\n    \\begin{matrix}\n    1 & 0 & \\frac{1}{3}  \\\\ 0 & 1 & -\\frac{1}{3}  \\\\ 0 & 0 & 0\n    \\end{matrix}}{\n\\begin{matrix}\n    0 \\\\  0 \\\\  0\n    \\end{matrix}\n}\n\\]\nwhose solutions are \\([-t/3,t/3,t]\\) for all \\(t\\). Since there are nonzero solutions to the linear system, the original set of vectors is linearly dependent.\n\n\nBesides the presence of a zero vector, there is one easy case to detect dependence.\n\nTheorem 3.10 Any collection of \\(n\\) vectors in \\(\\real^m\\) is dependent if \\(n &gt; m\\).\n\n\nProof. The rank of \\(\\bfA\\) in Theorem 3.9 is no greater than \\(m\\), which is less than \\(n\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nTheorem 3.10 says, “if \\(n &gt; m\\), then dependent,” but the converse statement, “if dependent, then \\(n &gt; m\\),” is not logically correct. There are examples of dependent sets of vectors with \\(n &lt; m\\), \\(n=m\\), and \\(n&gt;m\\).\n\n\nDependent vectors lengthen the description of a spanned space without contributing any information. We will come back to finish this thought in the next chapter. But first, we are going to take a closer look at the algebra of vectors and matrices."
  },
  {
    "objectID": "matrix_algebra.html#elementwise-operations",
    "href": "matrix_algebra.html#elementwise-operations",
    "title": "4  Matrix algebra",
    "section": "4.1 Elementwise operations",
    "text": "4.1 Elementwise operations\nIn this game, we often refer to mere numbers as scalars. That’s because they just scale every element, like in\n\\[\nc \\begin{bmatrix}\nA_{11} & \\cdots & A_{1n} \\\\\n\\vdots & & \\vdots \\\\\nA_{m1} & \\cdots & A_{mn}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ncA_{11} & \\cdots & cA_{1n} \\\\\n\\vdots & & \\vdots \\\\\ncA_{m1} & \\cdots & cA_{mn}\n\\end{bmatrix}.\n\\]\nIt’s easy to add or subtract two vectors or two matrices that have the same size. Just act elementwise:\n\\[\n\\begin{bmatrix}\nA_{11} & \\cdots & A_{1n} \\\\\n\\vdots & & \\vdots \\\\\nA_{m1} & \\cdots & A_{mn}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nB_{11} & \\cdots & B_{1n} \\\\\n\\vdots & & \\vdots \\\\\nB_{m1} & \\cdots & B_{mn}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nA_{11}+B_{11} & \\cdots & A_{1n}+B_{1n} \\\\\n\\vdots & & \\vdots \\\\\nA_{m1}+B_{m1} & \\cdots & A_{mn}+B_{mn}\n\\end{bmatrix}\n\\]\nWe consider the operation of adding matrices of different sizes to be undefined.\n\n\n\n\n\n\nNote\n\n\n\nMathematically, we leave the operation of adding a scalar to a vector or matrix undefined as well, although MATLAB and NumPy will happily do that for you.\n\n\nYou would probably expect that we define matrix multiplication similarly:\n\\[\n\\begin{bmatrix}\nA_{11} & \\cdots & A_{1n} \\\\\n\\vdots & & \\vdots \\\\\nA_{m1} & \\cdots & A_{mn}\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nB_{11} & \\cdots & B_{1n} \\\\\n\\vdots & & \\vdots \\\\\nB_{m1} & \\cdots & B_{mn}\n\\end{bmatrix}\n\\stackrel{??}{=}\n\\begin{bmatrix}\nA_{11}B_{11} & \\cdots & A_{1n}B_{1n} \\\\\n\\vdots & & \\vdots \\\\\nA_{m1}B_{m1} & \\cdots & A_{mn}B_{mn}\n\\end{bmatrix}\n\\]\nBut we don’t! OK, technically this is called a Hadamard product, and it has some uses. But 99.9999% of the time a different, less obvious way of multiplying matrices does a better job of respecting critical mathematical structure."
  },
  {
    "objectID": "matrix_algebra.html#matrix-times-vector",
    "href": "matrix_algebra.html#matrix-times-vector",
    "title": "4  Matrix algebra",
    "section": "4.2 Matrix times vector",
    "text": "4.2 Matrix times vector\nThe idea of linear combinations, as defined in Definition 3.13, serves as the foundation of multiplication between a matrix and a vector.\n\nDefinition 4.1 (Matrix times vector) Given \\(\\bfA\\in\\cmn{m}{n}\\) and \\(\\bfx\\in\\complex^{n}\\), the product \\(\\bfA\\bfx\\) is defined as\n\\[\n\\bfA\\bfx = x_1 \\bfa_1 + x_2 \\bfa_2 + \\cdots + x_n \\bfa_n = \\sum_{j=1}^n x_j \\bfa_j,\n\\tag{4.1}\\]\nwhere \\(\\bfa_j\\) refers to the \\(j\\)th column of \\(\\bfA\\).\n\n\nIn order for \\(\\bfA\\bfx\\) to be defined, the number of columns in \\(\\bfA\\) has to be the same as the number of elements in \\(\\bfx\\).\n\nNote that when \\(\\bfA\\) is \\(m\\times n\\), then \\(\\bfx\\) must be in \\(\\real^n\\) or \\(\\complex^n\\), and \\(\\bfA\\bfx\\) has dimension \\(m\\).\n\nExample 4.1 Calculate the product\n\\[\n\\begin{bmatrix}\n1 & -1 & -1 \\\\ 3 & -2 & 0 \\\\ 1 & -2 & -1 \\end{bmatrix} \\threevec{-1}{2}{-1}.\n\\]\n\nSolution. The product is equivalent to\n\\[\n(-1) \\threevec{1}{3}{1} + (2) \\threevec{-1}{-2}{-2} + (-1) \\threevec{-1}{0}{-1} = \\threevec{-2}{-7}{-4}.\n\\]\nWe don’t often write out the product in this much detail. Instead we “zip together” the rows of the matrix with the entries of the vector:\n\\[\n\\threevec{(-1)(1)+(2)(-1)+(-1)(-1)}{(-1)(3)+(2)(-2)+(-1)(0)}{(-1)(1)+(2)(-2)+(-1)(-1)}  = \\threevec{-2}{-7}{-4}.\n\\]\nYou might recognize the “zip” expressions in this vector as dot products from vector calculus.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can regard a vector \\(\\bfx \\in \\real^n\\) as also being a matrix, in two ways: as a member of \\(\\rmn{1}{n}\\), making it a row vector, or as a member of \\(\\rmn{n}{1}\\), making it a column vector. Our convention is that when we want to interpret a named vector as a matrix, it’s a column vector.\nHowever, that Python assumes a row vector, MATLAB lets you choose either, and Julia considers it a column vector. It’s a mess that can lead to frustrating errors in computer codes.\n\n\n\n4.2.1 Properties\nWhat justifies calling this operation multiplication? In large part, it’s the natural distributive properties\n\\[\n\\begin{split}\n\\bfA(\\bfx+\\bfy) & =  \\bfA\\bfx + \\bfA\\bfy,\\\\\n(\\bfA+\\bfB)\\bfx & =  \\bfA\\bfx + \\bfB\\bfx,\n\\end{split},\n\\]\nand the associative property\n\\[\n\\bfA(\\bfB\\bfx) = (\\bfA\\bfB)\\bfx,\n\\]\nall of which can be checked with a little effort. It’s also true that \\(\\bfA(c\\bfx)=c(\\bfA\\bfx)\\) for any number \\(c\\).\nBut there is a major departure from multiplication as we usually know it.\n\n\n\n\n\n\nWarning\n\n\n\nMatrix-vector products are not commutative. In fact, \\(\\bfx\\bfA\\) is not defined even when \\(\\bfA\\bfx\\) is.\n\n\n\n\n4.2.2 Connection to linear systems\nThe following observation finally brings us back around to the introduction of linear systems through the insultingly simple scalar equation \\(ax=b\\).\n\nTheorem 4.1 The linear system with coefficient matrix \\(\\bfA\\), forcing vector \\(\\bfb\\), and solution \\(\\bfx\\) is equivalent to the equation \\(\\bfA\\bfx=\\bfb\\).\n\nThe following result follows quickly from our definitions.\n\nTheorem 4.2 The linear system \\(\\bfA\\bfx=\\bfb\\) is consistent if and only if \\(\\bfb\\) is in the span of the columns of \\(\\bfA\\).\n\n\n\n4.2.3 Connection to independence\nBecause \\(\\bfA\\bfx\\) is a linear combination of \\(\\bfA\\)’s columns, statements we made previously in connection with linear combinations have corresponding restatements in terms of matrix columns.\n\nTheorem 4.3 The null space of a matrix contains nonzero vectors if and only if the columns of the matrix are linearly dependent.\n\n\nProof. Vector \\(\\bfx\\) is in the nullspace of \\(\\bfA\\) if and only if \\(\\bfA\\bfx=\\bfzero\\). Therefore, if \\(\\bfx\\) is nonzero, then we have a nontrivial linear combination of \\(\\bfA\\)’s columns that gives the zero vector."
  },
  {
    "objectID": "matrix_algebra.html#matrix-times-matrix",
    "href": "matrix_algebra.html#matrix-times-matrix",
    "title": "4  Matrix algebra",
    "section": "4.3 Matrix times matrix",
    "text": "4.3 Matrix times matrix\nWe can think of vectors as a special kind of matrix, and accordingly we can generalize matrix-vector products to matrix-matrix products. There are many equivalent ways to define these products. Here is the one we start with.\n\nDefinition 4.2 (Matrix times matrix) If \\(\\bfA\\) is \\(m\\times n\\) and \\(\\bfB\\) is \\(n\\times p\\), then the product \\(\\bfA\\bfB\\) is defined as\n\\[\n\\bfA\\mathbf{B}\n= \\bfA \\begin{bmatrix} \\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\end{bmatrix}\n= \\begin{bmatrix} \\bfA\\mathbf{b}_1 & \\bfA\\mathbf{b}_2 & \\cdots & \\bfA\\mathbf{b}_p \\end{bmatrix}.\n\\tag{4.2}\\]\n\nIn words, a matrix-matrix product is the horizontal concatenation of matrix-vector products involving the columns of the right-hand matrix.\n\n\n\n\n\n\nWarning\n\n\n\nIn order to define \\(\\bfA\\bfB\\), we require that the number of columns in \\(\\bfA\\) is the same as the number of rows in \\(\\bfB\\). That is, the inner dimensions must agree. The result has size determined by the outer dimensions of the original matrices.\n\n\nWhen we compute a matrix product by hand, we usually don’t write out the above. Instead we use a more compact definition for the individual entries of \\(\\mathbf{C} = \\bfA\\bfB\\),\n\\[\nC_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}, \\qquad i=1,\\ldots,m, \\quad j=1,\\ldots,p.\n\\tag{4.3}\\]\nThe sum to get a single \\(C_{ij}\\) is what we called a “zip”, or essentially a dot product, of row \\(i\\) from \\(\\bfA\\) with column \\(j\\) from \\(\\bfB\\).\n\nExample 4.2 Find \\(\\mathbf{A}\\mathbf{B}\\) if\n\\[\n\\bfA = \\begin{bmatrix}\n1 & -1 \\\\ 0 & 2 \\\\ -3 & 1\n\\end{bmatrix}, \\qquad\n\\mathbf{B} = \\begin{bmatrix}\n2 & -1 & 0 & 4 \\\\ 1 & 1 & 3 & 2\n\\end{bmatrix}.\n\\]\n\nSolution. Using Equation 4.3,\n\\[\n\\begin{split}\n\\bfA\\mathbf{B} &= \\begin{bmatrix}\n(1)(2) + (-1)(1) & (1)(-1) + (-1)(1) & (1)(0) + (-1)(3) & (1)(4) + (-1)(2) \\\\\n(0)(2) + (2)(1) & (0)(-1) + (2)(1) & (0)(0) + (2)(3) & (0)(4) + (2)(2) \\\\\n(-3)(2) + (1)(1) & (-3)(-1) + (1)(1) & (-3)(0) + (1)(3) & (-3)(4) + (1)(2)\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix}\n1 & -2 & -3 & 2 \\\\ 2 & 2 & 6 & 4 \\\\ -5 & 4 & 3 & -10\n\\end{bmatrix}\n\\end{split}.\n\\]\nObserve that\n\\[\n\\bfA \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = 2 \\begin{bmatrix} 1 \\\\ 0 \\\\ -3\n\\end{bmatrix} + 1 \\begin{bmatrix} -1 \\\\ 2 \\\\ 1 \\end{bmatrix}\n= \\begin{bmatrix} 1 \\\\ 2 \\\\ -5 \\end{bmatrix},\n\\]\nand so on.\n\n\n\n4.3.1 Properties\nFirst, there is something fundamentally different about multiplication of matrices compared to multiplication of numbers.\n\n\n\n\n\n\nWarning\n\n\n\nMatrix multiplication is not commutative. If \\(\\bfA\\bfB\\) is defined, then \\(\\bfB\\bfA\\) may not be, and even if it is, it may not equal \\(\\bfA\\bfB\\).\nIn other words, you cannot change the order of the terms in a matrix product without some explicit justification.\n\n\nFortunately, other familiar and handy properties of multiplication do come along for the ride:\n\n\\((\\bfA\\bfB)\\mathbf{C}=\\bfA(\\bfB \\mathbf{C})\\qquad\\) (association)\n\\(\\bfA(\\bfB+\\mathbf{C}) = \\bfA\\bfB + \\bfA\\mathbf{C}\\qquad\\) (right distribution)\n\\((\\bfA+\\bfB)\\mathbf{C} = \\bfA\\mathbf{C} + \\bfB\\mathbf{C}\\qquad\\) (left distribution)\n\nThese properties are easy to check computationally. (But keep in mind that a numerical demonstration, or an algebraic one at particular sizes, is not a general proof.) In addition, matrix multiplication plays well with numbers:\n\n\\((c\\bfA \\bfB) = c (\\bfA \\bfB) = \\bfA (c \\bfB)\\)\n\\(c(\\bfA + \\bfB) = (c\\bfA) + (c\\bfB)\\)\n\\((c+d) \\bfA = (c\\bfA) + (d\\bfA)\\)\n\nFinally, we observe that if \\(\\bfA\\) is \\(m\\times n\\) and \\(\\bfx\\) is an \\(n\\)-vector, then \\(\\bfA\\bfx\\) gives the same result whether we interpret \\(\\bfx\\) as a vector or as an \\(n\\times 1\\) matrix."
  },
  {
    "objectID": "matrix_algebra.html#identity-and-inverse",
    "href": "matrix_algebra.html#identity-and-inverse",
    "title": "4  Matrix algebra",
    "section": "4.4 Identity and inverse",
    "text": "4.4 Identity and inverse\nYou solve \\(ax=b\\) for nonzero \\(a\\) without thinking about it: \\(x=b/a\\). If we do break it down a little, we can see that when we multiply both sides of \\(ax=b\\) by the number \\(1/a\\), then on the left the terms \\(1/a\\) and \\(a\\) combine to give \\(1\\), and \\(1x=x\\). So the key to the solution is the presence of a multiplicative identity value \\(1\\), and the existence of the multiplicative inverse \\(1/a\\) when \\(a\\neq 0\\). These two items are also a way to discuss the vector case \\(\\bfA\\bfx=\\bfb\\).\n\n4.4.1 Identity matrix\nSuppose we are given an \\(m\\times n\\) matrix \\(\\bfA\\). Writing its columns as the vectors \\(\\bfa_1,\\ldots,\\bfa_n\\), we can make the rather obvious observations\n\\[\n\\begin{split}\n\\bfa_1 &= 1\\cdot \\bfa_1 + 0 \\cdot \\bfa_2 + \\cdots + 0\\cdot \\bfa_n,\\\\\n\\bfa_2 &= 0\\cdot \\bfa_1 + 1 \\cdot \\bfa_2 + \\cdots + 0\\cdot \\bfa_n,\\\\\n&\\; \\vdots \\\\\n\\bfa_n &= 0\\cdot \\bfa_1 + 0 \\cdot \\bfa_2 + \\cdots + 1\\cdot \\bfa_n.\n\\end{split}\n\\]\nThe purpose in using these expressions is to interpret them as linear combinations, and thus as matrix-vector products. Let’s define \\(\\bfe_j\\) for \\(j=1,\\ldots,n\\) as follows.\n\nDefinition 4.3 (Standard vectors) \\[\n\\text{$i$th component of }\\bfe_j = \\begin{cases} 1, & i=j, \\\\ 0, & i\\neq j. \\end{cases}\n\\]\n\nNow we can write\n\\[\n\\bfa_j = \\bfA \\bfe_j, \\quad j=1,\\ldots,n.\n\\tag{4.4}\\]\nFurthermore, we can use the definition of matrix products as a concatenation of matrix-vector products to derive\n\\[\n\\begin{split}\n    \\bfA &= \\begin{bmatrix} \\bfa_1 & \\bfa_2 & \\cdots & \\bfa_n \\end{bmatrix} \\\\\n    &=  \\begin{bmatrix} \\bfA\\bfe_1 & \\bfA\\bfe_2 & \\cdots & \\bfA\\bfe_n \\end{bmatrix}\\\\\n    &=  \\bfA \\begin{bmatrix} \\bfe_1 & \\bfe_2 & \\cdots & \\bfe_n \\end{bmatrix}.\n\\end{split}\n\\]\n\nDefinition 4.4 (Identity matrix) The \\(n\\times n\\) identity matrix is\n\\[\n\\meye = \\begin{bmatrix} \\bfe_1 & \\bfe_2 & \\cdots & \\bfe_n \\end{bmatrix} =\n    \\begin{bmatrix}\n    1 & 0 & \\cdots & 0 & 0 \\\\\n    0 & 1 & \\cdots & 0 & 0 \\\\\n    & & \\ddots & & \\\\\n    0 & 0 & \\cdots & 1 & 0 \\\\\n    0 & 0 & \\cdots & 0 & 1\n    \\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes, when we need to indicate the size of the identity, we use a subscript, as in \\(\\meye_4\\) to represent the \\(4\\times 4\\) case. Usually, though, it’s implied by the context.\n\n\n\nTheorem 4.4 (Multiplicative identity) If \\(\\bfA\\) is \\(m\\times n\\), then \\(\\bfA = \\meye_m \\bfA = \\bfA \\meye_n\\).\n\n\nExample 4.3 Compute\n\\[\n\\begin{bmatrix}\n7 & -2 & 11 \\\\ 1131 & \\pi & -\\sqrt{13}\n\\end{bmatrix}\n\\begin{bmatrix}\n2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2\n\\end{bmatrix}.\n\\]\n\nSolution. You can grind through the multiplication algorithm, of course, but there is a shortcut:\n\\[\n\\begin{split}\n    \\begin{bmatrix} 7 & -2 & 11 \\\\ 1131 & \\pi & -\\sqrt{13} \\end{bmatrix}\n    \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}\n    & = \\begin{bmatrix} 7 & -2 & 11 \\\\ 1131 & \\pi & -\\sqrt{13} \\end{bmatrix}\n    \\left( 2 \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\right) \\\\\n    & = 2 \\begin{bmatrix} 7 & -2 & 11 \\\\ 1131 & \\pi & -\\sqrt{13} \\end{bmatrix} \\cdot \\meye \\\\\n    & = \\begin{bmatrix} 14 & -4 & 22 \\\\ 2262 & 2\\pi & -2\\sqrt{13} \\end{bmatrix}.\n\\end{split}\n\\]\n\n\n\n\n4.4.2 Inverse\nWe are now going to introduce a major simplification.\n\nDefinition 4.5 (Square matrix) A square matrix has the same number of rows as columns.\n\nHere is what we seek from a multiplicative inverse.\n\nDefinition 4.6 (Inverse) Suppose \\(\\bfA\\) is a square matrix. A matrix \\(\\mathbf{Z}\\) of the same size such that \\(\\mathbf{Z}\\bfA = \\meye\\) and \\(\\bfA\\mathbf{Z}=\\meye\\) is called the inverse of \\(\\bfA\\), written \\(\\mathbf{Z} = \\bfA^{-1}\\). In this case we say \\(\\bfA\\) is invertible. A matrix that has no inverse is singular.\n\nVerifying whether a given matrix is the inverse of another matrix is simply a matter of multiplying them together and seeing if the result is an identity matrix.\n\nExample 4.4 The matrix \\(\\mathbf{R}(\\theta) = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\\) performs rotation in the plane around the origin by angle \\(\\theta\\). Show that \\(\\mathbf{R}(-\\theta)\\) is the inverse of \\(\\mathbf{R}(\\theta)\\).\n\nSolution. All we need to do is to check that the product, in either order, is the identity matrix:\n\\[\n\\begin{split}\n\\mathbf{R}(-\\theta)\\mathbf{R}(\\theta) &= \\begin{bmatrix}\n\\cos(-\\theta) & -\\sin(-\\theta) \\\\ \\sin(-\\theta) & \\cos(-\\theta)\n\\end{bmatrix} \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\cos(\\theta) & \\sin(\\theta) \\\\ -\\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix} \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\cos^2(\\theta)+\\sin^2(\\theta) & -\\cos(\\theta)\\sin(\\theta) + \\sin(\\theta)\\cos(\\theta) \\\\\n  -\\sin(\\theta)\\cos(\\theta) + \\cos(\\theta)\\sin(\\theta)  & \\sin^2(\\theta) + \\cos^2(\\theta)\n\\end{bmatrix} \\\\\n&= \\twomat{1}{0}{0}{1}.\n\\end{split}\n\\]\n\n\n\n4.4.2.1 Properties\nThere are some facts about inverses that we will use without justification.\n\nTheorem 4.5 If \\(\\bfA\\) and \\(\\bfB\\) are square matrices, then: 1. If \\(\\bfA\\) is invertible, the inverse is unique. 2. If either \\(\\mathbf{Z}\\bfA = \\meye\\) or \\(\\bfA\\mathbf{Z}=\\meye\\) is true, then both are true and \\(\\mathbf{Z}=\\bfA^{-1}\\). 3. \\((\\bfA^{-1})^{-1} = \\bfA\\); that is, \\(\\bfA\\) is the inverse of \\(\\bfA^{-1}\\). 4. If \\(\\bfA\\) is invertible and \\(c\\) is a nonzero number, then \\((c\\bfA)^{-1}= \\dfrac{1}{c}\\bfA^{-1}\\). 5. If \\(\\bfA\\) and \\(\\bfB\\) are invertible and the same size, then \\(\\bfA\\bfB\\) is invertible, and\n\\[\n(\\bfA\\bfB)^{-1} = \\bfB^{-1}\\bfA^{-1}.\n\\]\n\nThe last identity above is easy to get wrong, so it bears restatement in words.\n\n\n\n\n\n\nImportant\n\n\n\nThe inverse of a product is the product of the inverses in the reverse order.\n\n\nThe statement above extends to products of three or more invertible matrices as well.\n\n\n\n4.4.3 Singular matrices\nIf \\(\\mathbf{S}\\) is an \\(n\\times n\\) matrix of all zeros, then \\(\\mathbf{S}\\bfA\\) and \\(\\bfA\\mathbf{S}\\) are also zero matrices whenever the sizes are compatible. Therefore, \\(\\mathbf{S}\\) is singular—no inverse is possible. That much is the same as with numbers. However, there is a major difference with matrices:\n\n\n\n\n\n\nImportant\n\n\n\nSome nonzero matrices are singular.\n\n\n\nExample 4.5 Let \\(\\bfA = \\twomat{0}{0}{1}{0}\\). Suppose that\n\\[\n\\meye = \\twomat{a}{b}{c}{d} \\bfA = \\twomat{b}{0}{d}{0}.\n\\]\nThis is clearly impossible for any choices of \\(a,b,c,d\\). Hence \\(\\bfA\\) is singular.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is possible for the product of two nonzero matrices to be zero.\n\n\n\nExample 4.6 Again let \\(\\bfA = \\twomat{0}{0}{1}{0}\\). Note that\n\\[\n\\bfA^2 = \\twomat{0}{0}{1}{0} \\cdot \\twomat{0}{0}{1}{0} = \\twomat{0}{0}{0}{0}.\n\\]\n\nAs a result, we cannot make the implication \\[\n\\bfA\\bfB = \\bfzero \\implies \\bfA = \\bfzero \\text{ or } \\bfB=\\bfzero, \\qquad \\text{(FALSE!)}\n\\]\nwhich has been so useful when it comes to scalars. Now, if \\(\\bfA\\bfB = \\bfzero\\) and \\(\\bfA\\) is invertible, then we are back in business, because \\[\n\\bfB=\\bfA^{-1}\\cdot \\bfzero = \\bfzero.\n\\]"
  },
  {
    "objectID": "matrix_algebra.html#fundamental-theorem",
    "href": "matrix_algebra.html#fundamental-theorem",
    "title": "4  Matrix algebra",
    "section": "4.5 Fundamental Theorem",
    "text": "4.5 Fundamental Theorem\nThe following theorem is in every linear algebra course, but it does not have a universally accepted name.\n\nTheorem 4.6 (Fundamental Theorem of Linear Algebra, FTLA) If \\(\\bfA\\) is an \\(n\\times n\\) matrix, then each of these statements is equivalent to all of the others.\n\n\\(\\bfA\\) is invertible.\nThe linear system \\(\\bfA\\bfx=\\bfb\\) has the unique solution \\(\\bfx=\\bfA^{-1}\\bfb\\) for each \\(\\bfb\\).\nThe null space of \\(\\bfA\\) is just \\(\\{\\bfzero\\}\\).\nThe RRE form of \\(\\bfA\\) is the identity matrix.\n\\(\\rank(\\bfA)=n\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe statement “\\(\\bfA\\) is singular” for the linear system \\(\\bfA\\bfx = \\bfb\\) is the multidimensional equivalent of “\\(a\\) is zero” in the 1D problem \\(ax=b\\). For a singular matrix, a unique solution is impossible–the system has either no solution or infinitely many of them.\n\n\n\nProof. We’ll only look at statement 1 implying statement 2. Let \\(\\bfx\\) be any vector that solves \\(\\bfb=\\bfA\\bfx\\). Multiply both sides on the left by \\(\\bfA^{-1}\\). Then\n\\[\n\\bfA^{-1} \\bfb =  \\bfA^{-1}(\\bfA\\bfx)= (\\bfA^{-1}\\bfA) \\bfx= \\meye \\bfx = \\bfx.\n\\]\nSince the inverse is unique, \\(\\bfx\\) is unique as well."
  },
  {
    "objectID": "matrix_algebra.html#computing-the-inverse",
    "href": "matrix_algebra.html#computing-the-inverse",
    "title": "4  Matrix algebra",
    "section": "4.6 Computing the inverse",
    "text": "4.6 Computing the inverse\nThe solution formula \\(\\bfx=\\bfA^{-1}\\bfb\\) from Theorem 4.6 is theoretically valuable but can be applied only if the inverse is available. In general, computing a matrix inverse is harder than doing row elimination on a linear system, so it’s not a useful algorithm.\nThere are a few cases for which finding the inverse is not difficult, however.\n\n4.6.1 Diagonal matrix\n\nDefinition 4.7 A diagonal matrix \\(\\mathbf{D}\\) is one in which \\(D_{ij}=0\\) whenever \\(i\\neq j\\).\n\nIf any diagonal element \\(D_{ii}\\) is zero, then a diagonal matrix is singular. Otherwise, its inverse is trivial, thanks to how matrix multiplication is defined.\n\nTheorem 4.7 (Inverse of a diagonal matrix) \\[\n\\begin{bmatrix} a_{11} & & & \\\\  & a_{22} & & \\\\ & & \\ddots & \\\\ & & & a_{nn} \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{a_{11}} & & & \\\\  & \\frac{1}{a_{22}} & & \\\\ & & \\ddots & \\\\ & & & \\frac{1}{a_{nn}} \\end{bmatrix}\n\\tag{4.5}\\]\n\n\n\n4.6.2 \\(2\\times 2\\)\nIn the \\(2\\times 2\\) case, the inverse is easy enough to memorize.\n\nTheorem 4.8 (Inverse of \\(2\\times 2\\)) \\[\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc}\\: \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}.\n\\tag{4.6}\\]\nThis formula breaks down if \\(ad=bc\\), in which case the matrix is singular."
  },
  {
    "objectID": "matrix_algebra.html#sec-matrix-subspaces",
    "href": "matrix_algebra.html#sec-matrix-subspaces",
    "title": "4  Matrix algebra",
    "section": "4.7 Subspaces",
    "text": "4.7 Subspaces\nThe nullspace of a matrix is an example of a vital type of set.\n\nDefinition 4.8 (Subspace) A subspace of \\(\\real^n\\) is a subset \\(S\\) satisfying:\n\nThe zero vector is in \\(S\\).\nEvery linear combination of vectors in \\(S\\) is also in \\(S\\).\n\n\nThe second property above is called closure under linear combination.\n\n\n\n\n\n\nNote\n\n\n\nWe will be making statements about real spaces like \\(\\real^n\\), but everything also works for \\(\\complex^n\\), which turns out to be important later.\n\n\n\nExample 4.7 The equation \\(x + 2y + 3z = 0\\) describes a plane passing through the origin in \\(\\real^3\\). It’s clear geometrically that scaling a vector in the plane leaves you in the plane, and adding vectors in the plane does as well. This is enough to show that this plane is a subspace of \\(\\real^3\\). In fact, it is the null space of the matrix \\(\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}.\\)\nThe equation \\(x+y+z=1\\) is also a plane in \\(\\real^3\\), but it does not pass through the origin, so it cannot be a subspace. (It also fails closure for scaling and addition.)\n\n\nTheorem 4.9 The null space of an \\(m\\times n\\) matrix is a subspace of \\(\\real^n\\).\n\n\nProof. Let \\(S=\\nullsp(\\bfA)\\). If \\(\\bfu\\) and \\(\\bfv\\) are in \\(S\\), then by definition, \\(\\bfA\\bfu = \\bfA\\bfv = \\bfzero\\). Then by basic algebraic properties,\n\\[\n\\bfA( c_1 \\bfu + c_2 \\bfv) = c_1 \\bfA \\bfu + c_2 \\bfA \\bfv = \\bfzero.\n\\]\nThe derivation applies to linear combinations of any length.\n\nThere is at least one easy way to generate subspaces. The following is not hard to prove.\n\nTheorem 4.10 If \\(S=\\span(\\bfv_1,\\ldots,\\bfv_k)\\) for any vectors \\(\\bfv_j\\) in \\(\\real^n\\), then \\(S\\) is a subspace of \\(\\real^n\\).\n\nIn addition to the null space, there is another important subspace closely associated with a matrix.\n\nDefinition 4.9 (Column space) Let \\(\\bfA\\) be an \\(m\\times n\\) matrix. The column space of \\(\\bfA\\), \\(\\colsp(\\bfA)\\), is the span of the columns of \\(\\bfA\\).\n\nBy Theorem 4.10, \\(\\colsp(\\bfA)\\) is a subspace of \\(\\real^m\\).\n\n4.7.1 Basis\n\n\nDefinition 4.10 (Basis) A basis of a subspace \\(S\\) is any set of linearly independent vectors that spans \\(S\\).\n\nFinding a basis for a null space was demonstrated in Example 3.13. The column space is also found from the RRE form.\n\nTheorem 4.11 Let \\(\\bfA\\) have RRE form with pivot columns numbered \\(j_1,\\ldots,j_r\\). Then columns \\(j_1,\\ldots,j_r\\) of \\(\\bfA\\) are a basis for \\(\\colsp(\\bfA)\\).\n\n\nExample 4.8 Find bases for the null space and column space of\n\\[\n\\bfA = \\begin{bmatrix}\n1 & 2 & 0 & -4 \\\\\n-2 & -4 & 1 & 9 \\\\\n-3 & -6 & 1 & 13 \\\\\n-2 & -4 & 0 & 8   \n\\end{bmatrix}.\n\\]\n\nSolution. You can compute that the RRE form of \\(\\bfA\\) is\n\\[\n\\begin{bmatrix}\n1 & 2 & 0 & -4 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0   \n\\end{bmatrix}.\n\\]\nTo get a basis for \\(\\colsp(\\bfA)\\) we choose columns 1 and 3 of \\(\\bfA\\), i. e., \\(\\{[1,-2,-3,-2], [0,1,1,0] \\}\\).\nThe homogeneous system \\(\\bfA\\bfx = \\bfzero\\) has free variables \\(x_2=s\\), \\(x_4=t\\). Solving for the other variables gives the solution set\n\\[\n\\bfx = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 4 \\\\ 0 \\\\ -1 \\\\ 1 \\end{bmatrix},\n\\]\nwhich makes \\(\\{[-2,1,0,0],[4,0,-1,1] \\}\\) a basis for \\(\\nullsp(\\bfA)\\).\n\n\n\nExample 4.9 Find a basis for span of the vectors \\(\\bfv_1=[1,-2,-3,-2]\\), \\(\\bfv_2=[2,-4,-6,-4]\\), \\(\\bfv_3=[0,1,1,0]\\), \\(\\bfv_4=[-4,9,13,8]\\).\n\nSolution. If we put the given vectors in columns of a matrix, then their span is equivalent to the column space of the matrix:\n\\[\n\\colsp\\left(\n\\begin{bmatrix}\n1 & 2 & 0 & -4 \\\\\n-2 & -4 & 1 & 9 \\\\\n-3 & -6 & 1 & 13 \\\\\n-2 & -4 & 0 & 8   \n\\end{bmatrix}\n\\right).\n\\]\nThis is the same matrix whose column space was found in Example 4.8. Columns 1 and 3 are the pivot columns, and we get the basis \\(\\bfv_1,\\bfv_3\\) as before.\n\n\n\n\n4.7.2 Dimension\nYou have an intuitive idea of dimension, but it may seem hard to define rigorously. We’re set up to do that now, thanks to bases.\n\nTheorem 4.12 Every basis for a subspace \\(S\\) has the same number of members.\n\n\nDefinition 4.11 (Dimension) The dimension of a subspace \\(S\\), written \\(\\dim(S)\\), is the number of vectors in any basis of \\(S\\).\n\nAs you would expect, \\(\\dim(\\real^n)=n\\). The only way to have \\(k\\) independent vectors that span a subspace \\(S\\) is if \\(k=\\dim(S)\\). More specifically:\n\nTheorem 4.13 Suppose \\(V\\) is a set of \\(k\\) vectors in subspace \\(S\\).\n\nIf \\(k &lt; \\dim(S)\\), then \\(V\\) cannot span \\(S\\).\nIf \\(k &gt; \\dim(S)\\), then \\(V\\) cannot be linearly independent.\nSuppose \\(k=\\dim(S)\\). If \\(V\\) is independent or if \\(V\\) spans \\(S\\), then \\(V\\) is a basis for \\(S\\).\n\n\n\nExample 4.10 Determine whether the vectors \\(\\bfv_1=[3,2,1]\\), \\(\\bfv_2=[0,-1,2]\\), \\(\\bfv_3=[1,1,1]\\) are a basis of \\(\\real^3\\).\n\nSolution. We find a basis for their span by putting them as columns of a matrix, then looking for the column space. The RRE form of this matrix is a \\(3\\times 3\\) identity, so every column is a pivot column. The original three vectors form a basis for their span, so they are independent. Since there are 3 of them, they are also a basis of \\(\\real^3\\).\n\n\nEarlier we defined rank as the number of pivot columns in the RRE form of the matrix. So now we have\n\nTheorem 4.14 For any \\(m\\times n\\) matrix \\(\\bfA\\), \\(\\rank(\\bfA)=\\dim(\\colsp(\\bfA))\\).\n\nThe dimension of the null space also gets its own name.\n\nDefinition 4.12 (Nullity) The nullity of a matrix \\(\\bfA\\), written \\(\\nullity(\\bfA)\\), is the dimension of \\(\\nullsp(\\bfA)\\).\n\nEach free variable in the RRE form contributes a vector to the basis of \\(\\nullsp(\\bfA)\\). That leads to the following.\n\nTheorem 4.15 For any \\(m\\times n\\) matrix \\(\\bfA\\),\n\n\\(\\dim(\\nullsp(\\bfA))\\) is the number of free variables in the RRE form of \\(\\mathbf{A}\\).\n\\(\\rank(\\bfA) + \\nullity(\\bfA) = n\\).\n\n\nHere is a table that tries to organize much of the language of the linear algebra learned so far.\n\n\nTable 4.1: Related statements in different areas of linear algebra\n\n\n\n\n\n\n\nLinear combinations\nMatrices\nLinear systems\n\n\n\n\n\\(\\bfb\\) is a combination of columns\n\\(\\bfA \\bfx = \\bfb\\)\nSolve \\(\\augmat{\\bfA}{\\bfb}\\)\n\n\ncolumns are dependent\nnontrivial \\(\\nullsp(\\bfA)\\); \\(\\nullity(\\bfA) &gt; 0\\)\nnonzero solution of \\(\\augmat{\\bfA}{\\bfzero}\\)\n\n\ncolumns are independent\n\\(\\rank(A) = \\text{column size of }\\bfA\\)\nno free variables\n\n\ncolumns are a basis\n\\(\\bfA\\) is square; \\(\\bfA^{-1}\\) exists\nunique solution; \\(\\bfA\\) reduces to \\(\\meye\\)"
  },
  {
    "objectID": "matrix_algebra.html#determinants",
    "href": "matrix_algebra.html#determinants",
    "title": "4  Matrix algebra",
    "section": "4.8 Determinants",
    "text": "4.8 Determinants\nThere are many ways to characterize singular matrices, but only a few of them are computationally attractive. One that stands out is a function of square matrices called the determinant. (You probably saw some \\(2\\times 2\\) and \\(3\\times 3\\) determinants in vector calculus. This is the same thing.)\nA definition of the determinant from fundamentals is actually quite tricky. We are going to take a shortcut and define it by a formula for computing it. The \\(2\\times 2\\) case is easy:\n\\[\n\\det\\left( \\twomat{a}{b}{c}{d} \\right) = \\twodet{a}{b}{c}{d} = ad-bc.\n\\]\nThis definition can be bootstrapped into a real-valued function for square matrices of any size.\n\nDefinition 4.13 (Determinant) If \\(\\bfA\\) is \\(n\\times n\\), then its determinant is\n\\[\n\\det(\\bfA) = \\sum (-1)^{i+j} a_{ij} \\det\\bigl( \\mathbf{M}_{ij} \\bigr),\n\\tag{4.7}\\]\nwhere the sum is taken over any row or column of \\(\\bfA\\) and \\(\\mathbf{M}_{ij}\\) is the matrix that results from deleting row \\(i\\) and column \\(j\\) from \\(\\bfA\\).\n\nThe formula Equation 4.7, which is called cofactor expansion, is recursive: the \\(n\\times n\\) case is defined in terms of the \\((n-1)\\times (n-1)\\) case, and so on all the way back down to \\(2\\times 2\\).\nSince expanding along any row or column gives the same result, it can be advantageous to choose one with lots of zeros to cut down on the total computation.\n\nExample 4.11 Compute the determinant of\n\\[\n\\begin{bmatrix}\n2 & 0 & -1 \\\\ -2 & 3 & -1 \\\\ 2 & 0 & 1\n\\end{bmatrix}.\n\\]\n\nSolution. Using cofactor expansion along the first row,\n\\[\n\\begin{split}\n\\begin{vmatrix} 2 & 0 & -1 \\\\ -2 & 3 & -1 \\\\ 2 & 0 & 1 \\end{vmatrix} & =  (2) \\twodet{3}{-1}{0}{1} - (0) \\twodet{-2}{-1}{2}{1} + (-1)\\twodet{-2}{3}{2}{0}    \\\\\n& = 2(3-0) + (-1)(0-6) = 12. \\\\\n\\end{split}\n\\]\nIn this case it might have been a tad easier to exploit the zeros by expanding along the second column instead:\n\\[\n\\begin{split}\n\\begin{vmatrix} 2 & 0 & -1 \\\\ -2 & 3 & -1 \\\\ 2 & 0 &  1 \\end{vmatrix} & =  -(0) \\begin{vmatrix} \\cdots \\end{vmatrix} + (3) \\twodet{2}{-1}{2}{1} - (0)\\begin{vmatrix} \\cdots \\end{vmatrix}    \\\\\n& = 3(2+2) = 12. \\\\\n\\end{split}\n\\]\n\n\n\n4.8.1 Triangular matrices\nThere is one class of matrices for which determinants are super easy to calculate: the triangular matrices.\n\nDefinition 4.14 A matrix \\(\\bfA\\) is upper triangular if \\(A_{ij}=0\\) whenever \\(i&gt;j\\). It is lower triangular if \\(A_{ij}=0\\) whenever \\(i&lt;j\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nA triangular matrix has to have zeros in designated elements, but its other elements may or may not be zero.\n\n\n\n\n\n\n\n\nNote\n\n\n\nA matrix that is both upper and lower triangular is diagonal.\n\n\nThe following ensues easily from cofactor expansion.\n\nTheorem 4.16 The determinant of a triangular matrix is the product of its diagonal elements. That is,\n\\[\n\\det(\\mathbf{T}) = \\prod_{i=1}^n t_{ii}\n\\]\nif \\(\\mathbf{T}\\) is triangular.\n\n\n\n4.8.2 Properties\n\nTheorem 4.17 Let \\(\\bfA\\) and \\(\\bfB\\) be \\(n\\times n\\), and let \\(c\\) be a number. Then\n\n\\(\\det(c\\bfA) = c^n \\det(\\bfA)\\),\n\\(\\det(\\bfA\\bfB) = \\det(\\bfA)\\det(\\bfB)\\),\nIf \\(\\bfA\\) is nonsingular, \\(\\det(\\bfA^{-1})=\\bigl[\\det(\\bfA)\\bigr]^{-1}\\).\n\\(\\det(\\bfA)=0\\) if and only if \\(\\bfA\\) is singular.\n\n\nIt’s the last property above that is of the greatest interest.\n\n\n\n\n\n\nNote\n\n\n\nThe determinant is often the easiest way to check by hand for the invertibility of a small matrix.\n\n\n\n\n4.8.3 Cramer’s Rule\nEven though a 2x2 inverse is easy, it’s still not the most convenient way to solve a linear system \\(\\bfA\\bfx=\\bfb\\) by hand. There is another shortcut known as Cramer’s Rule:\n\\[\n\\begin{split}\nx_1 & = \\frac{ \\twodet{b_1}{a_{12}}{b_2}{a_{22}} }{ \\det(\\bfA) },\\\\[1ex]\nx_2 & = \\frac{ \\twodet{a_{11}}{b_1}{a_{21}}{b_2} }{ \\det(\\bfA) }.\n\\end{split}\n\\]\nObviously this does not work if \\(\\det(\\bfA)=0\\), i. e., when the matrix is singular. In that case, you have to fall back on row elimination.\n\nExample 4.12 Solve\n\\[\\begin{split}\n-x + 3y & = 1 \\\\\n3x + y & = 7\n\\end{split}\\]\nby Cramer’s Rule.\n\nSolution. Plug and play (or is it plug and pray?):\n\\[\\begin{split}\nx & = \\frac{ \\twodet{1}{3}{7}{1} }{ \\det(\\bfA) }=  \\frac{ \\twodet{1}{3}{7}{1} }{ \\twodet{-1}{3}{3}{1} } = \\frac{-20}{-10} = 2, \\\\\ny & = \\frac{ \\twodet{-1}{1}{3}{7} }{ \\det(\\bfA) } = \\frac{ \\twodet{-1}{1}{3}{7} }{ \\twodet{-1}{3}{3}{1} } = \\frac{-10}{-10} = 1.\\\\\n\\end{split}\\]\n."
  },
  {
    "objectID": "matrix_algebra.html#eigenvalues",
    "href": "matrix_algebra.html#eigenvalues",
    "title": "4  Matrix algebra",
    "section": "4.9 Eigenvalues",
    "text": "4.9 Eigenvalues\n\n\n\n\n\n\nImportant\n\n\n\nFor the rest of the chapter, we deal with square matrices only.\n\n\nThe importance and usefulness of the following definition won’t be apparent for a while.\n\nDefinition 4.15 (Eigenvalue and eigenvector) Suppose \\(\\bfA\\in\\cmn{n}{n}\\). If there exist a number \\(\\lambda\\) and a nonzero vector \\(\\bfv\\) such that\n\\[\n\\bfA \\bfv = \\lambda \\bfv,\n\\]\nthen \\(\\lambda\\) is an eigenvalue of \\(\\bfA\\) with associated eigenvector \\(\\bfv\\).\n\nIf you think of \\(\\bfA\\) as acting on vectors, then an eigenvector is a direction in which the action of \\(\\bfA\\) is one-dimensional.\nFor example, let \\(\\bfA = -\\dfrac{1}{6}\\twomat{1}{5}{10}{-4}\\). For any value of \\(\\theta\\), \\(\\bfx = [\\cos\\theta,\\sin\\theta]\\) is a vector in \\(\\real^2\\) in the direction of \\(\\theta\\). If we choose the direction randomly, then there is no special relationship between \\(\\bfx\\) (blue) and \\(\\bfA\\bfx\\) (red). But in two special directions, the result \\(\\bfA\\bfx\\) is parallel to \\(\\bfx\\). For these directions, \\(\\bfx\\) is an eigenvector, and the corresponding eigenvalue is either \\(1.5\\) or \\(-1\\).\n\n\nCode\nV = [-2 1;4 1]\nD = [1.5 0;0 -1]\nA = V*D/V\n\nplot(size=(600,200),layout=(1,3),frame=:zerolines,aspect_ratio=1,xticks=[],yticks=[],xlims=[-1,1],ylims=1.4*[-1,1])\nt = (0:360)*2pi/360\n\nplot!(cos.(t),sin.(t),color=RGBA(0,0,0,.5),l=1,subplot=1)\nx = normalize([-1,-0.4])\ny = A*x\nplot!([0,x[1]],[0,x[2]],color=:darkblue,l=3,arrow=true,subplot=1)\nannotate!(x[1]/2+.1,x[2]/2-0.2,L\"\\mathbf{x}\",color=:darkblue,subplot=1)\nplot!([0,y[1]],[0,y[2]],color=:red,l=3,arrow=true,subplot=1)\nannotate!(y[1]/2+.33,y[2]/2-0.1,L\"\\mathbf{Ax}\",color=:red,subplot=1)\n\nplot!(cos.(t),sin.(t),color=RGBA(0,0,0,.5),l=1,subplot=2)\nx = normalize([-2,4])\ny = A*x\nplot!([0,y[1]],[0,y[2]],color=:red,l=3,arrow=true,subplot=2)\nplot!([0,x[1]],[0,x[2]],color=:darkblue,l=3,arrow=true,subplot=2)\n\nplot!(cos.(t),sin.(t),color=RGBA(0,0,0,.5),l=1,subplot=3)\nx = normalize([1,1])\ny = A*x\nplot!([0,y[1]],[0,y[2]],color=:red,l=3,arrow=true,subplot=3)\nplot!([0,x[1]],[0,x[2]],color=:darkblue,l=3,arrow=true,subplot=3)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.9.1 Eigenspaces\nAn eigenvalue is a clean, well-defined target. Eigenvectors are a little slipperier. For starters, if \\(\\bfA\\bfv=\\lambda\\bfv\\), then\n\\[\n\\bfA(c\\bfv) = c(\\bfA\\bfv)=c(\\lambda\\bfv)=\\lambda(c\\bfv).\n\\]\nHence:\n\nTheorem 4.18 Every nonzero multiple of an eigenvector is also an eigenvector at the same eigenvalue.\n\nBut a simple example shows that there can be even more ambiguity.\n\nExample 4.13 Let \\(\\meye\\) be an identity matrix. Then \\(\\meye\\bfx=\\bfx\\) for any vector \\(\\bfx\\), so every nonzero vector is an eigenvector!\n\nFortunately we already have the tools we need to describe a more robust target, based on the very simple reformulation\n\\[\n\\bfzero=\\bfA\\bfv-\\lambda\\bfv=(\\bfA-\\lambda\\meye)\\bfv.\n\\]\nThe requirement of an eigenvector to be nonzero, combined with Theorem 4.6, leads to the following crucial conclusion.\n\nTheorem 4.19 \\(\\lambda\\) is an eigenvalue of \\(\\bfA\\) if and only if \\(\\bfA-\\lambda\\meye\\) is singular.\n\n\nDefinition 4.16 (Eigenspace) Let \\(\\lambda\\) be an eigenvalue of \\(\\bfA\\). The eigenspace associated with \\(\\lambda\\) is the null space of \\((\\bfA-\\lambda\\meye)\\bfx\\).\n\nEigenspaces, unlike eigenvectors, are uniquely associated with their eigenvalues. It’s common, though, to use eigenvectors anyway and silently ignore the nonuniqueness.\n\n\n4.9.2 Properties\n\nTheorem 4.20 Suppose \\(\\bfA\\) is a square matrix.\n\nIf \\(\\lambda\\) is an eigenvalue of \\(\\bfA\\), then \\(c\\lambda\\) is an eigenvalue of \\(c\\bfA\\) with the same eigenspace.\nIf \\(\\lambda\\) is an eigenvalue of \\(\\bfA\\), then \\(\\lambda-c\\) is an eigenvalue of \\(\\bfA-c\\meye\\) with the same eigenspace.\nIf \\(\\bfA\\) is a triangular square matrix, then its eigenvalues are its diagonal elements.\nA matrix is singular if and only if \\(0\\) is among its eigenvalues.\n\n\n\nExample 4.14 Find the eigenvalues and eigenspaces of \\(\\bfA = \\begin{bmatrix} -2 & 4 & 0 \\\\ 0 & 1 & -3 \\\\ 0 & 0 & -2 \\end{bmatrix}.\\)\n\nSolution. Because the matrix is upper triangular, we see right away that its eigenvalues are \\(\\lambda_1=-2\\) and \\(\\lambda_2=1\\). The eigenspace for \\(\\lambda_1\\) is the null space of\n\\[\n\\bfA - (-2)\\meye = \\begin{bmatrix} 0 & 4 & 0 \\\\ 0 & 3 & -3 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\quad \\overset{\\text{RREF}}{\\Longrightarrow} \\quad\n\\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}.\n\\]\nSince the solution of a homogeneous system with this matrix is \\(x_1=t\\), \\(x_2=x_3=0\\), the basis for this eigenspace is thus \\([1,0,0]\\). The eigenspace for \\(\\lambda_2\\) is the null space of\n\\[\n\\bfA - (1)\\meye = \\begin{bmatrix} -3 & 4 & 0 \\\\ 0 & 0 & -3 \\\\ 0 & 0 & -3 \\end{bmatrix}\n\\quad \\overset{\\text{RREF}}{\\Longrightarrow} \\quad\n\\begin{bmatrix} 1 & -\\frac{4}{3} & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}.\n\\]\nA basis for this eigenspace is \\([\\frac{4}{3},1,0]\\), though a more convenient choice is \\([4,3,0]\\).\n\n\n\n\n4.9.3 Fundamental Theorem redux\nWe can extend Theorem 4.6 to include statements about determinants and eigenvalues.\n\nTheorem 4.21 (FTLA, Extended Edition) If \\(\\bfA\\) is an \\(n\\times n\\) matrix, then each of these statements is equivalent to all of the others.\n\n\\(\\bfA\\) is invertible.\nThe linear system \\(\\bfA\\bfx=\\bfb\\) has the unique solution \\(\\bfx=\\bfA^{-1}\\bfb\\) for each \\(\\bfb\\).\nThe null space of \\(\\bfA\\) is just \\(\\{\\bfzero\\}\\).\nThe RRE form of \\(\\bfA\\) is the identity matrix.\n\\(\\rank(\\bfA)=n\\).\n\\(\\det(\\bfA)\\neq 0\\).\nNone of the eigenvalues of \\(\\bfA\\) is zero."
  },
  {
    "objectID": "matrix_algebra.html#computing-eigenvalues",
    "href": "matrix_algebra.html#computing-eigenvalues",
    "title": "4  Matrix algebra",
    "section": "4.10 Computing eigenvalues",
    "text": "4.10 Computing eigenvalues\nThe most common way to find eigenvalues by hand is to use the determinant to detect when \\(\\bfA-\\lambda\\meye\\) is singular, as required by Theorem 4.19. This determinant has a particular form and name.\n\nDefinition 4.17 (Characteristic polynomial of a matrix) Suppose \\(\\bfA\\) is an \\(n\\times n\\) matrix. The function \\(p(z) = \\det(\\bfA-z\\meye)\\) is a polynomial of degree \\(n\\) in \\(z\\) called the characteristic polynomial of \\(\\bfA\\).\n\n\nTheorem 4.22 (Computing eigenvalues and eigenspaces) Given an \\(n\\times n\\) matrix \\(\\bfA\\):\n\nFind the characteristic polynomial \\(p\\) of \\(\\bfA\\).\nLet \\(\\lambda_1,\\ldots,\\lambda_k\\) be the distinct roots of \\(p\\). These are the eigenvalues. (If \\(k&lt;n\\), it’s because one or more roots has multiplicity greater than 1.)\nFor each \\(\\lambda_j\\), find the general solution of \\((\\bfA-\\lambda_j\\meye)\\bfv=\\bfzero\\). This is the eigenspace associated with \\(\\lambda_j\\).\n\n\n\nExample 4.15 Find the eigenvalues and eigenspaces of\n\\[\n\\bfA = \\begin{bmatrix} 1 & 1 \\\\ 4 & 1 \\end{bmatrix}.\n\\]\n\nSolution. Start by computing the characteristic polynomial:\n\\[\n\\det \\left(\\twomat{1}{1}{4}{1} - \\twomat{\\lambda}{0}{0}{\\lambda} \\right) = \\twodet{1-\\lambda}{1}{4}{1-\\lambda} = (1-\\lambda)^2 - 4 = \\lambda^2-2\\lambda-3.\n\\]\nWe find eigenvalues by finding its roots, in this case \\(\\lambda_1=3\\) and \\(\\lambda_2=-1\\).\nFor \\(\\lambda_1=3\\),\n\\[\n\\bfA-3 \\meye = \\twomat{-2}{1}{4}{-2} \\quad \\overset{\\text{RREF}}{\\Longrightarrow} \\quad \\twomat{1}{-1/2}{0}{0}.\n\\]\nThe homogeneous solution can be expressed as \\(x_1=s/2\\), \\(x_2=s\\), or \\(\\bfx=s\\cdot[1/2;\\,1]\\). So \\([1/2;\\,1]\\) is a basis for this eigenspace. Since eigenvectors can be rescaled at will, we prefer to use \\(\\twovec{1}{2}\\) as the basis vector.\nFor \\(\\lambda_2=-1\\),\n\\[\n\\bfA+ \\meye = \\twomat{2}{1}{4}{2} \\quad \\overset{\\text{RREF}}{\\Longrightarrow} \\quad \\twomat{1}{1/2}{0}{0},\n\\]\nleading to the eigenspace basis \\([-1/2;\\,1]\\) or equivalently, \\(\\twovec{-1}{2}\\).\n\n\nBecause eigenvalues are the roots of the characteristic polynomials, real matrices can have complex eigenvalues occurring in conjugate pairs. We catch a little break from the following fact.\n\nTheorem 4.23 If \\(\\bfv\\) is an eigenvector for a complex eigenvalue \\(\\lambda\\) of a real matrix, then its conjugate \\(\\overline{\\bfv}\\) is an eigenvector for \\(\\overline{\\lambda}\\).\n\n\nExample 4.16 Find eigenvalues and eigenspaces of \\(\\bfA = \\begin{bmatrix} 0 & 0 & 6 \\\\ 0 & 0 & -3 \\\\ -3 & -3 & 0 \\end{bmatrix}.\\)\n\nSolution. \\[\n\\begin{split}\n\\det(\\bfA - z\\meye) & = \\begin{vmatrix}\n  -z & 0 & 6 \\\\ 0 & -z & -3 \\\\ -3 & -3 & -z\n\\end{vmatrix} \\\\\n& = -z(z^2-9) + 6(-3z) \\\\\n& = -z(z^2+18-9) = -z(z+3i)(z-3i),\n\\end{split}\n\\]\nwhich gives us \\(\\lambda_1=0\\), \\(\\lambda_2=3i\\), \\(\\lambda_3=-3i\\).\nThe eigenspace for \\(\\lambda_1\\) is the null space of\n\\[\n\\bfA - (0)\\meye = \\begin{bmatrix} 0 & 0 & 6 \\\\ 0 & 0 & -3 \\\\ -3 & -3 & 0 \\end{bmatrix}\n\\quad \\overset{\\text{RREF}}{\\Longrightarrow} \\quad\n\\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}.\n\\]\nThe only free column is the second, and a basis for this eigenspace is \\([-1,1,0]\\).\nThe eigenspace for \\(\\lambda_2\\) is the null space of\n\\[\n\\bfA - (3i)\\meye = \\begin{bmatrix} -3i & 0 & 6 \\\\ 0 & -3i & -3 \\\\ -3 & -3 & -3i \\end{bmatrix}\n\\quad \\overset{\\text{RREF}}{\\Longrightarrow} \\quad\n\\begin{bmatrix} 1 & 0 & 2i \\\\ 0 & 1 & -i \\\\ 0 & 0 & 0 \\end{bmatrix}.\n\\]\nThe third column is free, and a basis for this eigenspace is \\([-2i,i,1]\\).\nBecause \\(\\lambda_3\\) is the conjugate of \\(\\lambda_2\\), its eigenspace has basis \\([2i,-i,1]\\).\n\n\n\n4.10.1 Eigenvectors for \\(2\\times 2\\)\nFinding the exact roots of a cubic polynomial is rarely easy. Thus most of our hand computations will be for \\(2\\times 2\\) matrices. This allows some shortcuts.\nSuppose \\(\\lambda\\) is known to be an eigenvalue of \\(\\bfA\\). Then \\(\\bfA-\\lambda\\meye\\) must be singular, and its RRE form has at least one free column. In the \\(2\\times 2\\) case, row elimination must therefore zero out the second row entirely, which spares us from having to do the process at all.\nIn summary, we can deduce the following.\n\nTheorem 4.24 (Eigenvectors for \\(2\\times 2\\)) Given an eigenvalue \\(\\lambda\\) of \\(2\\times 2\\) matrix \\(\\bfA\\), let the first row of \\(\\bfA-\\lambda\\meye\\) be written as the vector \\([\\alpha,\\beta]\\).\n\nIf \\(\\alpha=\\beta=0\\), then \\(\\bfA-\\lambda\\meye\\) is a zero matrix and all of \\(\\complex^2\\) is the eigenspace of \\(\\lambda\\).\nOtherwise, the vector \\([\\beta;\\,-\\alpha]\\) is a basis of the eigenspace of \\(\\lambda\\).\n\n\n\nExample 4.17 Find the eigenstuff of\n\\[\n\\bfA = \\twomat{1}{1}{-1}{1}.\n\\]\n\nSolution. We start by finding eigenvalues.\n\\[\n\\det(\\bfA - \\lambda \\meye) = \\twodet{1-\\lambda}{1}{-1}{1-\\lambda} = (1-\\lambda)^2 + 1.\n\\]\nThe eigenvalues are therefore roots of \\(\\lambda^2 - 2\\lambda + 2\\), or\n\\[\n\\lambda = 1 \\pm \\sqrt{1-2} = 1 \\pm i.\n\\]\nThis is our first case of a real matrix that has complex eigenvalues. We continue as always, only using complex arithmetic.\nThe eigenspace for \\(\\lambda_1=1+i\\) is the homogeneous solution of\n\\[\n\\bfA - (1+i) \\meye = \\twomat{-i}{1}{-1}{-i}.\n\\]\nTo find a basis we just use the first row as explained above, getting \\(\\twovec{1}{i}\\).\nSince the matrix is real, a basis for the other eigenspace can be found by conjugating this one to get \\(\\twovec{1}{-i}\\)."
  },
  {
    "objectID": "matrix_algebra.html#diagonalization",
    "href": "matrix_algebra.html#diagonalization",
    "title": "4  Matrix algebra",
    "section": "4.11 Diagonalization",
    "text": "4.11 Diagonalization\nThe eigenvalues of a matrix are the roots of its characteristic polynomial \\(p\\). In the general \\(n\\times n\\) case, we can factor \\(p\\) as\n\\[\np(z) = (z-\\lambda_1)^{m_1}(z-\\lambda_2)^{m_2}\\cdots(z-\\lambda_k)^{m_k},\n\\tag{4.8}\\]\nfor positive integer exponents such that \\(m_1+\\cdots+m_k=n\\). The easiest situation is when all of the exponents are 1, and we say each eigenvalue is simple. Subtle things happen when an exponent is larger than 1.\n\n4.11.1 Multiplicity\nThe exponents in Equation 4.8 are one of two ways to define the the multiplicities of the eigenvalues.\n\nDefinition 4.18 (Algebraic multiplicity) The algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic polynomial.\n\nThe following example illustrates a peculiar possibility for eigenvalues of algebraic multiplicity greater than 1.\n\nExample 4.18 Find the eigenspaces of \\(\\bfA=\\twomat{4}{1}{0}{4}\\).\n\nSolution. The characteristic polynomial is\n\\[\n\\twodet{4-\\lambda}{1}{0}{4-\\lambda} = (4-\\lambda)^2,\n\\]\nso the double root \\(\\lambda_1=4\\) is the only eigenvalue. Since\n\\[\n\\bfA - 4\\meye = \\twomat{0}{1}{0}{0},\n\\]\nthe eigenspace has basis \\(\\twovec{1}{0}\\).\n\n\nThis leads us to define a second notion of multiplicity for an eigenvalue.\n\nDefinition 4.19 (Geometric multiplicity) The geometric multiplicity of an eigenvalue is the dimension of its associated eigenspace.\n\nHere is an important fact we won’t try to justify.\n\nTheorem 4.25 If \\(\\text{AM}\\) and \\(\\text{GM}\\) are the algebraic and geometric multiplicities respectively of an eigenvalue, then\n\\[\n1 \\le \\text{GM} \\le \\text{AM}.\n\\]\n\n\n\n4.11.2 Defectiveness\nIn the Example 4.18 we found a lone eigenvalue \\(\\lambda_1=4\\) of algebraic multiplicity 2 whose geometric multiplicity is 1. The identity matrix is a different sort of example.\n\nExample 4.19 The \\(2\\times 2\\) identity matrix \\(\\meye\\) has a lone eigenvalue \\(\\lambda_1=1\\) of algebraic multiplicity 2. The system \\((\\meye - \\meye)\\bfv=\\bfzero\\) has an RRE form that is the zero matrix, so there are two free variables and two basis vectors. Hence the geometric multiplicity of \\(\\lambda_1\\) is also 2.\n\nThe distinction between these cases is significant enough to warrant another definition.\n\nDefinition 4.20 (Defectiveness) An eigenvalue \\(\\lambda\\) whose geometric multiplicity is strictly less than its algebraic multiplicity is said to be defective. A matrix is called defective if any of its eigenvalues are defective.\n\nAs we will see later, defective matrices often complicate the application of eigenvalue analysis.\nSince multiplicities are always at least one, there is a simple and common case in which we are certain that a matrix is not defective.\n\nTheorem 4.26 (Distinct eigenvalues) If \\(\\bfA\\in\\cmn{n}{n}\\) has \\(n\\) distinct eigenvalues, then \\(\\bfA\\) is not defective.\n\nFor \\(n=2\\), the possibilities in the case of algebraic multiplicity equal to 2 are easy to pin down even further.\n\nTheorem 4.27 (\\(2\\times 2\\) defectiveness) Any \\(\\bfA\\in\\cmn{2}{2}\\) that has a single repeated eigenvalue is either defective or a multiple of the identity matrix.\n\n\n\n4.11.3 Diagonalization\nSuppose \\(\\bfA\\) is \\(n\\times n\\) and that it has eigenvalues \\(\\lambda_1,\\ldots,\\lambda_n\\) associated with linearly independent eigenvectors \\(\\bfv_1,\\ldots,\\bfv_n\\). Writing out the equations \\(\\bfA\\bfv_j = \\lambda_j \\bfv_j\\) in columns, we find\n\\[\n\\begin{split}\n\\begin{bmatrix}\n  \\bfA \\bfv_1 & \\bfA \\bfv_2 & \\cdots & \\bfA \\bfv_n\n\\end{bmatrix}\n&= \\begin{bmatrix}\n  \\lambda_1 \\bfv_1 & \\lambda_2 \\bfv_2 & \\cdots & \\lambda_n \\bfv_n\n\\end{bmatrix} \\\\\n\\bfA \\begin{bmatrix}\n  \\bfv_1 & \\bfv_2 & \\cdots & \\bfv_n\n\\end{bmatrix}\n&= \\begin{bmatrix}\n  \\bfv_1 &  \\bfv_2 & \\cdots & \\bfv_n\n\\end{bmatrix} \\begin{bmatrix}\n  \\lambda_1 & & & \\\\ & \\lambda_2  & & \\\\ & & \\ddots & \\\\ & & & \\lambda_n\n\\end{bmatrix} \\\\\n\\bfA \\bfV &= \\bfV \\mathbf{D},\n\\end{split}\n\\]\nwhere we defined\n\\[\n\\bfV = \\begin{bmatrix} \\bfv_1 &  \\bfv_2 & \\cdots & \\bfv_n \\end{bmatrix}, \\qquad\n\\mathbf{D} = \\begin{bmatrix}\n  \\lambda_1 & & & \\\\ & \\lambda_2  & & \\\\ & & \\ddots & \\\\ & & & \\lambda_n\n\\end{bmatrix}.\n\\tag{4.9}\\]\nSince we assumed that the columns of \\(\\bfV\\) are linearly independent vectors, the column space of \\(\\bfV\\) is \\(n\\)-dimensional. Hence \\(\\rank(\\bfV)=n\\) and \\(\\bfV\\) is invertible.\n\nDefinition 4.21 (Diagonalization) A diagonalization of square matrix \\(\\bfA\\) is the factorization\n\\[\n\\bfA = \\bfV \\mathbf{D} \\bfV^{-1},\n\\tag{4.10}\\]\nwhere (as defined in Equation 4.9) \\(\\mathbf{D}\\) is a diagonal matrix of eigenvalues and \\(\\bfV\\) is a square matrix whose columns are corresponding eigenvectors.\n\nWhat can we do about the requirement of linearly independent eigenvectors in the columns of \\(\\bfV\\)? Without wading into the details, the following wraps this assumption up nicely.\n\nTheorem 4.28 A matrix has a diagonalization if and only if it is not defective."
  },
  {
    "objectID": "ode_systems.html#introduction-to-systems",
    "href": "ode_systems.html#introduction-to-systems",
    "title": "5  Systems of ODEs",
    "section": "5.1 Introduction to systems",
    "text": "5.1 Introduction to systems\nA vector-valued ODE is an equation in the form\n\\[\n\\frac{d \\bfx}{dt} = \\bff(t,\\bfx),\n\\tag{5.1}\\]\nwhere \\(\\bfx\\) is an \\(n\\)-dimensional vector. An equivalent point of view and terminology is a coupled system of scalar ODEs,\n\\[\n\\begin{split}\n\\frac{dx_1}{dt} &= f_1(t,x_1,\\ldots,x_n),\\\\\n& \\vdots \\\\\n\\frac{dx_n}{dt} &= f_n(t,x_1,\\ldots,x_n).\n\\end{split}\n\\tag{5.2}\\]\n\nExample 5.1 A famous system of three ODEs is the Lorenz system,\n\\[\n\\begin{split}\n\\dot{x} & = \\sigma(y-x), \\\\\n\\dot{y} & = \\rho x - y - x z, \\\\\n\\dot{z} & = -\\beta z + x y,\n\\end{split}\n\\]\nwhere the dots indicate time derivatives and \\(\\sigma\\), \\(\\rho\\), and \\(\\beta\\) are constant parameters. Conversion to the system notation follows from the definitions \\(x_1=x\\), \\(x_2=y\\), and \\(x_3=z\\), though of course the ordering is arbitrary.\n\nTo define an initial-value problem, we supplement Equation 5.1 with the initial condition\n\\[\n\\bfx(0) = \\bfx_0.\n\\]\nWe will shortly be limiting our attention to systems in which the dependence in Equation 5.2 of each \\(f_i\\) on each \\(x_j\\) is linear. First, though, we mention an important connection between first-order systems and higher-order problems. ### Conversion of higher-order ODEs\nThe restriction to first-order ODEs in Equation 5.1 is actually no limitation at all. Any higher-order equation, or system of higher-order equations, can be converted to a first-order system by increasing the dimension.\n\n\n\n\n\n\nNote\n\n\n\nEach order beyond the first of a derivative can be exchanged for a dimension.\n\n\n\nExample 5.2 The nonlinear pendulum equation\n\\[\n\\theta'' + \\frac{g}{L} \\sin(\\theta) = f(t)\n\\]\ncan be transformed by the change of variables \\(x_1=\\theta\\), \\(x_2=\\theta'\\) into\n\\[\n\\begin{split}\nx_1' &= x_2 \\\\\nx_2' &= f(t) - \\frac{g}{L}\\sin(x_1),    \n\\end{split}\n\\]\nwhich is of the form Equation 5.1.\n\nThe transformation in Example 5.2 has the physical interpretation of using position and angular momentum as the dependent unknowns; together these are the state of the system. The technique is straightforward to generalize to any problem or system having ODEs of order greater than one, and most software for solving ODEs numerically requires you to make that transformation yourself. We will encounter just one more significant example, though.\n\nExample 5.3 The simple harmonic oscillator\n\\[\nx'' + 2 \\zeta \\omega_0 x' + \\omega_0^2 x = f(t)\n\\]\ncan be converted to an equivalent first-order system using the definitions\n\\[\nu_1 = x, \\quad u_2 = x'.\n\\]\nWe can easily derive an ODE for \\(\\bfu\\) without reference to \\(x\\). First, by definition, \\(u_1'=u_2\\). Next, \\(u_2' = x''\\), and we can isolate \\(x''\\) in the original equation to get\n\\[\nu_2' = f - 2 \\zeta \\omega_0 x' - \\omega_0^2 x = f - 2 \\omega_0\\zeta  u_2 - \\omega_0^2 u_1.\n\\]\nHence\n\\[\n\\begin{split}\nu_1' &= u_2,\\\\\nu_2' &= f - 2 \\omega_0 \\zeta  u_2 - \\omega_0^2 u_1.\n\\end{split}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nWhile every high-order problem can be converted to a first-order system, the converse is not true. That is, there are first-order systems that are not equivalent to any higher-order problem."
  },
  {
    "objectID": "ode_systems.html#linear-ode-systems",
    "href": "ode_systems.html#linear-ode-systems",
    "title": "5  Systems of ODEs",
    "section": "5.2 Linear ODE systems",
    "text": "5.2 Linear ODE systems\nRather than the fully general system Equation 5.1, we will focus on linear problems.\n\nDefinition 5.1 (Linear system of ODEs) A linear ODE system is an equation of the form\n\\[\n\\mathbf{x}' = \\mathbf{A}(t)\\mathbf{x} + \\bff(t),\n\\tag{5.3}\\]\nwhere \\(\\mathbf{x}\\) is an \\(n\\)-dimensional vector, \\(\\mathbf{A}\\) is an \\(n\\times n\\) coefficient matrix, and \\(\\bff(t)\\) is an \\(n\\)-dimensional forcing function. If the coefficient matrix does not depend on time, the system is said to be constant-coefficient.\nIf given, an initial condition of the system is a time \\(t_0\\) and vector \\(\\mathbf{x}_0\\) such that \\(\\mathbf{x}(t_0)=\\mathbf{x}_0\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe matrix in Equation 5.3 must be square.\n\n\n\nExample 5.4 The simple harmonic oscillator system in (Example?) {number}  is equivalent to\n\\[\n\\bfu' = \\twomat{0}{1}{-\\omega_0^2}{-2 \\omega_0 \\zeta} \\bfu + \\twovec{0}{f},\n\\]\nwhere \\(u_1 = x\\), \\(u_2 = x'\\) is the state vector.\n\n\nExample 5.5 Suppose two connected tanks hold brine. Tank 1 holds 100 L, has an input of 4 L/hr of brine of concentration 3 kg/L, and an output of 6 L/hr. Tank 2 holds 200 L, has an input of 7 L/hr of brine of concentration 5 kg/L, and an output of 5 L/hr. Tank 1 pumps 1 L/hr into tank 2, and tank 2 pumps 3 L/hr into tank 1.\nLet \\(x_i(t)\\) be the mass of salt in tank \\(i\\). The statements above imply\n\\[\n\\begin{split}\n\\dd{x_1}{t} & = 4\\cdot 3 - 6\\cdot \\frac{x_1}{100} - 1\\cdot \\frac{x_1}{100} + 3 \\cdot \\frac{x_2}{200}  \\\\\n\\dd{x_2}{t} & = 7\\cdot 5 - 5\\cdot \\frac{x_2}{200} - 3\\cdot \\frac{x_2}{200} +  1\\cdot \\frac{x_1}{100}\\\\\n\\end{split}.\n\\]\nThis is neatly expressed using linear algebra:\n\\[\n\\dd{}{t} \\twovec{x_1}{x_2} = \\frac{1}{200}\\twomat{-14}{3}{2}{-8} \\twovec{x_1}{x_2} + \\twovec{12}{35}.\n\\]\n\n\nExample 5.6 Here is a basic model for heating in a house. Let \\(b(t)\\) be the temperature of the basement, \\(m(t)\\) be the temperature of the main living area, and \\(a(t)\\) be the temperature of the attic. Suppose the ground is at a constant 10 degrees C. We use a Newtonian model to describe how the temperature of the basement evolves due to interactions with the earth and the main floor:\n\\[\n\\frac{db}{dt} = -k_b (b - 10) - k_{mb} (b-m).\n\\]\nSimilarly, the attic interacts with the air, which we will hold at 2 degrees, and the main floor:\n\\[\n\\frac{da}{dt} = -k_a (a - 2) - k_{ma} (a-m).\n\\]\nFinally, suppose the main area interacts mostly with the other levels and experiences input from a heater:\n\\[\n\\frac{dm}{dt} = -k_{mb} (m - b) - k_{ma} (m - a) + h(t).\n\\]\nTo write the system in vector form, we define \\(x_1=b\\), \\(x_2=m\\), and \\(x_3=a\\). Then\n\\[\n\\frac{d\\bfx}{dt} = \\threevec{(-k_b - k_{mb}) x_1  +k_{mb} x_2}{ k_{mb}x_1 -(k_{mb}+k_{ma})x_2 + k_{ma}x_3}{k_{ma}x_2  -(k_{ma}+k_a)x_3} + \\threevec{10k_b}{h(t)}{2k_a}.\n\\]\nObserve that on the right side, the terms depending on \\(\\bfx\\) have been separated from the others. Hence\n\\[\n\\bfx' = \\begin{bmatrix} -k_b-k_{mb} & k_{mb} & 0 \\\\ k_{mb} & -k_{mb}-k_{ma} & k_{ma} \\\\  0 & k_{ma} & -k_{ma}-k_a \\end{bmatrix}  \\bfx + \\threevec{10k_b}{h(t)}{2k_a}.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nYou need to read and write carefully when in vector-land. In particular, note that \\(\\mathbf{x}_1\\) refers to the first vector of a collection, while \\(x_1\\) means the first component of a vector \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "ode_systems.html#general-solutions",
    "href": "ode_systems.html#general-solutions",
    "title": "5  Systems of ODEs",
    "section": "5.3 General solutions",
    "text": "5.3 General solutions\nVirtually all of the theoretical statements we made about the scalar linear problem \\(x'=a(t)x+f(t)\\) can be remade with boldface/capital letters for the linear system \\(\\mathbf{x}'=\\mathbf{A}(t)\\mathbf{x}+\\bff(t)\\). Those statements relied mainly on linearity. Most notably:\n\nTheorem 5.1 (General solution of ODE linear system) Every solution of\n\\[\n\\mathbf{x}'=\\mathbf{A}(t)\\mathbf{x}+\\bff(t)\n\\]\ncan be written in the form\n\\[\n\\mathbf{x}=\\mathbf{x}_h+\\mathbf{x}_p,\n\\]\nwhere \\(\\mathbf{x}_h\\) is the general solution of \\(\\mathbf{x}'=\\mathbf{A}(t)\\mathbf{x}\\) and \\(\\mathbf{x}_p\\) is any solution of \\(\\mathbf{x}'=\\mathbf{A}(t)\\mathbf{x}+\\bff(t)\\).\n\nOnce again, then, we look first at the homogeneous system with no forcing term."
  },
  {
    "objectID": "ode_systems.html#homogeneous-equations",
    "href": "ode_systems.html#homogeneous-equations",
    "title": "5  Systems of ODEs",
    "section": "5.4 Homogeneous equations",
    "text": "5.4 Homogeneous equations\nGiven \\[\n\\mathbf{x}' = \\mathbf{A}(t)\\mathbf{x},\n\\tag{5.4}\\]\nwhere \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\), we can easily show in the usual way that any linear combination of solutions is also a solution. In fact, we have a new way of stating this result.\n\nDefinition 5.2 (Solution space) The set of all solutions of a homogeneous equation \\(\\mathbf{x}' = \\mathbf{A}(t)\\mathbf{x}\\) is a subspace of differentiable functions called the solution space.\n\nOur first major goal is to find a basis for the solution space.\n\n5.4.1 Fundamental matrix\nSuppose \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_m\\) are homogeneous solutions of Equation 5.4, and that we use a linear combination of them to satisfy an initial condition \\(\\mathbf{x}(0)=\\mathbf{x}_0\\):\n\\[\n    c_1 \\mathbf{x}_1(0) + \\cdots + c_m \\mathbf{x}_m(0) = \\mathbf{x}_0.\n\\]\nUsing the equivalence of linear combination with matrix-vector multiplication, we define the \\(n\\times m\\) matrix\n\\[\n\\fundm(t) = \\bigl[ \\mathbf{x}_1(t) \\; \\mathbf{x}_2(t) \\; \\cdots \\; \\mathbf{x}_m(t)  \\bigr],\n\\tag{5.5}\\]\nso that\n\\[\n\\fundm(0) \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{bmatrix} = \\mathbf{x}_0.\n\\tag{5.6}\\]\nThis is a linear algebraic system for the coefficients \\(c_i\\). We can expect a unique solution if and only if \\(m=n\\) and \\(\\bfX(0)\\) is invertible.\n\nDefinition 5.3 (Fundamental matrix) The \\(n\\times n\\) matrix \\(\\fundm(t)\\) is a fundamental matrix of the homogeneous system \\(\\mathbf{x}' = \\mathbf{A}(t)\\mathbf{x}\\) if its columns satisfy\n\\[\n\\mathbf{x}_j'=\\mathbf{A}\\mathbf{x}_j, \\quad j=1,\\ldots,n,\n\\tag{5.7}\\]\nand \\(\\fundm(t)\\) is invertible at all times in an open interval \\(I\\) where \\(\\bfA\\) is continuous. Equivalently,\n\\[\n\\fundm'=\\bfA\\fundm.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nBecause Definition 5.3 says nothing about an initial condition like in Equation 5.6, a particular homogeneous ODE has infinitely many fundamental matrices.\n\n\nA fundamental matrix gives all we need to construct the general homogeneous solution.\n\nTheorem 5.2 If \\(\\fundm\\) is a fundamental matrix for \\(\\bfx'=\\bfA(t)\\bfx\\), then the general solution of this system is\n\\[\n\\bfx_h = \\mathbf{x}(t)=\\fundm(t)\\mathbf{c},\n\\]\nfor an arbitrary constant vector \\(\\mathbf{c}\\).\n\n\n\n5.4.2 Wronskian\nSince invertibility is required for a fundamental matrix, we should not be surprised to see its determinant popping up.\n\nDefinition 5.4 (Wronskian) Let \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) be a collection of vector-valued functions where each \\(\\bfx_i\\) is \\(n\\)-dimensional. Their Wronskian is\n\\[\nW(t) = \\det\\Bigl( \\bigl[ \\mathbf{x}_1(t) \\: \\cdots \\: \\mathbf{x}_n(t)  \\bigr] \\Bigr).\n\\]\n\n\nTheorem 5.3 Suppose that \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) are solutions of Equation 5.4, and \\(I\\) is an interval where \\(\\bfA(t)\\) is continuous. The following are equivalent statements:\n\n\\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) are linearly independent in \\(I\\).\n\\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) are columns of a fundamental matrix.\nThe Wronksian of \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) is nonzero at all \\(t\\in I\\).\n\n\n\nExample 5.7 Suppose we have two solutions\n\\[\n\\mathbf{x}_1(t)=e^{\\lambda_1 t}\\mathbf{v}_1, \\quad \\mathbf{x}_2(t)=e^{\\lambda_2 t}\\mathbf{v}_2,\n\\]\nwhere \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are columns of an invertible \\(2\\times 2\\) constant matrix. Show that the Wronskian of these solutions is nonzero at all times.\n\nSolution. Say \\(\\mathbf{v}_1=[a,\\,c]\\) and \\(\\mathbf{v}_2=[b,\\,d]\\). Then\n\\[\n\\begin{split}\nW(t) & = \\det\\Bigl( \\bigl[ \\mathbf{x}_1(t) \\: \\mathbf{x}_2(t)  \\bigr] \\Bigr)\\\\\n& = \\twodet{a e^{\\lambda_1 t}}{be^{\\lambda_2 t}}{c e^{\\lambda_1 t}}{d e^{\\lambda_2 t}}\\\\\n& = e^{t(\\lambda_1+\\lambda_2)} (ad-bc) =  e^{t(\\lambda_1+\\lambda_2)} \\det\\Bigl(\\bigl[\\mathbf{v}_1\\: \\mathbf{v}_2\\bigr]\\Bigr) \\\\\n\\end{split}\n\\]\nThe exponential function is never zero, and the problem statement guarantees that the last determinant is nonzero. Hence \\(W\\) is nonzero as well."
  },
  {
    "objectID": "ode_systems.html#constant-coefficient-problems",
    "href": "ode_systems.html#constant-coefficient-problems",
    "title": "5  Systems of ODEs",
    "section": "5.5 Constant-coefficient problems",
    "text": "5.5 Constant-coefficient problems\nWhile the theory of homogeneous linear ODE systems is fairly straightforward, the good news for the general case pretty much ends there. In most problems we cannot get a useful expression for a fundamental matrix. But there is a very important special case in which we can say a lot: when the coefficient matrix is constant. Accordingly, we now specialize to\n\\[\n\\bfx' = \\bfA \\bfx,\n\\tag{5.8}\\]\nwhere \\(\\bfA\\) is an \\(n\\times n\\) constant matrix. We are finally ready to see the significance of the eigenvalue condition\n\\[\n\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}.\n\\]\nIf we seek a solution in the form \\(\\mathbf{x}(t)=g(t)\\mathbf{v}\\) for this eigenvector, then\n\\[\ng'\\mathbf{v} = \\mathbf{A}(g\\mathbf{v}) = g\\cdot (\\mathbf{A}\\mathbf{v}) = (g\\lambda) \\mathbf{v},\n\\]\nwhich can be satisfied if \\(g'=\\lambda g\\). That is, we have a solution\n\\[\n\\bfx(t) = e^{\\lambda t} \\bfv.\n\\]\n\n5.5.1 Homogeneous solution\nCounting algebraic multiplicities, we know that \\(\\mathbf{A}\\) has eigenvalues \\(\\lambda_1,\\ldots,\\lambda_n\\). Say that we have eigenvectors \\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\) to go with them. Then we have \\(n\\) homogeneous solutions\n\\[\n\\mathbf{x}_j(t) = e^{t \\lambda_j} \\mathbf{v}_j, \\quad j=1,\\ldots,n.\n\\]\nOur next move is to determine whether\n\\[\n\\bigl[ \\mathbf{x}_1(t) \\; \\mathbf{x}_2(t) \\; \\cdots \\; \\mathbf{x}_n(t)  \\bigr]\n\\]\nis a fundamental matrix. According to Theorem 5.3, we can ask that question at any value of \\(t\\), including \\(t=0\\). So the key issue is whether\n\\[\n\\bfV = \\bigl[ \\mathbf{v}_1 \\; \\mathbf{v}_2 \\; \\cdots \\; \\mathbf{v}_n  \\bigr]\n\\tag{5.9}\\]\nis invertible. If so, then we have the ingredients of the general homogeneous solution.\n\nTheorem 5.4 (Solution by eigenvectors) Let \\(\\mathbf{A}\\) have eigenvalues \\(\\lambda_1,\\ldots,\\lambda_n\\) and corresponding eigenvectors \\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\). If \\(\\bfA\\) is diagonalizable, then\n\\[\n\\fundm(t) = \\begin{bmatrix}\ne^{t \\lambda_1} \\mathbf{v}_1 & \\cdots & e^{t \\lambda_n} \\mathbf{v}_n\n\\end{bmatrix}\n\\tag{5.10}\\]\nis a fundamental matrix for \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\), and the general solution can be expressed as\n\\[\n\\mathbf{x}(t) = \\fundm(t)\\mathbf{c} = c_1 e^{t \\lambda_1} \\mathbf{v}_1 + \\cdots + c_n e^{t \\lambda_n} \\mathbf{v}_n.\n\\tag{5.11}\\]\nIn particular, Equation 5.10 and Equation 5.11 hold if the eigenvalues of \\(\\bfA\\) are distinct (i. e., all of them are simple).\n\n\n\n\n\n\n\nNote\n\n\n\nTheorem 5.4 requires that \\(\\bfA\\) be diagonalizable. Solutions for defective matrices will have to wait for now.\n\n\n\nExample 5.8 Given that\n\\[\n\\mathbf{A} = \\begin{bmatrix} 1 & 1 \\\\ 4 & 1 \\end{bmatrix}\n\\]\nhas eigenpairs\n\\[\n\\lambda_1=3,\\: \\mathbf{v}_1 = \\twovec{1}{2}, \\quad \\lambda_2=-1,\\: \\mathbf{v}_2=\\twovec{1}{-2},\n\\]\nfind the general solution of \\(\\bfx'=\\bfA\\bfx\\).\n\nSolution. With the eigenvalues and eigenvectors provided, the general solution is formulaic:\n\\[\n\\mathbf{x}(t) =  c_1 e^{3t} \\twovec{1}{2} + c_2 e^{-t} \\twovec{1}{-2}\n= \\twovec{ c_1 e^{3t} + c_2 e^{-t}}{2c_1 e^{3t} - 2 c_2e^{-t}}.\n\\]\n(For the record, the determinant of\n\\[\n\\bfV = \\twomat{1}{1}{2}{-2}\n\\]\nis \\(-4\\), so this matrix is not singular, as guaranteed by Theorem 5.4.)\n\n\n\n\n5.5.2 Complex eigenvalues\nTheorem 5.4 works fine if the eigenvalues and eigenvectors are complex. However, a purely real-number alternative form is also available.\nSuppose that\n\\[\n\\lambda = r + i\\omega, \\qquad \\bfv = \\bfu_1 + i\\, \\bfu_2.\n\\]\nThen\n\\[\ne^{\\lambda t} \\bfv = e^{rt} (\\cos(\\omega t) + i\\sin(\\omega t) )\\, (\\bfu_1 + i\\, \\bfu_2).\n\\]\nThe real and imaginary parts of the above product form the basis of the real solution. Specifically,\n\\[\n\\bfx(t) = a_1 e^{rt} \\Bigl( \\cos(\\omega t)\\,\\bfu_1 - \\sin(\\omega t) \\,\\bfu_2(t) \\Bigr) + a_2 e^{rt} \\Bigl( \\cos(\\omega t)\\,\\bfu_2 + \\sin(\\omega t) \\,\\bfu_1(t) \\Bigr),\n\\] {#sys-complex-real-form}\nfor arbitrary real \\(a_1\\) and \\(a_2\\).\n\nExample 5.9 Find the general solution of\n\\[\n\\bfx' = \\twomat{1}{1}{-1}{1} \\bfx.\n\\]\n\nSolution. In the first chapter we found the eigenvalues \\(1\\pm i\\) and corresponding eigenvectors \\(\\twovec{1}{\\pm i}\\). This leads to the solution\n\\[\n\\bfx(t) = c_1 e^{(1+i)t} \\twovec{1}{i} + c_2 e^{(1-i)t} \\twovec{1}{-i}.\n\\]\nIf we expect a real solution (because of real initial values, say), then \\(c_2 = \\overline{c_1}\\) and the result is real.\nInstead, we use Euler’s formula to write out\n\\[\ne^{(1+i)t} \\twovec{1}{i} = e^t \\twovec{\\cos t + i \\sin t}{i\\cos t - \\sin t}\n= e^t \\twovec{\\cos t}{-\\sin t} + i e^t \\twovec{\\sin t}{\\cos t}.\n\\]\nThe real and imaginary parts of the last line are independent real solutions, and\n\\[\n\\bfx(t) = a_1 e^t \\twovec{\\cos t}{-\\sin t} + a_2 e^t \\twovec{\\sin t}{\\cos t}\n\\]\nfor arbitrary real \\(a_1\\) and \\(a_2\\).\n\n\n\n\n5.5.3 The oscillator reloaded\nEarlier we showed that the harmonic oscillator\n\\[\nu'' + 2 Z \\omega_0 u' + \\omega_0^2 u = 0\n\\]\ncan be converted to the constant-coefficient system\n\\[\n\\bfx' = \\twomat{0}{1}{-\\omega_0^2}{-2Z\\omega_0} \\bfx.\n\\]\nvia \\(x_1=u\\), \\(x_2=u'\\). The eigenvalues of the coefficient matrix are the roots of the characteristic polynomial\n\\[\n\\twodet{-z}{1}{-\\omega_0^2}{-2 \\zeta \\omega_0-z} = z^2 + 2 \\zeta \\omega_0 z + \\omega_0^2.\n\\]\nThis is identical to what we called the characteristic polynomial of the oscillator ODE! That is, the characteristic values we used there are the eigenvalues here.\nIn the underdamped case \\(\\zeta &lt; 1\\), for instance, the eigenvalues are complex: \\(\\lambda = -\\zeta \\omega_0 \\pm i \\omega_d\\), where \\(\\omega_d=\\omega_0 \\sqrt{1-\\zeta^2}\\). The first eigenvalue \\(\\lambda_1\\) has eigenvector\n\\[\n\\twovec{1}{\\lambda_1}.\n\\]\nIf we want the general solution of just the original \\(u\\) variable, then we only need the first component of the eigenvector, and the general solution combines the real and imaginary parts of \\(e^{\\lambda_1 t}\\):\n\\[\nu(t) = x_1(t) = a_1 e^{-\\zeta \\omega_0t} \\cos(\\omega_d t) + a_2 e^{-\\zeta \\omega_0t} \\sin(\\omega_d t).\n\\]\nAgain, the second-order problem is fully covered by the equivalent first-order system."
  },
  {
    "objectID": "ode_systems.html#stability-and-the-phase-plane",
    "href": "ode_systems.html#stability-and-the-phase-plane",
    "title": "5  Systems of ODEs",
    "section": "5.6 Stability and the phase plane",
    "text": "5.6 Stability and the phase plane\n\nDefinition 5.5 Any constant solution of the homogeneous system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\) is an equilibrium solution or steady-state solution.\n\nClearly, \\(\\mathbf{x}(t)\\equiv \\bfzero\\) is always an equilibrium solution.\n\n\n\n\n\n\nNote\n\n\n\nIf \\(\\mathbf{A}\\) is singular, then there are nonzero equilibrium solutions of the homogeneous system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\) as well, but we will not pursue those.\n\n\n\n5.6.1 Stability\nJust as with scalar first-order ODEs, stability is a crucial property of an equilibrium solution. Suppose we start \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\) at infinitesimally small initial conditions. If the state always approaches zero as \\(t\\to\\infty\\), we say that the equilibrium at zero is asymptotically stable. This is what happens, for instance, to a damped linear oscillator. If, on the other hand, the system has infinitesimal initial states that grow without bound, then the equilibrium is unstable.\nThe stability properties depend on the eigenvalues of \\(\\mathbf{A}\\). This is most easily summarized in the special case of a \\(2\\times 2\\) real matrix with eigenvalues \\(\\lambda_1,\\lambda_2\\). The major cases to be considered, leaving out some pesky edge cases, are summarized in Table 5.1. (The third column of that table is to be explained next.)\n\n\nTable 5.1: Eigenvalues and stability\n\n\nEigenvalues\nStability\nType\n\n\n\n\n\\(\\lambda_1 &lt; \\lambda_2 &lt; 0\\)\nasymptotically stable\nnode\n\n\n$_1 &lt; 0 &lt; _2 $\nunstable\nsaddle\n\n\n$0 &lt; _1 &lt; _2 $\nunstable\nnode\n\n\n\\(\\lambda = a\\pm i b,\\: a&lt; 0\\)\nasymptotically stable\nspiral\n\n\n\\(\\lambda = \\pm i b\\)\nstable, not asymptotically stable\ncenter\n\n\n\\(\\lambda = a\\pm i b,\\: a&gt; 0\\)\nunstable\nspiral\n\n\n\n\n\n\n5.6.2 Phase plane\nIn the two-dimensional case, the solutions of \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\) can be plotted as trajectories parameterized by time in the \\((x_1,x_2)\\) plane, often called the phase plane. Each of the cases in Table 5.1 has a characteristic type of phase plane portrait that illustrates major aspects of solution behaviors.\n\n5.6.2.1 Node\nWhen the eigenvalues are real and have the same sign, the steady state is called a node. If the sign is negative, all trajectories head into the steady state and it is stable; conversely, in the positive case it is unstable.\nHere is a phase plane plot for \\(\\mathbf{A}=\\twomat{-4}{-1}{-2}{-5}\\), which has eigenvalues \\(-3\\) and \\(-6\\).\n\n\nCode\nA = [-4 -1; -2 -5]\nλ, V = eigen(A)\n\n# plot eigenvectors\nP = Point2.(eachcol(V))\nfig, ax, scat = lines([-5P[1], 5P[1], Point2([NaN, NaN]), -5P[2], 5P[2]], \n    color=:black, linewidth=3, axis = (xlabel = \"x₁\", ylabel = \"x₂\") )\n\ncolr = cgrad(:seaborn_colorblind)[1:8]\nt = range(-0.5,8,300)\nx = zeros(2,300)\nxa = Point2{Float64}[]\nda = Point2{Float64}[]\nab = 20\nfor theta in 2pi*(0.5:23.5)/24\n    for j in 1:length(t)\n        X = exp(t[j]*A);\n        x[:,j] = X*[ cos(theta), sin(theta) ];\n    end\n    lines!(x[1,:], x[2,:], color=colr[1])\n  push!(xa, Point2(x[:,ab]))\n  push!(da, Point2(x[:,ab+1] - x[:,ab]))\nend\n\narrows!(xa,1da,linecolor=colr[1],arrowsize=12)\nscatter!([0],[0],markersize=10,color=:red)\n\nlimits!(1.1*[-1,1, -1,1]...)\nax.title = (\"stable node\")\nax.aspect = DataAspect()\n\nfig\n\n\n\n\n\nThe black lines show the directions of the eigenvectors. An initial state on one of those lines stays on it forever. Other initial conditions follow a path tending to become parallel to eigenvector \\(\\mathbf{v}_1\\), since the other component decays out more quickly. The plot for \\(-\\mathbf{A}\\) would just reverse all of the arrows and show an unstable steady state.\n\n\n5.6.2.2 Saddle\nWhen the eigenvalues are real and of different signs, the steady state is called a saddle. A saddle point is always unstable. The part of the solution along the negative eigenvector decays away, but the contribution from the positive eigenvector grows with time.\nHere is a phase plane plot for \\(\\mathbf{A}=\\twomat{-2}{-3}{-6}{-5}\\), which has eigenvalues \\(1\\) and \\(-8\\).\n\n\nCode\nA = [-2 -3; -6 -5]\nλ, V = eigen(A)\n\n# plot eigenvectors\nP = Point2.(eachcol(V))\nfig,ax,scat = lines([-5P[1], 5P[1], Point2([NaN, NaN]), -5P[2], 5P[2]],\n    color=:black, linewidth=3, axis = (xlabel = \"x₁\", ylabel = \"x₂\") )\n\ncolr = cgrad(:seaborn_colorblind)[1:8]\nt = range(-0.5,8,300)\nx = zeros(2,300)\nxa = Point2{Float64}[]\nda = Point2{Float64}[]\nab = 20\nfor theta in 2pi*(0.5:23.5)/24\n    for j in 1:length(t)\n        X = exp(t[j]*A);\n        x[:,j] = X*[ cos(theta), sin(theta) ];\n    end\n    lines!(x[1,:], x[2,:], color=colr[1])\n  push!(xa, Point2(x[:,ab]))\n  push!(da, Point2(x[:,ab+1] - x[:,ab]))\nend\n\narrows!(xa, 1da, linecolor=colr[1], arrowsize=12)\nscatter!([0],[0], markersize=10, color=:red)\n\nlimits!(1.1*[-1,1,-1,1]...)\nax.title = (\"saddle\")\nax.aspect = DataAspect()\n\nfig\n\n\n\n\n\nAn initial condition exactly on the stable black line (eigenvector) will approach the origin, but anything else ends up shooting away more or less in the direction of the unstable eigenvector.\n\n\n5.6.2.3 Spiral\nWhen the eigenvalues are complex conjugates with nonzero real part, the steady state is called a spiral. If the eigenvalues are \\(a \\pm i b\\), then all solutions contain \\(e^{at}e^{\\pm i b t}\\), or equivalently, \\(e^{at}\\cos{b t}\\) and \\(e^{at}\\sin{b t}\\). The real part causes growth and instability if \\(a&gt; 0\\), or decay and stability if \\(a &lt; 0\\). The imaginary part determines the angular speed of the spiral.\nHere is a phase plane plot for \\(\\mathbf{A}=\\twomat{1}{-2}{4}{-3}\\), which has eigenvalues \\(-1\\pm 2i\\).\n\n\nCode\nA = [1 -2; 4 -3];\nλ, V = eigen(A)\n\n# plot eigenvectors\nP = Point2.(eachcol(V))\nfig, ax, scat = lines([NaN], [NaN], color=:black, linewidth=3,\n    axis = (xlabel = \"x₁\",ylabel = \"x₂\") )\n\ncolr = cgrad(:seaborn_colorblind)[1:8]\nt = range(-1,7,300)\nx = zeros(2,300)\nxa = Point2{Float64}[]\nda = Point2{Float64}[]\nab = 50\nfor theta in 2pi*(0.5:6.5)/7\n    for j in 1:length(t)\n        X = exp(t[j]*A);\n        x[:,j] = X*[ cos(theta), sin(theta) ];\n    end\n    lines!(x[1,:], x[2,:], color=colr[1])\n  push!(xa, Point2(x[:,ab]))\n  push!(da, Point2(x[:,ab+1] - x[:,ab]))\nend\n\narrows!(xa, 1da, linecolor=colr[1], arrowsize=12)\nscatter!([0], [0], markersize=10, color=:red)\n\nlimits!(1.1*[-1,1,-1,1]...)\nax.title = (\"spiral\")\nax.aspect = DataAspect()\n\nfig\n\n\n\n\n\nThe eigenvectors are complex and don’t show up on the plot; there are no purely linear trajectories as in the other cases.\n\n\n5.6.2.4 Center\nThe special case of imaginary eigenvalues, \\(\\lambda=\\pm i b\\), is called a center. You can think of the trajectories as spirals that are perfectly neutral — that is, circles or ellipses. These equilibria are often associated with conservation principles, such as conservation of energy.\nHere is a phase plane plot for \\(\\mathbf{A}=\\twomat{0}{-1}{4}{0}\\), which has eigenvalues \\(\\pm 2i\\).\n\n\nCode\nA = [0 -1; 4 0];\nλ, V = eigen(A)\n\n# plot eigenvectors\nP = Point2.(eachcol(V))\nfig, ax, scat = lines([NaN], [NaN],\n    color=:black, linewidth=3, axis = (xlabel = \"x₁\", ylabel = \"x₂\") )\n\ncolr = cgrad(:seaborn_colorblind)[1:8]\nt = range(0,7,300)\nx = zeros(2,300)\nxa = Point2{Float64}[]\nda = Point2{Float64}[]\nab = 5\nfor r = 0.15:0.1:0.55\n    for j = 1:length(t)\n        X = exp(t[j]*A);\n        x[:,j] = X*[r; 0];\n    end\n    lines!(x[1,:], x[2,:], color=colr[1])\n    push!(xa, Point2(x[:,ab]))\n    push!(da, Point2(x[:,ab+1] - x[:,ab]))\nend\n\narrows!(xa, 1da, linecolor=colr[1], arrowsize=12)\nscatter!([0], [0], markersize=10, color=:red)\n\nlimits!(1.2*[-1,1,-1,1]...)\nax.title = (\"center\")\nax.aspect = DataAspect()\n\nfig\n\n\n\n\n\nA center is on a knife edge between asymptotically stable and unstable: small perturbations do not decay away, but they also can grow only by a bounded amount. This situation might be called weakly stable, neutrally stable, or simply stable but not asymptotically stable.\n\n\n\n5.6.3 Trace/determinant\nThe trace of a square matrix is the sum of its diagonal entries. In the \\(2\\times 2\\) case, the trace \\(\\tau\\) and determinant \\(\\Delta\\) completely determine the eigenvalues, and therefore they determine the nature of the phase portrait, as shown in Figure 5.1.\n\n\n\nFigure 5.1: Phase portrait type based on trace \\(\\tau\\) and determinant \\(\\Delta\\) of a \\(2\\times 2\\) matrix.\n\n\nYou can safely ignore the degenerate cases along \\(\\tau=4\\Delta^2\\). One easy takeaway is that the fixed point is stable if \\(\\tau \\le 0\\) ans \\(\\Delta \\ge 0\\), and unstable otherwise."
  },
  {
    "objectID": "ode_systems.html#the-matrix-exponential",
    "href": "ode_systems.html#the-matrix-exponential",
    "title": "5  Systems of ODEs",
    "section": "5.7 The matrix exponential",
    "text": "5.7 The matrix exponential\nAs we well know by now, the solution of the scalar linear IVP \\(x'=ax\\), \\(x(0)=x_0\\) is\n\\[\nx(t) = e^{at} x_0.\n\\]\nWouldn’t it be interesting if in the vector case \\(\\mathbf{x}'=\\mathbf{A} \\mathbf{x}\\), \\(\\bfx(0)=\\bfx_0\\), we could write\n\\[\n\\bfx(t) = e^{\\bfA t} \\bfx_0 ?\n\\]\nFunny you should ask.\n\n5.7.1 Diagonalization\nLet’s review the process of solving \\(\\mathbf{x}'=\\mathbf{A} \\mathbf{x}\\) using Theorem 5.4. If \\(\\bfA\\) has a diagonalization \\(\\bfA=\\bfV \\mathbf{D}\\bfV^{-1}\\), where \\(\\bfV\\) has eigenvector columns and \\(\\mathbf{D}\\) is a diagonal matrix of the eigenvalues, then the general solution is\n\\[\n\\bfx(t) = c_1 e^{t \\lambda_1} \\mathbf{v}_1 + \\cdots + c_n e^{t \\lambda_n} \\mathbf{v}_n = \\bfV \\bfE(t) \\bfc,\n\\]\nfor a constant vector \\(\\bfc\\), where\n\\[\n\\bfE(t) = \\begin{bmatrix} e^{t \\lambda_1} & & & \\\\ & e^{t \\lambda_2} & & \\\\ & & \\ddots & \\\\ & & & e^{t \\lambda_n} \\end{bmatrix}.\n\\]\nIf we also have an initial condition \\(\\bfx(0)=\\bfx_0\\), then\n\\[\n\\bfx_0 = \\bfx(0) = \\bfV \\bfE(0) \\bfc = \\bfV \\bfc,\n\\]\nso that \\(\\bfc = \\bfV^{-1} \\bfx_0\\). Hence the IVP solution is\n\\[\n\\bfx(t) = \\bfV \\bfE(t) \\bfV^{-1} \\bfx_0.\n\\]\nThe matrix \\(\\bfV \\bfE(t) \\bfV^{-1}\\) is essentially a special fundamental matrix \\(\\fundm\\) for which \\(\\fundm(0)=\\meye\\), which makes it especially convenient for an IVP.\n\n\n5.7.2 Change of basis\nGiven the diagonalization \\(\\bfA=\\bfV \\mathbf{D}\\bfV^{-1}\\), we can rewrite the ODE \\(\\bfx'=\\bfA\\bfx\\) as\n\\[\n\\bfx'=\\bfV \\mathbf{D}\\bfV^{-1} \\bfx.\n\\]\nWe multiply by \\(\\bfV^{-1}\\) on the left and, observing that \\(\\bfV\\) and its inverse are constant with respect to time, we get\n\\[\n(\\bfV^{-1} \\bfx)'= \\mathbf{D} (\\bfV^{-1} \\bfx) \\quad \\Rightarrow \\quad  \\bfy' = \\mathbf{D} \\bfy,\n\\]\nwhere we defined\n\\[\n\\bfy = \\bfV^{-1}\\bfx.\n\\]\nNote that \\(\\bfy\\) is simply the coordinate vector for \\(\\bfx\\) in the basis of eigenvectors. The diagonal system \\(\\bfy' = \\mathbf{D} \\bfy\\) completely decouples into \\(n\\) scalar equations\n\\[\ny_i' = \\lambda_i y_i, \\quad i=1,\\ldots,n.\n\\]\nThese are trivial to solve:\n\n\\[\ny_i(t) = e^{\\lambda_i t}y_i(0), \\quad i=1,\\ldots,n, \\Longrightarrow \\bfy = \\bfE(t) \\bfy(0).\n\\]\nChanging back to standard coordinates returns us again to\n\\[\n\\bfx(t) = \\bfV \\bfE(t) \\bfV^{-1} \\bfx(0).\n\\]\n\n\n5.7.3 Power series\nThe matrix \\(\\bfE(t)\\) has a nice structure:\n\\[\n\\begin{split}\n\\bfE(t) &= \\diagm{1 + (\\lambda_1 t) + \\frac{1}{2!} (\\lambda_1 t)^2 + \\cdots }{1 + (\\lambda_2 t) + \\frac{1}{2!} (\\lambda_2 t)^2 + \\cdots }{1 + (\\lambda_n t) + \\frac{1}{2!} (\\lambda_n t)^2 + \\cdots }\\\\\n&= \\diagm{1}{1}{1} + t \\diagm{\\lambda_1}{\\lambda_2}{\\lambda_n} + \\frac{t^2}{2!} \\diagm{\\lambda_1^2}{\\lambda_2^2}{\\lambda_n^2} + \\cdots.\n\\end{split}\n\\]\nThe simplicity of the diagonal matrix \\(\\bfD\\) lets us write\n\\[\n\\bfE(t) = \\meye + t\\bfD + \\frac{t^2}{2!}\\bfD^2 + \\cdots.\n\\]\nThis strongly hints at the definition\n\\[\ne^{t\\mathbf{A}} = \\mathbf{I} + t\\mathbf{A} + \\frac{1}{2!}t^2 \\mathbf{A}^2 + \\frac{1}{3!} t^3 \\mathbf{A}^3 + \\cdots\n\\tag{5.12}\\]\nfor a square matrix \\(\\bfA\\), so that \\(\\bfE(t) = e^{t\\bfD}\\). Let’s not worry too much about whether this series converges (it does). What are its properties?\n\nTheorem 5.5 (Matrix exponential) Let \\(\\mathbf{A},\\mathbf{B}\\) be \\(n\\times n\\) matrices. Then\n\n\\(e^{t\\mathbf{A}}=\\mathbf{I}\\) if \\(t=0\\),\n\\(\\displaystyle \\dd{}{t}e^{t\\mathbf{A}} = \\mathbf{A} e^{t\\mathbf{A}} = e^{t\\mathbf{A}}\\mathbf{A}\\),\n\\([e^{t\\mathbf{A}}]^{-1} = e^{-t\\mathbf{A}}\\),\n\\(e^{(s+t)\\mathbf{A}} = e^{s\\bfA} \\cdot e^{t\\bfA} = e^{t\\bfA} \\cdot e^{s\\bfA}\\)\nIf \\(\\mathbf{A}\\mathbf{B}=\\mathbf{B}\\mathbf{A}\\), then \\(e^{t(\\mathbf{A}+\\mathbf{B})} = e^{t\\mathbf{A}}\\cdot e^{t\\mathbf{B}} = e^{t\\mathbf{B}}\\cdot e^{t\\mathbf{A}}\\).\n\n\nThese conclusions follow pretty easily from the series definition Equation 5.12. They are all essential to what we normally expect from an exponential function, although in the last case we have a new restriction.\nFrom these properties we can make the connection to the IVP.\n\nTheorem 5.6 If \\(\\bfA\\) is a constant square matrix, then\n\\[\n\\bfx(t) = e^{t \\bfA} \\bfx_0\n\\]\nsolves the initial-value problem \\(\\mathbf{x}'=\\mathbf{A} \\mathbf{x}\\), with \\(\\bfx(0)=\\bfx_0\\).\n\n\nProof. With \\(\\bfx\\) as defined in the theorem statement, we calculate\n\\[\n\\dd{}{t} \\bfx(t) = \\Bigl( \\dd{}{t}e^{t \\bfA} \\Bigr) \\bfx_0 =  \\mathbf{A} e^{t\\mathbf{A}} \\bfx_0 = \\bfA \\bfx(t),\n\\]\nusing property 2 above. Furthermore,\n\\[\n\\bfx(0) = e^{0 \\bfA} \\bfx_0 = \\meye \\bfx_0 = \\bfx_0,\n\\]\nusing property 1 above.\n\n\n\n5.7.4 Computation\nSumming the infinite series of matrices in Equation 5.12 seems a daunting project. Fortunately we have a number of ways to avoid that.\n\n5.7.4.1 Diagonal matrix\nAs derived above, if \\(\\bfD\\) is diagonal with \\(\\lambda_1,\\ldots,\\lambda_n\\) on the diagonal, then \\(e^{t\\bfD}\\) is diagonal with \\(e^{t\\lambda_1},\\ldots,e^{t\\lambda_n}\\) on the diagonal.\n\n\n5.7.4.2 Nilpotent matrix\n\nDefinition 5.6 A nilpotent matrix \\(\\bfA\\) satisfies \\(\\bfA^k=\\bfzero\\) for some integer \\(k\\).\n\n\nTheorem 5.7 A matrix is nilpotent if and only if all of its eigenvalues are zero.\n\nFor a nilpotent matrix, the infinite series for the matrix exponential simply truncates at a finite power.\n\nExample 5.10 Find \\(e^{t\\bfA}\\) if \\(\\bfA= \\begin{bmatrix} 0 & 1 & 2 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\).\n\nSolution. The matrix is triangular and evidently has all zero eigenvalues. We get\n\\[\n\\begin{split}\ne^{t\\mathbf{A}} &= \\mathbf{I} + t\\mathbf{A} + \\frac{1}{2!}t^2 \\mathbf{A}^2 + \\frac{1}{3!} t^3 \\mathbf{A}^3 + \\cdots \\\\\n&= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} + t \\begin{bmatrix} 0 & 1 & 2 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} + t^2 \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix},\n\\end{split}\n\\]\nsince \\(\\bfA^3=\\bfzero\\). This simplifies to\n\\[\ne^{t\\mathbf{A}} = \\begin{bmatrix}\n1 & t & 2t+t^2 \\\\\n0 & 1 & t \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\n\n\n\n\n\n5.7.4.3 Fundamental matrix\n\nTheorem 5.8 (Matrix exponential by fundamental matrix) If \\(\\fundm(t)\\) is any fundamental matrix for \\(\\bfx'=\\bfA\\bfx\\), then\n\\[\ne^{t\\bfA} = \\fundm(t) \\fundm(0)^{-1}.\n\\]\n\n\nExample 5.11 Given\n\\[\n\\mathbf{A} = \\twomat{1}{1}{4}{1}\n\\]\nand the eigenpairs \\(\\lambda_1=3\\), \\(\\mathbf{v}_1 = \\twovec{1}{2}\\) and \\(\\lambda_2=-1\\), \\(\\mathbf{v}_2=\\twovec{1}{-2}\\), find \\(e^{t\\mathbf{A}}\\).\n\nSolution. We start with the fundamental matrix\n\\[\n\\fundm(t) = \\begin{bmatrix} e^{3t} \\mathbf{v}_1 & e^{-t} \\mathbf{v}_2 \\end{bmatrix}\n= \\twomat{e^{3t}}{e^{-t}}{2e^{3t}}{-2e^{-t}}.\n\\]\nNow we find\n\\[\n\\fundm(0)^{-1} = \\twomat{1}{1}{2}{-2}^{-1} = \\frac{1}{-4} \\twomat{-2}{-1}{-2}{1}.\n\\]\nThus\n\\[\ne^{t\\mathbf{A}} = \\frac{1}{4} \\twomat{e^{3t}}{e^{-t}}{2e^{3t}}{-2e^{-t}}\n\\twomat{2}{1}{2}{-1} = \\frac{1}{4} \\twomat{2e^{3t}+2e^{-t}}{e^{3t}-e^{-t}}{4e^{3t}-4e^{-t}}{2e^{3t}+2e^{-t}} .\n\\]\n\n\n\n\n5.7.4.4 Diagonalization\nWe derived the formula \\(e^{t\\bfA} = \\bfV e^{t\\bfD} \\bfV^{-1}\\). If the diagonalization is fully known, this is handy, but otherwise it’s no easier than using the fundamental matrix method above.\n\n\n5.7.4.5 Polynomial representation\nThanks to a result known as the Cayley–Hamilton theorem, the following is true.\n\nTheorem 5.9 (Matrix exponential by polynomial) If \\(\\bfA\\) has eigenvalues \\(\\lambda_1,\\ldots,\\lambda_n\\), then there exist functions \\(c_0(t),\\ldots,c_{n-1}(t)\\) such that if we define\n\\[\nf(t,z) = c_0(t) + c_1(t) z + \\cdots + c_{n-1}(t) z,\n\\tag{5.13}\\]\nthen\n\\[\nf(t,\\lambda_k) = e^{t\\lambda_k}, \\qquad k=1,\\ldots,n,\n\\tag{5.14}\\]\nand\n\\[\ne^{t\\bfA} = f(t,\\bfA) = c_0(t)\\meye + c_1(t)\\bfA + \\cdots + c_{n-1}(t)\\bfA^{n-1}.\n\\tag{5.15}\\]\n\nThe idea is to use the eigenvalues to define the linear system in Equation 5.14, which is solved to determine the \\(c_k(t)\\) functions. The matrix exponential is then computed from Equation 5.15. The eigenvectors aren’t needed at all.\n\nExample 5.12 Given that\n\\[\n\\mathbf{A} = \\twomat{1}{1}{4}{1}\n\\]\nhas eigenvalues \\(\\lambda_1=3\\) and \\(\\lambda_2=-1\\), find \\(e^{t\\bfA}\\).\n\nSolution. We write out Equation 5.14:\n\\[\n\\begin{split}\nc_0 + 3c_1 &= e^{3t}   \\\\\nc_0 - c_1 &= e^{-t}.\n\\end{split}\n\\]\nCramer’s rule leads to\n\\[\nc_0 = \\frac{-e^{3t}-3e^{-t}}{-4}, \\qquad c_1 = \\frac{e^{-t}-e^{3t}}{-4}.\n\\]\nInserting these into Equation 5.15 gives\n\\[\n\\begin{split}\ne^{t\\bfA} &= \\frac{e^{3t}+3e^{-t}}{4}\\, \\meye + \\frac{e^{3t}-e^{-t}}{4} \\, \\bfA \\\\\n&= \\frac{1}{4}\\twomat{2(e^{3t}+e^{-t})}{e^{3t}-e^{-t}}{4(e^{3t}-e^{-t})}{2(e^{3t}+e^{-t})}.\n\\end{split}\n\\]\n\n\nThe process is modified a bit for complex eigenvalues. Note that the \\(c_k\\) functions in Equation 5.13 must be real.\n\nExample 5.13 Given that\n\\[\n\\mathbf{A} = \\twomat{1}{-1}{5}{-3}\n\\]\nhas eigenvalues \\(-1\\pm i\\), find \\(e^{t\\bfA}\\) and the solution of the IVP \\(\\bfx'=\\mathbf{A}\\bfx\\), \\(\\bfx(0)=\\twovec{2}{1}\\).\n\nSolution. Rather than writing out Equation 5.14 for both conjugate eigenvalues, we use just one and apply Euler’s formula to take the real and imaginary parts:\n\\[\ne^{-t+it} = c_0 + c_1 (-1+i) \\quad \\Rightarrow \\quad e^{-t}\\cos(t) = c_0 - c_1, \\quad e^{-t}\\sin(t) = c_1.\n\\]\nThis gives us \\(c_1=e^{-t}\\sin(t)\\) and \\(c_0 = e^{-t}[\\cos(t)+\\sin(t)]\\). Thus,\n\\[\n\\begin{split}\ne^{t\\bfA} &= e^{-t}[\\cos(t)+\\sin(t)]\\, \\meye + e^{-t}\\sin(t)\\, \\bfA \\\\\n&= e^{-t}\\twomat{\\cos(t)+2\\sin(t)}{-\\sin(t)}{5\\sin(t)}{\\cos(t)-2\\sin(t)}.\n\\end{split}\n\\]\nThe solution of the IVP is then\n\\[\n\\bfx(t) = e^{t\\bfA} \\bfx(0) = e^{-t}\\twomat{\\cos(t)+2\\sin(t)}{-\\sin(t)}{5\\sin(t)}{\\cos(t)-2\\sin(t)} \\twovec{2}{1} = e^{-t} \\twovec{2\\cos(t)+2\\sin(t)}{\\cos(t)+8\\sin(t)}.\n\\]\n\n\nThe process also must be modified if there are repeated eigenvalues. Suppose \\(\\lambda_1=\\lambda_2\\) has algebraic multiplicity 2. In that case, the second equation in the linear system Equation 5.14 is a duplicate of the first. We replace it with\n\\[\n\\pp{f}{z}(t,\\lambda_1) = \\pp{}{z} \\bigl[ e^{tz}  \\bigr]_{z=\\lambda_1} = t e^{\\lambda_1 t}.\n\\tag{5.16}\\]\n\nExample 5.14 A critically damped free oscillator with natural frequency \\(\\omega_0\\) is equivalent to an ODE system with the matrix\n\\[\n\\bfA = \\twomat{0}{1}{-\\omega_0^2}{-2\\omega_0}.\n\\]\nThe characteristic polynomial is \\(z^2 + 2\\omega_0z + \\omega_0^2\\), which has the double root \\(\\lambda=-\\omega_0\\). We apply Equation 5.14 and Equation 5.16 to get the system\n\\[\n\\begin{split}\nc_0 - \\omega_0 c_1 &= e^{-\\omega_0 t} \\\\\nc_1 &= te^{-\\omega_0 t}.\n\\end{split}\n\\]\nFrom this we get \\(c_1 = te^{-\\omega_0 t}\\), \\(c_0 = (1+\\omega_0 t)e^{-\\omega_0 t}\\). Hence\n\\[\ne^{t\\bfA} = e^{-\\omega_0 t}(1+\\omega_0 t)\\, \\meye + te^{-\\omega_0 t}\\, \\bfA = e^{-\\omega_0 t} \\twomat{1+\\omega_0 t}{t}{-\\omega_0^2 t}{1-\\omega_0t}.\n\\]\nThe oscillator position is the first component of the general solution,\n\\[\ne^{-t\\omega_0}[ C_1 (1+t\\omega_0) + C_2 t],\n\\]\nwhich is equivalent to the general solution we saw before for this problem.\n\nIf \\(\\lambda_1=\\lambda_2=\\lambda_3\\), then the first three equations of Equation 5.14 become\n\\[\n\\begin{split}\nf(\\lambda_1) &= e^{\\lambda_1 t} \\\\\nf_z(\\lambda_1) &= t e^{\\lambda_1 t} \\\\\nf_{zz}(\\lambda_1) &= t^2 e^{\\lambda_1 t}.\n\\end{split}\n\\]\nIn principle, the method generalizes to any algebraic multiplicity.\n\nExample 5.15 The matrix \\(\\bfA = \\begin{bmatrix} {2}& {1}& {1}\\\\ {1} & {2} & {1}\\\\ {-2}& {-2} & {-1} \\end{bmatrix}\\) has a triple eigenvalue \\(\\lambda_1=1\\). Find \\(e^{t\\bfA}\\).\n\nSolution. The system to solve is\n\\[\n\\begin{split}\nc_0 + \\lambda_1 c_1 + \\lambda_1^2 c_2 &= e^{\\lambda_1 t} \\\\\nc_1 + 2\\lambda_1 c_2 &= t e^{\\lambda_1 t} \\\\\n2c_2 &= t^2 e^{\\lambda_1 t},\n\\end{split}\n\\]\nwhich is\n\\[\n\\begin{split}\nc_0 + c_1 + c_2 &= e^{t} \\\\\nc_1 + 2 c_2 &= te^{t} \\\\\n2 c_2 &= t^2e^{t}.\n\\end{split}\n\\]\nThis has solution \\(c_0 = \\left(1-t+\\frac{1}{2}t^2\\right)e^t\\), \\(c_1=(t-t^2)e^t\\), \\(c_2=\\frac{1}{2} t^2 e^{t}\\). To finish the problem we calculate\n\\[\ne^t \\left[ \\left(1-t+\\frac{1}{2}t^2\\right) \\meye + (t-t^2)\\bfA + \\frac{1}{2} t^2 \\bfA^2 \\right] = e^t \\begin{bmatrix}\nt+1 & t & t \\\\\nt & t+1 & t \\\\\n-2 t & -2 t & 1-2 t \\\\\n\\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "ode_systems.html#linearization",
    "href": "ode_systems.html#linearization",
    "title": "5  Systems of ODEs",
    "section": "5.8 Linearization",
    "text": "5.8 Linearization\nA pendulum with length \\(L\\) and angle of deflection \\(\\theta(t)\\) from the vertical is governed by the nonlinear second-order equation\n\\[\n\\ddd{\\theta}{t} + \\frac{g}{L} \\sin(\\theta) = 0,\n\\]\nwhere \\(g\\) is gravitational acceleration. It’s standard to argue that as long as \\(|\\theta|\\) remains small, a good approximation is the linear problem\n\\[\n\\ddd{\\theta}{t} + \\frac{g}{L} \\theta = 0,\n\\]\nbecause \\(\\sin \\theta \\approx \\theta\\). We want to get more systematic with this process.\nFirst note that we can recast the nonlinear second-order problem as a first-order system in two variables, just as we did for linear equations. Define \\(x=\\theta\\) and \\(y=\\theta'\\). Then\n\\[\n\\begin{split}\n    \\dd{x}{t} & = y, \\\\\n    \\dd{y}{t} & = -\\frac{g}{L}\\sin(x).\n\\end{split}\n\\]\nThis trick works for any nonlinear second-order equation \\(\\theta''=g(t,\\theta,\\theta')\\). Thus we can focus on problems in the general form\n\\[\n\\begin{split}\n    \\dd{x}{t} & = F(x,y), \\\\\n    \\dd{y}{t} & = G(x,y).\n\\end{split}\n\\]\nAs we did with single scalar equations, we will pay close attention to steady states or fixed points of these systems. Here this means finding constants \\((\\hat{x},\\hat{y})\\) such that\n\\[\nF(\\hat{x},\\hat{y})=G(\\hat{x},\\hat{y})=0.\n\\]\nFor the nonlinear pendulum, both \\((0,0)\\) (straight down, at rest) and \\((\\pi,0)\\) (straight up, at rest) are steady states.\nWe are interested in the stability of fixed points, i.e., the dynamics close to them. We will use linear approximations of the functions \\(F\\) and \\(G\\) near a fixed point:\n\\[\n\\begin{split}\n    F(x,y) & \\approx F(\\hat{x},\\hat{y}) + \\pp{F}{x}\\cdot (x-\\hat{x}) + \\pp{F}{y} \\cdot (y-\\hat{y}),\\\\\n    G(x,y) & \\approx G(\\hat{x},\\hat{y}) + \\pp{G}{x} \\cdot (x-\\hat{x}) + \\pp{G}{y} \\cdot (y-\\hat{y}),\\\\\n\\end{split}\n\\]\nwhere it is understood for brevity that the partial derivatives are all evaluated at \\((\\hat{x},\\hat{y})\\). Given that \\((\\hat{x},\\hat{y})\\) is a fixed point, \\(F(\\hat{x},\\hat{y})=G(\\hat{x},\\hat{y})=0\\).\nNow define\n\\[\nu_1(t)=x(t)-\\hat{x},\\quad u_2(t)=y(t)-\\hat{y}.\n\\]\nThen\n\\[\n\\begin{split}\n    \\dd{u_1}{t} = \\dd{x}{t} & \\approx \\pp{F}{x} u_1 + \\pp{F}{y} u_2, \\\\\n    \\dd{u_2}{t} = \\dd{y}{t} & \\approx \\pp{G}{x} u_1 + \\pp{G}{y} u_2.\n\\end{split}\n\\]\nThis inspires the following definition.\n\nDefinition 5.7 (Jacobian matrix) The Jacobian matrix of the system \\(x'=F(x,y),\\, y'=G(x,y)\\) is\n\\[\n:label: nl-eq-jacobian\n\\mathbf{J}(x,y) = \\twomat{\\displaystyle \\pp{F}{x}}{\\displaystyle \\pp{F}{y}}{\\displaystyle \\pp{G}{x}}{\\displaystyle \\pp{G}{y}}.\n\\]\n\nThe Jacobian matrix is essentially the derivative of a 2D vector field. As such, it is also a function of the variables \\(x\\) and \\(y\\).\n\n5.8.1 Stability\nLet’s summarize. In the neighborhood of a fixed point \\((\\hat{x},\\hat{y})\\), we can define the deviation from equilibrium by the variables \\(u_1(t)=x(t)-\\hat{x}\\), \\(u_2(t)=y(t)-\\hat{y}\\). These variables approximately satisfy \\(\\mathbf{u}'=\\mathbf{J}(\\hat{x},\\hat{y})\\mathbf{u}\\), which is a linear, constant-coefficient system in two dimensions. Thus, near a steady state, dynamics are mostly linear.\nWe are now right back into the phase plane we studied earlier. In particular:\n\nTheorem 5.10 The stability of a steady state is (usually) determined by the eigenvalues of the Jacobian matrix at the steady state.\n\n\nExample 5.16 Let’s examine the steady states of a pendulum with damping,\n\\[\n\\ddd{\\theta}{t} + \\gamma \\theta' + \\frac{g}{L} \\sin(\\theta) = 0,\n\\]\nwith \\(\\gamma &gt; 0\\). It transforms into the first-order system\n\\[\n\\begin{split}\n    \\dd{x}{t} & = y, \\\\\n    \\dd{y}{t} & = -\\frac{g}{L}\\sin(x) - \\gamma y.\n\\end{split}\n\\]\nThese define \\(F(x,y)\\) and \\(G(x,y)\\). From here we note all the first partial derivatives:\n\\[\n\\begin{split}\n\\pp{F}{x} = 0, & \\qquad \\pp{F}{y} = 1,\\\\\n\\pp{G}{x} = - \\frac{g}{L}\\cos(x), & \\qquad \\pp{G}{y} = -\\gamma.\n\\end{split}\n\\]\nThe first steady state we consider is at the origin. Here the Jacobian matrix is\n\\[\n\\begin{bmatrix}\n0 & 1 \\\\ -g/L & -\\gamma\n\\end{bmatrix}.\n\\]\nThe eigenvalues are roots of \\(\\lambda^2 +\\gamma \\lambda + \\frac{g}{L}\\), the same as a linear oscillator with \\(\\omega_0 = \\sqrt{g/L}\\) and \\(Z = \\gamma/2\\omega_0\\). If \\(Z&lt;1\\), we get a stable spiral. If \\(Z\\ge 1\\), we get a stable node. This all agrees well with our experience with a pendulum hanging straight down.\nThe other steady state is at \\((\\pi,0)\\). Now the Jacobian is\n\\[\n\\begin{bmatrix}\n0 & 1 \\\\ g/L & -\\gamma\n\\end{bmatrix},\n\\]\nwith characteristic polynomial \\(\\lambda^2 + \\gamma \\lambda - \\frac{g}{L}\\). The eigenvalues are\n\\[\n\\lambda_1 = -\\gamma + \\sqrt{\\gamma^2+4\\frac{g}{L}}, \\quad \\lambda_2 = -\\gamma - \\sqrt{\\gamma^2+4\\frac{g}{L}}.\n\\]\nIt’s obvious that \\(\\lambda_2 &lt; 0\\). Since \\(\\gamma^2+\\frac{4g}{L} &gt; \\gamma^2\\), it follows that \\(\\lambda_1 &gt; 0\\). Hence this equilibrium is a saddle point.\n\nThe caveat on using eigenvalues for stability is when they both have zero real part, which is weakly stable in the linear sense. In such cases the details of the nonlinear terms of the system can swing the stability either way.\n\nExample 5.17 The system\n\\[\n\\begin{split}\n\\frac{dx}{dt} &= 3x-\\frac{xy}{2}\\\\\n\\frac{dy}{dt} &= -y+\\frac{xy}{4}\\\\\n\\end{split}\n\\]\nis called a predator–prey equation. If species \\(y\\) is set to zero, species \\(x\\) would grow exponentially on its own (the prey). Similarly, species \\(y\\) would die off on its own (predator). We assume that encounters between the species are jointly proportional to the population of each, and they subtract from the prey and add to the predators.\nWe find fixed points when \\(3x-(x y/2)=(x y/4)-y=0\\), which has two solutions, \\((0,0)\\) and \\((4,6)\\). The Jacobian matrix of the system is\n\\[\n\\mathbf{J}(x,y) = \\begin{bmatrix} F_x & F_y  \\\\ G_x & G_y \\end{bmatrix} = \\begin{bmatrix} 3-y/2 & -x/2 \\\\ y/4 & -1+x/4 \\end{bmatrix}.\n\\]\nAt the origin, the Jacobian becomes\n\\[\n\\mathbf{J}(0,0) = \\begin{bmatrix} 3 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n\\]\nThe eigenvalues of a diagonal matrix (in fact, any triangular matrix) are the diagonal entries. Hence the origin is a saddle point.\nAt the other steady state we have\n\\[\n\\mathbf{J}(4,6) = \\twomat{0}{-2}{3/2}{0}.\n\\]\nThe characteristic polynomial is \\(\\lambda^2 + 3\\), so the eigenvalues are \\(\\pm i\\sqrt{3}\\). Close to the point, the orbits are impossible to tell apart from those of a linear center:\n\n\n\nlinear center\n\n\nHowever, as we zoom out, the nonlinearity of the system makes its influence felt:\n\n\n\nnonlinear center\n\n\nFor a different system, however, the nonlinearity could cause this point to be asymptotically stable or unstable."
  }
]