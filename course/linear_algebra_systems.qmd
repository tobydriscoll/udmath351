---
format:
  html:
    code-fold: true
jupyter:
  jupytext:
    cell_metadata_filter: '-all'
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.0
  kernelspec:
    display_name: Julia 1.9
    language: julia
    name: julia-1.9
---

# Linear algebraic systems

::: {.content-visible unless-format="pdf"}
{{< include _macros.qmd >}}
:::

```{julia}
using Plots, DifferentialEquations
default(label="", linewidth=3, markersize=4, size=(500,320))
```

## Overview 

It might feel silly, but let's review what it means to solve the linear equation

$$ax = b$$

for $x$.

- If $a\neq 0$, there is a unique solution, $x=b/a$.
- Otherwise,
   - If also $b=0$, then every value of $x$ is a valid solution.
   - Otherwise, there are no solutions.

It turns out that no matter how many equations and variables we have, the only three possibilities are the ones above: zero, one, or infinitely many solutions. The main difference is that the condition "$a=0$" has to be generalized.

::::{#def-linsys-consistent} 
# Consistent linear system
A linear system that has at least one solution is called **consistent**. Otherwise, it is **inconsistent**.
::::

### Two variables

Going one more baby step to two equations in two variables, we want to solve

$$
\begin{split}
ax + by &= f, \\
cx + dy &= g.
\end{split}
$$

There is some easy geometric intuition here. Each equation represents a straight line in the plane, and solving both equations simultaneously means finding an intersection of these lines. Our cases above translate directly into:

- If the lines are not parallel, there is a unique solution.
- Otherwise,
   - If the lines are identical, there are infinitely many solutions.
   - Otherwise, there are no solutions.

It's not hard to turn those statements into algebraic conditions. The slopes of the two lines are (ignoring infinities for a moment) $-a/b$ and $-c/d$, and we can say the slopes are equal if and only if $ad=bc$.

- If $ad \neq bc$, there is a single solution.
- Otherwise,
   - If one equation is a multiple of the other, there are infinitely many solutions.
   - Otherwise, there are no solutions.

::::{#exm-}
Find all solutions to the equations

$$
\begin{split}
x - 3y & = 1, \\
-2x + 6y & = 2.
\end{split}
$$

:::{.solution}
We identify $(a,b,c,d)=(1,-3,-2,6)$, and $ad-bc=6-6=0$. So we know there is not a unique solution (i.e., the lines are parallel). Dividing the second equation by $-2$ leads to the equivalent system

$$
\begin{split}
x - 3y & = 1, \\
x - 3y & = -1.
\end{split}
$$

It's now clear that there is no way to satisfy both equations simultaneously. The system is inconsistent.
:::
::::

### General case

Time to put on our big-kid pants. For $m$ equations in $n$ variables, we need to use subscripts rather than different letters for everything.

::::{#def-linsys-system}
A **linear-algebraic system**, usually just written **linear system**, is the set of simultaneous equations
$$
\begin{split}
A_{11} x_1 + A_{12} x_2 + \cdots + A_{1n} x_n & = b_1 \\
A_{21} x_1 + A_{22} x_2 + \cdots + A_{2n} x_n & = b_2 \\
& \vdots \\
A_{m1} x_1 + A_{m2} x_2 + \cdots + A_{mn} x_n & = b_m,
\end{split}
$$ {#eq-linsys-def}
where all the $A_{ij}$ and $b_i$ are assumed to be known. A **solution** of the system is a list of values $x_1,\ldots,x_n$ that makes all the equations true. 
::::

We want to gather similar elements in @eq-linsys-def into collective mathematical objects.

::::{#def-linsys-vector} 
# Vector
A **vector** is a finite collection of numbers known as its **elements**. The set of all vectors with $n$ real-valued elements is denoted $\real^n$. If the elements are complex numbers, we use the symbol $\complex^n$. 
::::

I use boldfaced lowercase letters to represent vectors, and a subscript to refer to an individual element within one. For instance, $x_3$ is the third element of a vector $\bfx$. 

Next, we come to the doubly-indexed values $A_{ij}$. The first index is the equation number, and the second corresponds to the variable number. This implies a 2D table or array of values.

::::{#def-linsys-matrix} 
# Matrix
A **matrix** is an $m\times n$ array of numbers known as its **elements**. The set of all $m\times n$ matrices with real elements is denoted $\rmn{m}{n}$, and with complex elements it's $\cmn{m}{n}$. 
::::

I use boldfaced uppercase letters to represent matrices, and a pair of subscripts to refer to an individual element within one. For instance, $A_{23}$ is the element in row 2, column 3 of the matrix $\bfA$.

:::{.callout-important}
Matrix subscripts are always in the order row first, column second. (This is the opposite of Excel---which is just one of myriad reasons that Excel is evil.)
:::

In the context of @eq-linsys-def, we call the $m\times n$ matrix $\bfA$ the **coefficient matrix** of the linear system. 

::: {.callout-important}
The rows of the coefficient matrix correspond to the equations in the linear system, and the columns correspond to the variables of the system.
:::

The vector $\bfb\in\real^m$ implied in @eq-linsys-def doesn't have a standard name, but I will call it the **forcing vector** by analogy with linear ODEs. Altogether, $\bfA$ and $\bfb$ are the data of the linear system, and the vector $\bfx \in \real^{n}$ is the solution.

::::{#exm-}
Sometimes zeros are needed as placeholders in the coefficient matrix. In the linear system

$$
\begin{split}
  x_1 - x_2 + 3 x_3 &= 4,\\
  2x_2 + 5x_4 &= -1,
\end{split}
$$

the coefficient matrix is $2\times 4$, 

$$
\bfA =
\begin{bmatrix}
1 & -1 & 3 & 0 \\ 0 & 2 & 0 & 5
\end{bmatrix},
$$

and the right-side vector is $\twovec{4}{-1}$.
::::

## Row elimination

You've probably solved small systems of equations by substitution. In order to solve systems with more equations and variables, we systematize this idea as an *elimination algorithm*. The goal of elimination is to transform the system to an equivalent one whose solution(s) we can deduce easily. There are three legal moves in this game:

::::{#def-row_elimination-operations} 
# Row operations
The following operations do not change the solutions of a linear system:

1. Swap the positions of two equations.
2. Multiply an equation by a nonzero constant.
3. Add a multiple of one equation to another equation.
::::

::: {.callout-note}
Recall that the equations of a linear system correspond to rows of the coefficient matrix $\bfA$ and forcing vector $\bfb$, hence the name "row operations."
:::

:::::{#exm-gauss-elimination}
Let's use elimination to solve the $3\times 3$ system

$$
\begin{split}
x_1 - x_2  - x_3 & = 2, \\
3x_1 - 2x_2 & = 9, \\
x_1 - 2x_2 - x_3 & = 5.
\end{split}
$$

The first step is to use the first equation to eliminate $x_1$ from the second and third equations. We therefore add $-3$ times equation 1 to equation 2, and $-1$ times equation 1 to equation 3:

$$
\begin{split}
 x_1 - x_2  - x_3  & = 2, \\
(3x_1 - 2x_2) - 3(x_1 - x_2  - x_3) & = 9 - 3(2),\\
(x_1 - 2x_2 - x_3) - 1(x_1 - x_2  - x_3) & = 5 - 1(2) .
\end{split}
$$

This takes us to
$$
\begin{split}
 x_1 - x_2  - x_3  & = 2,  \\
x_2 + 3x_3 &= 3, \\
-x_2 &= 3.
\end{split}
$$

::: {.callout-note}
It's tempting to grab that last equation above and use it to remove $x_2$ from everything else. That move certainly works out here. But in the context of a fully automatic algorithm that works every time, we will continue as though the last equation still might contain a dependence on $x_3$. 
:::

Next, we leave the first equation alone and use the second to eliminate $x_2$ from all the others below it.

$$
\begin{split}
 x_1 - x_2  - x_3  & = 2, \\
x_2 + 3x_3 & = 3, \\
(-x_2) + (x_2+3x_3)  & = 3 + 3.
\end{split}
$$

We now have the system, equivalent to the original one, comprising

$$
\begin{split}
x_1 - x_2  - x_3  & = 2, \\
x_2 + 3x_3 & = 3, \\
3x_3  & = 6.
\end{split}
$$

We can now deduce that $x_3=2$ from the last equation. Moving up to the second equation, we then find that $x_2=3-3(2)=-3$. Finally, the first equation yields $x_1=2+(-3)+(2)=1$.
:::::

The process in @exm-gauss-elimination leading to triangular system is commonly known as **Gaussian elimination**. (It's a poor name, as the process was known at least in China thousands of years before Gauss.) Finding the solution from the triangular form is called **backward substitution**.

### Augmented matrix

Before looking further into the elimination process, we will lighten the notational load by using matrices. 

::::{#def-linsys-augmented}
The **augmented matrix** of the linear system @eq-linsys-def is the $m\times (n+1)$ matrix $\mathbf{G} = \augmat{\bfA}{\bfb}$.
::::

:::::{#exm-augmented-matrix}
The starting system in @exm-gauss-elimination has augmented matrix

$$
\augmat{\begin{matrix}
1 & -1 & -1  \\
3 &-2 & 0 \\
1 & -2 & -1
\end{matrix}}{
  \begin{matrix}2\\9\\5\end{matrix}
  }.
$$

The final system in that example has augmented matrix
$$
\augmat{\begin{matrix}
1 & -1 & -1  \\
0 & 1 & 3 \\
0 & 0 & 3
\end{matrix}}{
  \begin{matrix}2\\3\\6\end{matrix}
  }.
$$
:::::

The nonzeros in the final augmented matrix have a triangular shape that is the key to making back substitution work. Next, we will get specific about what this form requires.

### Row echelon form

Here is a formal description of the goal of row elimination.

::::{#def-row-echelon}
# Row echelon form (RE form)
A matrix is in RE form if:

1. Any rows that are completely zero are at the bottom.
2. The leftmost nonzero element in a row, called the **leading element**, is to the left of any other leading elements below it.
::::

To achieve RE form from a given starting matrix, we start at the top left and use row operations to introduce zeros under the leading element of the first row. We then move down one row and repeat. It's easier to understand from more examples than to write out a formal algorithm.

:::{#exm-RE-inconsistent}
We start from
$$
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 2 & 4 & 0 & 2 \\ 2 & 3 & 2 & 5 \\ -1 & 1 & 3 & 5
\end{bmatrix}
$$

In the first row, the 1 will be the leading element. We then multiples of the first row to create zeros in the rows underneath it.

$$
\begin{split}
& \stackrel{R_2=R_2 - 2R_1}{\Longrightarrow} 
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 0 & 0 & 8 & -8 \\ 2 & 3 & 2 & 5 \\ -1 & 1 & -8 & 5
\end{bmatrix} \\[1ex]
& \stackrel{R_3=R_3 - 2R_1}{\Longrightarrow} 
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 0 & 0 & 8 & -8 \\ 0 & -1 & 10 & -5 \\ -1 & 1 & -8 & 5
\end{bmatrix} \\[1ex]
& \stackrel{R_4=R_4 + 1R_1}{\Longrightarrow} 
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 0 & 0 & 8 & -8 \\ 0 & -1 & 10 & -5 \\ 0 & 3 & -12 & 10
\end{bmatrix}
\end{split}
$$

Now that leading 1 in the first row is all set. From now on, we can safely ignore the first column, as those zeros will remain throughout. 

Moving to the second row, we notice that if we designated the 8 to be the leading element, then at least one of the rows underneath it would have a leading element to its left. That's a violation of RE form. So we can swap either of those rows with the current row 2:

$$
\stackrel{R_2 \leftrightarrow R_3}{\Longrightarrow} 
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 0 & -1 & 10 & -5 \\ 0 & 0 & 8 & -8 \\ 0 & 3 & -12 & 10
\end{bmatrix}
$$

We now can select $-1$ as the leading element in row 2, and create zeros underneath it. Of course, we don't have any work to do in the current row 3.

$$
\stackrel{R_4 = R_4 + 3R_2}{\Longrightarrow} 
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 0 & -1 & 10 & -5 \\ 0 & 0 & 8 & -8 \\ 0 & 0 & 18 & -5
\end{bmatrix}
$$

Our first two rows are set. We're happy choosing $8$ as the leading element in row 3, and one more operation is needed to clear the element below it:

$$
\stackrel{R_4 = R_4 - \frac{18}{8}R_3}{\Longrightarrow} 
\begin{bmatrix}
1 & 2 & -4 & 5 \\ 0 & -1 & 10 & -5 \\ 0 & 0 & 8 & -8 \\ 0 & 0 & 0 & 13
\end{bmatrix}
$$

The matrix is now in RE form.

If our starting point is the augmented matrix of a linear system, then our new linear system would be 

$$
\begin{split}
1x_1 + 2x_2 -4x_3 &= 5  \\ 
0x_1 - 1x_2 +10x_3 &= -5 \\ 
0x_1 + 0x_2 +8x_3 &= -8 \\ 
0x_1 + 0x_2 + 0x_3 &= 13
\end{split}
$$

That last equation is a dealbreaker! The system is inconsistent.
:::

::::{#def-pivot}
# Pivots
The leading elements encountered during row elimination are called **pivots**. The columns they occur in are called **leading variables**, while the other variables (if any) are called **free variables**.
::::

:::{#exm-elimination-infinite}
Suppose we begin with the augmented matrix 
$$
\augmat{\begin{matrix}
1 & 1 & -1  \\
1 & 2 & 2  \\
2 & 1 & -5 
\end{matrix}}{
  \threevec{4}{3}{9}
}.
$$

The first 1 in the first row is a fine pivot, leading to 

$$
\stackrel{R_2 = R_2 - R_1;\; R_3 = R_3 - 2R_1}{\Longrightarrow} 
\augmat{\begin{matrix}
1 & 1 & -1  \\
0 & 1 & 3  \\
0 & -1 & -3 
\end{matrix}}{
  \begin{matrix}4\\-1\\1\end{matrix} 
}
$$

The leading 1 in the second row is the next pivot:

$$
\stackrel{R_3 = R_3 + R_2}{\Longrightarrow} 
\augmat{\begin{matrix}
1 & 1 & -1  \\
0 & 1 & 3  \\
0 & 0 & 0 
\end{matrix}}{
  \begin{matrix}4\\-1\\0\end{matrix}
}.
$$

Only the first two variables are considered leading variables, and $x_3$ remains free. So we say that $x_3=t$ for any arbitrary value of $t$. The last row of the final augmented matrix represents the equation $0=0$, which is true and implies no constraint. The second row implies 

$$
x_2 + 3x_3 = -1, 
$$

which is equivalent to $x_2=-1-3t$. Finally, the first row says that 

$$
x_1 + x_2 - x_3 = 4, 
$$

so that $x_1 = 4 - (-1-3t) + (t) = 5+4t$. The system has infinitely many solutions.
:::

## RRE form

An alternative to backward substitution is to transform the RE form further into something even simpler.

::::{#def-linalg-RREF} 
# RRE form

A matrix is in **RRE form** (reduced row-echelon form) if it meets all of these requirements:

1. Any rows of all zeros appear below all nonzero rows.
2. The leading element of any row is a one.
3. Every leading 1 that is lower than another leading 1 is also to the right of it.
4. Every leading 1 is the only nonzero in its column.

The columns that have leading ones are called **pivot columns**. The other columns are called **free columns**.
::::

There are two things that make RRE form useful. One is that it makes a unique target:

::::{#thm-RREF-unique} 
Every matrix is equivalent to a unique matrix in RRE form.
::::

The other useful fact is that RRE form completely exposes everything we want to know about a linear system. In fact, everything else we will do theoretically with matrices comes back to the RRE form, whether or not it's an augmented matrix.

::::{#exm-gauss-jordan} 
At the end of @exm-gauss-elimination, we had reached the augmented matrix

$$
\augmat{\begin{matrix}
1 & -1 & -1  \\
0 & 1 & 3 \\
0 & 0 & 3
\end{matrix}}{
  \begin{matrix}2\\3\\6\end{matrix}
  }.
$$

Note that the leading element of row 3 is not a 1. We fix that by multiplying it through:

$$
\stackrel{R_3 = \frac{1}{3}R_3}{\Longrightarrow}
\augmat{\begin{matrix}
1 & -1 & -1  \\
0 & 1 & 3 \\
0 & 0 & 1
\end{matrix}}{
  \begin{matrix}2\\3\\2\end{matrix}
  }.
$$

The leading one in row 1 is alone in its column. But the other two leading ones are not. We start at the bottom right and eliminate *upwards* now:

$$
\begin{split}
& \stackrel{R_2 = R_2 - 3R_3}{\Longrightarrow}
\augmat{\begin{matrix}
1 & -1 & -1  \\
0 & 1 & 0 \\
0 & 0 & 1
\end{matrix}}{
  \begin{matrix}2\\-3\\2\end{matrix}
  } \\[1ex]
  & \stackrel{R_1 = R_1 + 1R_3}{\Longrightarrow}
\augmat{\begin{matrix}
1 & -1 & 0  \\
0 & 1 & 0 \\
0 & 0 & 1
\end{matrix}}{
  \begin{matrix}4\\-3\\2\end{matrix}
  }.
\end{split}
$$

At this point, the leading one of row 3 is ready to go. We have just one more upwards step to perform from row 2:

$$
\stackrel{R_1 = R_1 + 1R_2}{\Longrightarrow}
\augmat{\begin{matrix}
1 & 0 & 0  \\
0 & 1 & 0 \\
0 & 0 & 1
\end{matrix}}{
  \begin{matrix}1\\-3\\2\end{matrix}
  }.
$$

The system is now trivial: $x_1=1$, $x_2=-3$, and $x_3=2$. Done!
::::

The process above is called **Gauss–Jordan elimination**, to distinguish it from Gaussian elimination. 
<!---
 
::::{#thm-linalg-phase1}  
# Row elimination downward phase
1. Set $i=1$.
2. Find the leftmost leading nonzero in rows $i$ and below. The column of this leading nonzero is known as the **pivot column**. If no such column exists, stop.
3. As necessary, swap rows and/or multiply a row by a constant to put a 1 in the pivot column of row $i$.
4. Add multiples of row $i$ to the rows below it in order to introduce zeros into the pivot column.
5. Increment $i$ and return to step 2.
::::

At the end of the downward phase, the augmented matrix is triangular, which is enough for solving a single system through backward substitution. However, it's not the cleanest form to work with theoretically, so we continue.

::::{#thm-linalg-phase2} 
# Row elimination upward phase
1. Set $i=m$ (the number of equations).
2. Use multiples of row $i$ to put zeros above the leading 1 in that row.
3. Decrement $i$. If $i> 1$, return to step 2.
::::

At the end of the upward phase, the matrix has been put into a form that we define formally here. 
--->

::::{#exm-RRE-julia}
Here is an example of reduction to RRE form for a linear system using Julia to do the arithmetic.

$$
\bfA = \begin{bmatrix}
2 & 0 & 4 & 3 \\ -2 & 0 & 2 & -13 \\ -4 & 5 & -7 & -10 \\ 1 & 15 & 2 & -4.5
\end{bmatrix}, \qquad 
\bfb = \begin{bmatrix}
4\\40\\9\\29
\end{bmatrix}.
$$

```{julia}
A = [
     2    0    4    3 
    -2    0    2  -13
    -4    5   -7  -10 
     1   15    2   -4.5
    ];
b = [ 4, 40, 9, 29 ];

S = Rational.([A b])
```

We start at the top, working downward and rightward. In the first row, the leading nonzero occurs in column 1, which is the pivot column for this row. We normalize this row so that the leading nonzero is a 1.

```{julia}
S[1,:] = S[1,:] / S[1,1]
S
```

Now multiples of row 1 are added to the rows below it in order to put zeros in the first column.

```{julia}
S[2,:] = S[2,:] - S[2,1]*S[1,:]
S[3,:] = S[3,:] - S[3,1]*S[1,:]
S[4,:] = S[4,:] - S[4,1]*S[1,:]
S
```

Looking at rows 2 to 4, we see that the leftmost nonzero occurs in column 2. Since row 2 has a zero there, we swap rows 2 and 3 to bring a nonzero up.

```{julia}
S[[2, 3],:] = S[[3, 2],:]
S
```

Now we normalize row 2 so that the leading nonzero is a 1.

```{julia}
S[2,:] = S[2,:] / S[2,2]
S
```

Multiples of row 2 are now used to put zeros below it in the pivot column.

```{julia}
S[3,:] = S[3,:] - S[3,2]*S[2,:]
S[4,:] = S[4,:] - S[4,2]*S[2,:]
S
```

Rows 3 and 4 have a pivot in column 3, and we only need to normalize row 3 to make it 1. Then we subtract a multiple of row 3 from row 4 to put a zero beneath it.

```{julia}
S[3,:] = S[3,:] / S[3,3];
S[4,:] = S[4,:] - S[4,3]*S[3,:]
S
```

We complete the downward phase by normalizing row 4 to get a leading 1.

```{julia}
S[4,:] = S[4,:] / S[4,4]
S
```

Now we turn around for the upward phase. The leading 1 in row 4 needs to have zeros above it. We accomplish that by subtracting multiples of row 4 from the others.

```{julia}
S[3,:] = S[3,:] - S[3,4]*S[4,:]
S[2,:] = S[2,:] - S[2,4]*S[4,:]
S[1,:] = S[1,:] - S[1,4]*S[4,:]
S
```

We move up to row 3 and use multiples of it to put zeros above its leading 1.

```{julia}
S[2,:] = S[2,:] - S[2,3]*S[3,:]
S[1,:] = S[1,:] - S[1,3]*S[3,:]
S
```

The last move is to use a multiple of row 2 to put a zero above its leading 1. As it has played out in this example, this line of code changes nothing because the position was already zero.

```{julia}
S[1,:] = S[1,:] - S[1,2]*S[2,:]
float(S)
```

This matrix is in RREF. We interpret it as the trivial linear system

$$
\begin{split}
x_1 &= -3,\\ x_2 &= 1,\\ x_3 &= 4, \\ x_4 &= -2.
\end{split}
$$
::::

:::{.callout-tip}
You can [use Wolfram Alpha]("https://www.wolframalpha.com/input/?i=rref%20%7B%7B1%2C2%2C3%7D%2C%7B4%2C5%2C6%7D%2C%7B7%2C8%2C9%7D%7D") to find the RRE form of a matrix.
:::

::: {.callout-caution}
While MATLAB has an `rref` command, it should not be relied on. It isn't really MATLAB's fault; the RRE form is a terrible target for numerical computations, because it is infinitely sensitive to rounding errors, and MATLAB calculates using numerical approximations for speed.
:::

### Solution from the RRE form

The RRE form of an augmented matrix represents a linear system that we can solve by inspection.

:::{#thm-RRE-solve}
To solve a system with augmented matrix in RRE form:

1. Ignore all zero rows.
2. If a leading one occurs in the last column, the system is inconsistent.
3. Otherwise, each variable associated with a free column is assigned to a free parameter (e.g., $s$, $t$, etc.).
4. Use the leading columns to solve for their corresponding variables in terms of the free parameters.
:::

::::{#exm-RREF-multiple}
A linear system has an augmented matrix equivalent to the RRE matrix

$$
\augmat{
\begin{matrix}
0 & 1 & -3 & 0 \\
0 & 0 & 0  & 1  \\
0 & 0 & 0  & 0   \\
0 & 0 & 0  & 0  
\end{matrix} }{
  \begin{matrix}2 \\ -4 \\ 0 \\ 0 \end{matrix}
}
$$

Find all solutions to the linear system.

:::{.solution}
The last two rows have no information and can be ignored. Columns 2 and 4 are pivot columns. We give variables from the other columns arbitrary values, $x_1=s$ and $x_3=t$. The pivot variables are solved in terms of these to get $x_2=2+3t$ and $x_4=-4$. All solutions are expressed as the vector

$$
\bfx = \begin{bmatrix}
s \\ 2+3t \\ t \\ -4
\end{bmatrix}, \qquad s,t\in \real.
$$
:::
::::

::::{#exm-}
A linear system has an augmented matrix equivalent to the RRE form

$$
\augmat{
\begin{matrix}
1 & 0 & -4 & 0  \\
0 & 1 & 1  & 0 \\
0 & 0 & 0  & 1  \\
0 & 0 & 0  & 0 
\end{matrix}}{
  \begin{matrix}0 \\ 0 \\ 0 \\ 1 \end{matrix}
}
$$

Find all solutions to the linear system.

:::{.solution}
Look at the last row of the system. It expresses the equation $0=1$, which is impossible to satisfy. Thus the system is inconsistent.
:::
::::

### Rank

Leaving aside the actual calculation of solutions, a glance at the RRE form of the augmented matrix tells us instantly whether the system has no solution, a unique solution, or infinitely many solutions. Because of the requirements on leading ones in the RREF definition, the outcomes are constrained simply by how many of those ones there are.

::::{#def-RREF-rank} 
# Rank
The **rank** of a matrix is the number of pivot columns in its RRE form, i. e., the number of leading ones.
::::

::::{#thm-RREF-rank} 
If $\bfA$ is an $m\times n$ matrix, then $\rank(\bfA) \le m$ and $\rank(\bfA) \le n$.
::::
:::{.proof}
Each leading one in the RRE form requires a row and column of its own.
:::

::::{#thm-RREF-deficient} 
Suppose $\bfA$ is $m\times n$. If $\rank(\bfA)<n$, then the linear system with augmented matrix $\augmat{\bfA}{\bfb}$ cannot have a unique solution. It is inconsistent for some choices of $\bfb$ and has infinitely many solutions for other choices of $\bfb$.
::::
:::{.proof}
Let $r=\rank(\bfA)$. Then the RRE form of $\bfA$ has all zero elements in rows $r+1$ to $m$. Now augment $\bfA$ with $\bfb$ and perform the row operations that reduce $\bfA$ to RRE form. If there are any nonzeros in rows $r+1$ to $m$ of the last column, the system is inconsistent. Otherwise, the system has at least one free variable. We can start with either type of outcome and run the row elimination process backward to find a $\bfb$ that led to it.
:::

::::{#cor-RREF-nonunique} 
A linear system with more variables than equations cannot have a unique solution.
::::
:::{.proof}
$\rank(\bfA) \le m < n$.
:::

## Homogeneous systems

There are only three possible outcomes for a linear system, all deducible from the RRE form of the augmented matrix:

1. There is a leading 1 in the last column, in which case there are no solutions.
2. There are fewer pivot columns than variables, in which case there are infinitely many solutions.
3. There is a unique solution.

There is an important situation where we can rule out case 1 above.

::::{#def-nullspace-def} 
# Homogeneous linear system
A **homogeneous** linear system is one that has forcing vector $\bfb$ equal to zero.
::::

As the following theorem points out, we can work with just the RRE form of the coefficient matrix $\bfA$ instead of the augmented matrix of a homogeneous system.

::::{#thm-nullspace-rref}
For a homogeneous linear system, the last column of the RRE form of the augmented matrix is a zero vector.
::::
:::{.proof}
Row operations can't change the zeros in that column.
:::

### Null space

::::{#def-nullspace-nullspace}
The **nullspace** of a matrix $\bfA$, written $\nullsp(\bfA)$, is the set of all solutions to the homogeneous linear system with coefficient matrix $\bfA$.
::::

:::{.callout-note}
Sometimes the null space is called the *kernel* of the matrix.
:::

<!-- As the name implies, the null space is our first example of what we will call a *subspace*. We won't worry about that definition just yet. -->

For a homogeneous linear system, the zero vector $\bfx=\bfzero$ is always a solution. The only interesting question is whether zero is all by itself in the nullspace. That is, we only need to look at case 2 from the beginning of this section.

The following conclusions ensue immediately from our definitions and previous results. Often, linear algebra is like standup comedy: all the work is in the setup, not the punchline.

::::{#thm-nullspace-uniquenes}
A homogeneous system in $n$ variables with coefficient matrix $\bfA$ has a unique solution if and only if $\rank(\bfA)=n$.
::::

::::{#cor-nullspace-uniqueness}
A homogeneous linear system with more variables than equations has infinitely many solutions.
::::

### Relating to nonhomogeneous systems

There is a strong correspondence between a nonhomogeneous linear ODE $\opA[x]=f$ and a nonhomogeneous linear system $\bfA\bfx=\bfb$. 

::::{#thm-linsys-nonhomogeneous}
If $\bfu$ is in the nullspace of $\bfA$ and $\bfv$ is any solution of $\bfA\bfx = \bfb$, then $\bfu+\bfv$ is also a solution of $\bfA\bfx = \bfb$.
::::
:::{.proof}
$$
\bfA(\bfu + \bfv) = \bfA\bfu + \bfA\bfv = \bfzero + \bfb = \bfb. 
$$
:::

::::{#exm-nonhomogeneous-1D}
Suppose 

$$
\begin{bmatrix}
2 & -3
\end{bmatrix}
\bfx = \begin{bmatrix}
5
\end{bmatrix}. 
$$

Geometrically, all solutions $[x_1,\:x_2] \in \real^2$ lie on the line defined by $2x_1 - 3x_2 = 5$. Note that the vector $[1,\:-1]$ is one such solution. 

The null space of the matrix is the set of vectors such that $2x_1-3x_2=0$. If we let $x_2=t$ be a free parameter, then $x_1=\frac{3}{2}t$. So we can write solutions of the original problem in the form

$$
\bfx = \begin{bmatrix}
\tfrac{3}{2}t \\ t 
\end{bmatrix} +
\begin{bmatrix}
1 \\ -1
\end{bmatrix}
= 
\begin{bmatrix}
1 + \tfrac{3}{2}t \\ -1 + t
\end{bmatrix},
$$

which is simply one way to parameterize the line $2x_1 - 3x_2 = 5$.
::::



## Linear combinations 


### Spanning sets

When we have infinitely many solutions in a linear system, it's nice to have a concise description of all of them. This takes us on a considerable side quest.

<!---
 Using a small matrix, we see an important connection to a familiar concept:

$$
\begin{split}
\begin{bmatrix}
A_{11} & A_{12} & A_{13} \\ 
A_{21} & A_{22} & A_{23} \\ 
A_{31} & A_{32} & A_{33} 
\end{bmatrix} \begin{bmatrix}
  x_1 & x_2 & x_3
\end{bmatrix}   &=  \begin{bmatrix}
A_{11}x_1 + A_{12}x_2 +  A_{13}x_3 \\ 
A_{21}x_1 + A_{22}x_2 +  A_{23}x_3 \\ 
A_{31}x_1 + A_{32}x_2 +  A_{33}x_3 
\end{bmatrix} \\ 
&= x_1 \begin{bmatrix}
A_{11} \\ A_{21} \\ A_{31}
\end{bmatrix} + x_2 \begin{bmatrix}
A_{12} \\ A_{22} \\ A_{32}
\end{bmatrix} + x_3 \begin{bmatrix}
A_{13} \\ A_{23} \\ A_{33}
\end{bmatrix}. 
$$

This relationship holds in general. 
--->

::::{#def-linalg-linear-comb} 
# Linear combination
A **linear combination** of vectors $\bfv_1, \ldots, \bfv_k$ with **coefficients** $c_1,\ldots,c_k$ is the vector

$$
c_1 \bfv_1 + c_2 \bfv_2 + \cdots + c_k \bfv_k.
$$
::::

<!---
 ::::{#thm-matvec-linear-combination}
The matrix--vector product $\bfA \bfx$ is a linear combination of the columns of $\bfA$ using the elements of $\bfx$ as coefficients.
::::
 
--->

As we are about to see, statements about linear combinations can be transformed into statements about linear systems of equations, and vice versa.

### Span

::::{#def-linalg-span}
The set of all linear combinations of $\bfv_1,\ldots,\bfv_k$ is known as the **span** of those vectors. The span of an empty set is the singleton set $\{\bfzero\}$.
::::

::::{#exm-nullspace-combo}
Show that $\bfv=[1,4,3]$ is in the span of $\bfu_1=[1,-2,-1]$ and $\bfu_2=[3,0,1]$. (That is, $\bfv$ can be written as a linear combination of these two vectors.)

:::{.solution}
Suppose that $\bfv = c_1 \bfu_1 + c_2 \bfu_2$. Looking at each component of this vector equation, we conclude that

$$
\begin{split}
  c_1 + 3c_2 &= 1 \\
  -2c_1 + 0c_2 &= 4 \\
  -c_1 + c_2 &= 3,
\end{split}
$$

which is a linear system with augmented matrix

$$
\augmat{
\begin{matrix}
  1 & 3  \\ -2 & 0  \\ -1 & 1 
\end{matrix}}{
    \begin{matrix}
   1 \\  4 \\  3
\end{matrix}}
$$

The RRE form of this matrix is

$$
\stackrel{RRE}{\Longrightarrow}
\augmat{
\begin{matrix}
  1 & 0  \\ 0 & 1  \\ 0 & 0
\end{matrix}}{
    \begin{matrix}
   -2 \\  1 \\  0
\end{matrix}
}
$$

which is consistent. In fact, the unique solution is $c_2=1$, $c_1=2$.
:::
::::

Here is a formal statement of the idea used in @exm-nullspace-combo. 

::::{#thm-linsys-span}
Let $\bfa_1,\ldots,\bfa_n$ be vectors in $\real^n$, and let 

$$
\bfA = \begin{bmatrix}
\bfa_1 & \bfa_2 & \cdots & \bfa_n
\end{bmatrix} \in \rmn{m}{n}. 
$$

Then a vector $\bfb \in \real^m$ lies in the span of $\bfa_1,\ldots,\bfa_n$ if and only if the system with augmented matrix $\augmat{\bfA}{\bfb}$ is consistent.
::::

The idea behind span is that it allows us to represent an infinite set, such as a null space, using a finite number of vectors. 

::::{#exm-nullspace-span}
Suppose the RRE form of a matrix $\bfA$ is

$$
\mathbf{R} = 
\begin{bmatrix}
0 & 1 & 4 & 0 & 1 \\
0 & 0 & 0  & 1 & -3 \\
0 & 0 & 0  & 0 & 0  \\
0 & 0 & 0  & 0 & 0
\end{bmatrix}.
$$

Therefore, the RRE form of the augmented matrix $\augmat{\mathbf{A}}{\boldsymbol{0}}$ is $\augmat{\mathbf{R}}{\boldsymbol{0}}$. The pivot columns are 2 and 4, which makes $x_1=r$, $x_3=s$, and $x_5=t$ free variables. The nonzero rows lead to

$$
\begin{split}
x_2 + 4x_3 + x_5 &= 0 &\quad ⇒ \quad x_2 &= -4s-t,\\
x_4 -3x_5 &= 0 & \quad ⇒ \quad x_4 &= 3t.
\end{split}
$$

One way to express a generic solution vector in the null space is by a linear combination of constant vectors:

$$
\begin{bmatrix}
  r \\ -4s-t \\ s \\ 3t\\ t
\end{bmatrix}
= r \begin{bmatrix}
  1 \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix}
+ s \begin{bmatrix}
  0 \\ -4 \\ 0 \\ 1 \\ 0
\end{bmatrix}
+ t \begin{bmatrix}
  0 \\ -1 \\ 0 \\ 3 \\ 1
\end{bmatrix} = r\bfv_1 + s\bfv_2 + t\bfv_3 .
$$

Hence $\nullsp(\bfA)=\span(\bfv_1,\bfv_2,\bfv_3)$, where the $\bfv_j$ are the constant vectors above.
::::

### Independence

Now that we have a finite description of the null space, we want to make sure that the description is as simple as possible. For this, we need to put some fresh paint on an old barn.

::::{#def-nullspace-indepdendence}
# Linear independence
Vectors $\bfv_1,\bfv_2,\ldots,\bfv_n$ are **linearly dependent** if there is a way to choose constants $c_1,\ldots,c_n$, *not all zero*, such that

$$
c_1\bfv_1 + c_2\bfv_2 + \cdots c_n \bfv_n = 0.
$$

If the vectors are not linearly dependent, then they are **linearly independent**.
::::

Because linear combinations are equivalent to linear systems, statements about linear independence are equivalent to statements about homogeneous linear systems, and therefore about null spaces.

::::{#thm-linalg-independence}
Let $\bfa_1,\ldots,\bfa_n$ be vectors in $\real^n$, and let 

$$
\bfA = \begin{bmatrix}
\bfa_1 & \bfa_2 & \cdots & \bfa_n
\end{bmatrix} \in \rmn{m}{n}. 
$$

The following statements are equivalent.

1. The vectors $\bfa_1,\ldots,\bfa_n$ are linearly independent.
2. The system with augmented matrix $\augmat{\bfA}{\bfzero}$ has only zero as a solution.
3. The null space of $\bfA$ contains only the zero vector.
4. $\rank(\bfA) = n$.
::::

::::{#exm-dependence-zerovector}
Any set of vectors that contains the zero vector must be dependent. That's because the zero vector will put a column of zeros in the matrix $\bfA$ in @thm-linalg-independence, and since that column can't possibly have a leading nonzero in it, the rank of $\bfA$ will be less than the number of columns.
::::


::::{#exm-nullspace-dependent}
Determine whether the vectors $\bfv_1=\threevec{1}{0}{-2}$, $\bfv_2=\threevec{1}{0}{1}$, and $\bfv_3 = \threevec{0}{0}{-1}$ are linearly independent.

:::{.solution}
These vectors are dependent if, and only if, we can find $c_1,c_2,c_3$, not all zero, such that

$$
c_1 \bfv_1 + c_2 \bfv_2 + c_3 \bfv_3 = \bfzero.
$$

Equating components on the two sides of this equation leads to the linear system with augmented matrix

$$
\augmat{
    \begin{matrix}
    1 & 1 & 0  \\ 0 & 0 & 0  \\ -2 & 1 & -1 
    \end{matrix}}{
\begin{matrix}
    0 \\  0 \\  0
    \end{matrix}
}
$$

The RRE form of this system is

$$
\augmat{
    \begin{matrix}
    1 & 0 & \frac{1}{3}  \\ 0 & 1 & -\frac{1}{3}  \\ 0 & 0 & 0
    \end{matrix}}{
\begin{matrix}
    0 \\  0 \\  0
    \end{matrix}
}
$$

whose solutions are $[-t/3,t/3,t]$ for all $t$. Since there are nonzero solutions to the linear system, the original set of vectors is linearly dependent.
:::
::::

Besides the presence of a zero vector, there is one easy case to detect dependence.

::::{#thm-linsys-dependence-too-many}
Any collection of $n$ vectors in $\real^m$ is dependent if $n > m$. 
::::
:::{.proof}
The rank of $\bfA$ in @thm-linalg-independence is no greater than $m$, which is less than $n$.
:::

::: {.callout-warning}
@thm-linsys-dependence-too-many says, "if $n > m$, then dependent," but the converse statement, "if dependent, then $n > m$," is not logically correct. There are examples of dependent sets of vectors with $n < m$, $n=m$, *and* $n>m$.  
:::

Dependent vectors lengthen the description of a spanned space without contributing any information. We will come back to finish this thought in the next chapter. But first, we are going to take a closer look at the algebra of vectors and matrices.

<!---
 ::::{#exm-nullspace-dependentspan}
The result of @exm-nullspace-dependent is that $\bfv_1=\threevec{1}{0}{-2}$, $\bfv_2=\threevec{1}{0}{1}$, and $\bfv_3 = \threevec{0}{0}{-1}$ are linearly dependent. For instance,

$$
\bfv_1 - \bfv_2 + 3 \bfv_3 = \bfzero.
$$

Therefore, any vector in $\span(\bfv_1,\bfv_2,\bfv_3)$ can be written as

$$
\begin{split}
c_1 \bfv_1 + c_2 \bfv_2 + c_3 \bfv_3 &= c_1(\bfv_2 - 3 \bfv_3) + c_2 \bfv_2 + c_3 \bfv_3 \\
&= (c_1+c_2) \bfv_2 + (c_3- 3c_1)\bfv_3.
\end{split}
$$

Hence $\span(\bfv_1,\bfv_2,\bfv_3) = \span(\bfv_2,\bfv_3)$.
::::

:::{.callout-note}
In @exm-nullspace-dependentspan, we could have removed $\bfv_2$ or $\bfv_3$ from the spanning set instead without changing the spanned result. The only requirement is that the coefficient of the removed vector is not zero in the linear combination of them that makes zero.
:::

A set of vectors that both spans the null space and is linearly independent is called a *basis* of the null space. We'll formalize that idea when we discuss subspaces more generally. For now, just know that a basis is a minimal set of vectors that can get the spanning job done.
 
--->
