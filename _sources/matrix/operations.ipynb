{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divided-purpose",
   "metadata": {},
   "source": [
    "# Algebraic operations\n",
    "\n",
    "## Matrix times vector\n",
    "\n",
    "The idea of linear combinations, as defined in {numref}`Definition {number} <definition-nullspace-linear-comb>`, serves as the foundation of multiplication between a matrix and a vector.\n",
    "\n",
    "```{index} ! matrix times vector\n",
    "```\n",
    "\n",
    "(definition-operations-matvec)=\n",
    "::::{proof:definition} Matrix times vector\n",
    "Given $\\bfA\\in\\cmn{m}{n}$ and $\\bfx\\in\\complex^{n}$, the product $\\bfA\\bfx$ is defined as\n",
    "\n",
    "```{math}\n",
    ":label: eq-operations-matvec\n",
    "\\bfA\\bfx = x_1 \\bfa_1 + x_2 \\bfa_2 + \\cdots + x_n \\bfa_n = \\sum_{j=1}^n x_j \\bfa_j,\n",
    "```\n",
    "\n",
    "where $\\bfa_j$ refers to the $j$th column of $\\bfA$.\n",
    "::::\n",
    "\n",
    "```{attention}\n",
    "In order for $\\bfA\\bfx$ to be defined, the number of columns in $\\bfA$ has to be the same as the number of elements in $\\bfx$. \n",
    "```\n",
    "\n",
    "Note that when $\\bfA$ is $m\\times n$, then $\\bfx$ must be in $\\real^n$ or $\\complex^n$, and $\\bfA\\bfx$ has dimension $m$. \n",
    "\n",
    "::::{proof:example}\n",
    "Calculate the product\n",
    "\n",
    "```{math}\n",
    "\\begin{bmatrix} \n",
    "1 & -1 & -1 \\\\ 3 & -2 & 0 \\\\ 1 & -2 & -1 \\end{bmatrix} \\threevec{-1}{2}{-1}.\n",
    "```\n",
    "\n",
    ":::{dropdown} Solution\n",
    "The product is equivalent to\n",
    "\n",
    "```{math}\n",
    "(-1) \\threevec{1}{3}{1} + (2) \\threevec{-1}{-2}{-2} + (-1) \\threevec{-1}{0}{-1} = \\threevec{-2}{-7}{-4}.\n",
    "```\n",
    "\n",
    "We don't often write out the product in this much detail. Instead we \"zip together\" the rows of the matrix with the entries of the vector:\n",
    "\n",
    "```{math}\n",
    "\\threevec{(-1)(1)+(2)(-1)+(-1)(-1)}{(-1)(3)+(2)(-2)+(-1)(0)}{(-1)(1)+(2)(-2)+(-1)(-1)}  = \\threevec{-2}{-7}{-4}.\n",
    "```\n",
    "\n",
    "You might recognize the \"zip\" expressions in this vector as dot products from vector calculus.\n",
    ":::\n",
    "::::\n",
    "\n",
    "### Properties\n",
    "\n",
    "What justifies calling this operation multiplication? In large part, it's the natural distributive properties\n",
    "\n",
    "\\begin{align*}\n",
    "\\bfA(\\bfx+\\bfy) & =  \\bfA\\bfx + \\bfA\\bfy,\\\\\n",
    "(\\bfA+\\bfB)\\bfx & =  \\bfA\\bfx + \\bfB\\bfx,\n",
    "\\end{align*}\n",
    "\n",
    "which can be checked with a little effort. It's also true that $\\bfA(c\\bfx)=c(\\bfA\\bfx)$ for any scalar $c$. However, there is a big departure from multiplication as we usually know it.\n",
    "\n",
    "```{warning}\n",
    "Matrix-vector products are not commutative. In fact, $\\bfx\\bfA$ is not defined even when $\\bfA\\bfx$ is.\n",
    "```\n",
    "\n",
    "### Connection to linear systems\n",
    "\n",
    "The following observation finally brings us back around to the introduction of linear systems through the insultingly simple scalar equation $ax=b$. \n",
    "\n",
    "::::{proof:observation}\n",
    "The linear system with coefficient matrix $\\bfA$, right-side vector $\\bfb$, and solution $\\bfx$ is equivalent to the equation $\\bfA\\bfx=\\bfb$.\n",
    "::::\n",
    "\n",
    "Bringing together more of our definitions:\n",
    "\n",
    "::::{proof:theorem}\n",
    "1. The linear system $\\bfA\\bfx=\\bfb$ is consistent if and only if $\\bfb$ is in the span of the columns of $\\bfA$.\n",
    "2. The null space of a matrix contains nonzero vectors if and only if the columns of the matrix are linearly dependent.\n",
    "::::\n",
    "\n",
    "## Matrix multiplication\n",
    "\n",
    "We can think of vectors as a special kind of matrix, and accordingly we can generalize matrix-vector products to matrix-matrix products. There are many equivalent ways to define these products. Here is the one we start with.\n",
    "\n",
    "::::{proof:definition} Matrix times matrix\n",
    "If $\\bfA$ is $m\\times n$ and $\\bfB$ is $n\\times p$, then the product $\\bfA\\bfB$ is defined as\n",
    "\n",
    "```{math}\n",
    ":label: matrix-mult\n",
    "\\bfA\\mathbf{B}\n",
    "= \\bfA \\begin{bmatrix} \\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\end{bmatrix}\n",
    "= \\begin{bmatrix} \\bfA\\mathbf{b}_1 & \\bfA\\mathbf{b}_2 & \\cdots & \\bfA\\mathbf{b}_p \\end{bmatrix}.\n",
    "```\n",
    "::::\n",
    "\n",
    "In words, a matrix-matrix product is the horizontal concatenation of matrix-vector products involving the columns of the right-hand matrix.\n",
    "\n",
    "```{warning}\n",
    "In order to define $\\bfA\\bfB$, we require that the number of columns in $\\bfA$ is the same as the number of rows in $\\bfB$. That is, the *inner dimensions* must agree. The result has size determined by the *outer dimensions* of the original matrices.\n",
    "```\n",
    "\n",
    "When we compute a matrix product by hand, we usually don't write out the above. Instead we use a more compact definition for the individual entries of $\\mathbf{C} = \\bfA\\bfB$,\n",
    "\n",
    "```{math}\n",
    ":label: matrix-mult-element\n",
    "C_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}, \\qquad i=1,\\ldots,m, \\quad j=1,\\ldots,p.\n",
    "```\n",
    "\n",
    "The sum to get a single $C_{ij}$ is what we called a \"zip\", or essentially a dot product, of row $i$ from $\\bfA$ with column $j$ from $\\bfB$.\n",
    "\n",
    "::::{proof:example}\n",
    "\n",
    "Find $\\mathbf{A}\\mathbf{B}$ if\n",
    "\n",
    "```{math}\n",
    "\\bfA = \\begin{bmatrix}\n",
    "1 & -1 \\\\ 0 & 2 \\\\ -3 & 1\n",
    "\\end{bmatrix}, \\qquad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "2 & -1 & 0 & 4 \\\\ 1 & 1 & 3 & 2\n",
    "\\end{bmatrix}.\n",
    "```\n",
    "\n",
    ":::{dropdown} Solution\n",
    "Using {eq}`matrix-mult-element`,\n",
    "\n",
    "\\begin{align*}\n",
    "\\bfA\\mathbf{B} &= \\begin{bmatrix}\n",
    "(1)(2) + (-1)(1) & (1)(-1) + (-1)(1) & (1)(0) + (-1)(3) & (1)(4) + (-1)(2) \\\\\n",
    "(0)(2) + (2)(1) & (0)(-1) + (2)(1) & (0)(0) + (2)(3) & (0)(4) + (2)(2) \\\\\n",
    "(-3)(2) + (1)(1) & (-3)(-1) + (1)(1) & (-3)(0) + (1)(3) & (-3)(4) + (1)(2)\n",
    "\\end{bmatrix} \\\\\n",
    "& = \\begin{bmatrix}\n",
    "1 & -2 & -3 & 2 \\\\ 2 & 2 & 6 & 4 \\\\ -5 & 4 & 3 & -10\n",
    "\\end{bmatrix}\n",
    "\\end{align*}.\n",
    "\n",
    "Observe that\n",
    "\n",
    "```{math}\n",
    "\\bfA \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = 2 \\begin{bmatrix} 1 \\\\ 0 \\\\ -3\n",
    "\\end{bmatrix} + 1 \\begin{bmatrix} -1 \\\\ 2 \\\\ 1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 1 \\\\ 2 \\\\ -5 \\end{bmatrix},\n",
    "```\n",
    "\n",
    "and so on.\n",
    ":::\n",
    "::::\n",
    "\n",
    "### Properties\n",
    "\n",
    "First, the bad news. \n",
    "\n",
    "```{warning}\n",
    "Matrix multiplication is not commutative. If $\\bfA\\bfB$ is defined, then $\\bfB\\bfA$ may not be, and even if it is, it may not equal $\\bfA\\bfB$. Put another way, you cannot simply change the order of the terms in a matrix product without some explicit justification.\n",
    "```\n",
    "\n",
    "Fortunately, other familiar and handy properties of multiplication do come along for the ride:\n",
    "\n",
    "1. $(\\bfA\\bfB)\\mathbf{C}=\\bfA(\\bfB \\mathbf{C})\\qquad$  (association)\n",
    "2. $\\bfA(\\bfB+\\mathbf{C}) = \\bfA\\bfB + \\bfA\\mathbf{C}\\qquad$  (right distribution)\n",
    "3. $(\\bfA+\\bfB)\\mathbf{C} = \\bfA\\mathbf{C} + \\bfB\\mathbf{C}\\qquad$   (left distribution)\n",
    "\n",
    "These properties are easy to demonstrate computationally. (But keep in mind that a numerical demonstration, or an algebraic one at particular sizes, is not a general proof.) \n",
    "\n",
    "## Transpose\n",
    "\n",
    "Here's a curious operation that we won't be using much, but it is important enough to know about.\n",
    "\n",
    "```{index} ! transpose of a matrix\n",
    "```\n",
    "\n",
    "(definition-algebra-transpose)=\n",
    "::::{proof:definition} Transpose of a matrix\n",
    "The **transpose** of $m\\times n$ matrix $\\bfA$, whose elements are $A_{ij}$, is the $n\\times m$ matrix $\\bfA^T$ with elements $A_{ji}$.\n",
    "::::\n",
    "\n",
    "When taking the transpose, rows become columns, and vice versa.\n",
    "\n",
    "### Properties\n",
    "\n",
    "(theorem-algebra-transpose)=\n",
    "::::{proof:theorem} \n",
    "If $\\bfA$ and $\\bfB$ are matrices of compatible sizes, and $c$ is a number, then\n",
    "1. $(\\bfA^T)^T = \\bfA$\n",
    "2. $(\\bfA+\\bfB)^T = \\bfA^T + \\bfB^T$\n",
    "3. $(c\\bfA^T) = c(\\bfA^T)$\n",
    "4. $(\\bfA\\bfB)^T = \\bfB^T \\bfA^T$\n",
    "::::\n",
    "\n",
    "Only the last of these is not intuitively clear."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}